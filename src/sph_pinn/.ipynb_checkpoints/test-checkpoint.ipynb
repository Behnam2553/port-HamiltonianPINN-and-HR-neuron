{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This script implements a Physics-Informed Stable Port-Hamiltonian Neural Network.\n",
    "The model learns the underlying physical structure of an error-feedback system\n",
    "by training a composite model that simultaneously predicts the system's state\n",
    "and enforces the port-Hamiltonian structure as a physics constraint.\n",
    "The model is composed of two main parts:\n",
    "1.  State Prediction Network: An MLP with Fourier features (StateNN)\n",
    "    learns the combined state trajectories q(t) and s(t).\n",
    "2.  Port-Hamiltonian Networks: Three networks that define the error system's\n",
    "    dynamics based on the predicted state s_pred:\n",
    "      s_dot = (J(s) - R(s)) * grad(H(s))\n",
    "    - HamiltonianNN (H): A convex network learning the system's energy.\n",
    "    - DynamicJ_NN (J): An MLP learning the conservative dynamics.\n",
    "    - DissipationNN (R): An MLP learning the dissipative dynamics.\n",
    "The training loss is a combination of:\n",
    "- Data Fidelity Loss: MSE between predicted states (s_pred, q_pred) and true data.\n",
    "- Conservative Loss: Enforces that the learned Hamiltonian is invariant\n",
    "  under the conservative flow (Lie derivative is zero).\n",
    "- Dissipative Loss: Enforces that the time derivative of the Hamiltonian\n",
    "  is correctly described by the dissipative flow.\n",
    "- Physics Structure Loss: Enforces that the output of the learned sPHNN\n",
    "  structure matches the analytical vector field.\n",
    "\"\"\"\n",
    "\n",
    "import jax, jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "from src.hr_model.model import DEFAULT_PARAMS\n",
    "import os\n",
    "\n",
    "# JAX configuration to use 64-bit precision.\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. NEURAL NETWORK DEFINITIONS\n",
    "# ==============================================================================\n",
    "\n",
    "class FourierFeatures(eqx.Module):\n",
    "    \"\"\"Encodes a 1D input into a higher-dimensional space using Fourier features.\"\"\"\n",
    "    b_matrix: jax.Array\n",
    "\n",
    "    def __init__(self, key, in_size=1, mapping_size=32, scale=1):\n",
    "        self.b_matrix = jax.random.normal(key, (mapping_size // 2, in_size)) * scale\n",
    "\n",
    "    def __call__(self, t):\n",
    "        if t.ndim == 1:\n",
    "            t = t[None, :]\n",
    "        t_proj = t @ self.b_matrix.T\n",
    "        return jnp.concatenate([jnp.sin(t_proj), jnp.cos(t_proj)], axis=-1).squeeze()\n",
    "\n",
    "\n",
    "class StateNN(eqx.Module):\n",
    "    \"\"\"An MLP with Fourier Features to approximate the combined state [q(t), s(t)].\"\"\"\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key, out_size=15, width=256, depth=3, mapping_size=32, scale=300):\n",
    "        fourier_key, *layer_keys = jax.random.split(key, depth + 1)\n",
    "        self.layers = [\n",
    "            FourierFeatures(fourier_key, in_size=1, mapping_size=mapping_size, scale=scale),\n",
    "            eqx.nn.Linear(mapping_size, width, key=layer_keys[0]),\n",
    "            *[eqx.nn.Linear(width, width, key=key) for i in range(1, depth - 1)],\n",
    "            eqx.nn.Linear(width, out_size, key=layer_keys[-1])\n",
    "        ]\n",
    "\n",
    "    def __call__(self, t):\n",
    "        x = self.layers[0](t)\n",
    "        for layer in self.layers[1:-1]:\n",
    "            x = jax.nn.tanh(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "\n",
    "# --- sPHNN Component Networks (from sPHNN implementation) ---\n",
    "\n",
    "class _FICNN(eqx.Module):\n",
    "    \"\"\"Internal helper class for a Fully Input Convex Neural Network.\"\"\"\n",
    "    w_layers: list\n",
    "    u_layers: list\n",
    "    final_layer: eqx.nn.Linear\n",
    "    activation: callable = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, key, in_size: int, out_size: int, width: int, depth: int):\n",
    "        self.activation = jax.nn.softplus\n",
    "        keys = jax.random.split(key, depth)\n",
    "        self.w_layers = [eqx.nn.Linear(in_size, width, key=keys[0])]\n",
    "        self.w_layers.extend([eqx.nn.Linear(in_size, width, key=key) for key in keys[1:-1]])\n",
    "        self.u_layers = [eqx.nn.Linear(width, width, use_bias=False, key=key) for key in keys[1:-1]]\n",
    "        self.final_layer = eqx.nn.Linear(width, out_size, use_bias=False, key=keys[-1])\n",
    "\n",
    "    def __call__(self, s):\n",
    "        z = self.activation(self.w_layers[0](s))\n",
    "        for i in range(len(self.u_layers)):\n",
    "            # Enforce non-negative weights for convexity\n",
    "            u_layer_non_negative = eqx.tree_at(lambda l: l.weight, self.u_layers[i], jnp.abs(self.u_layers[i].weight))\n",
    "            z = self.activation(u_layer_non_negative(z) + self.w_layers[i + 1](s))\n",
    "        return self.final_layer(z)[0]\n",
    "\n",
    "\n",
    "class HamiltonianNN(eqx.Module):\n",
    "    \"\"\"Learns a convex Hamiltonian function H(x) with a guaranteed minimum at x0.\"\"\"\n",
    "    ficnn: _FICNN\n",
    "    x0: jax.Array\n",
    "    epsilon: float = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, key, in_size, width, depth, x0, epsilon):\n",
    "        self.ficnn = _FICNN(key, in_size, out_size=1, width=width, depth=depth)\n",
    "        self.x0 = x0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Implements Equation (10) from the paper\n",
    "        f_x = self.ficnn(x)\n",
    "        f_x0 = self.ficnn(self.x0)\n",
    "        grad_f_x0 = jax.grad(self.ficnn)(self.x0)\n",
    "        # Normalization term to set H(x0)=0 and grad H(x0)=0\n",
    "        f_norm = f_x0 + jnp.dot(grad_f_x0, x - self.x0)\n",
    "        # Regularization term to ensure a strict minimum\n",
    "        f_reg = self.epsilon * jnp.sum((x - self.x0) ** 2)\n",
    "        return f_x - f_norm + f_reg\n",
    "\n",
    "\n",
    "class DissipationNN(eqx.Module):\n",
    "    \"\"\"Learns a positive semi-definite dissipation matrix R(s) = L(s)L(s)^T.\"\"\"\n",
    "    layers: list\n",
    "    activation: callable\n",
    "    state_dim: int\n",
    "\n",
    "    def __init__(self, key, state_dim, width, depth, activation):\n",
    "        self.state_dim = state_dim\n",
    "        num_l_elements = state_dim * (state_dim + 1) // 2\n",
    "        keys = jax.random.split(key, depth)\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(state_dim, width, key=keys[0]),\n",
    "            *[eqx.nn.Linear(width, width, key=key) for key in keys[1:-1]],\n",
    "            eqx.nn.Linear(width, num_l_elements, key=keys[-1])\n",
    "        ]\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, s):\n",
    "        for layer in self.layers[:-1]:\n",
    "            s = self.activation(layer(s))\n",
    "        l_elements = self.layers[-1](s)\n",
    "\n",
    "        # Build the lower triangular matrix L\n",
    "        L = jnp.zeros((self.state_dim, self.state_dim))\n",
    "        tril_indices = jnp.tril_indices(self.state_dim)\n",
    "        L = L.at[tril_indices].set(l_elements)\n",
    "\n",
    "        # Enforce positive diagonal elements for positive definiteness\n",
    "        positive_diag = jax.nn.softplus(jnp.diag(L))\n",
    "        L = L.at[jnp.diag_indices(self.state_dim)].set(positive_diag)\n",
    "\n",
    "        # Return R = L @ L.T\n",
    "        return L @ L.T\n",
    "\n",
    "\n",
    "class DynamicJ_NN(eqx.Module):\n",
    "    \"\"\"Learns a skew-symmetric structure matrix J(s).\"\"\"\n",
    "    layers: list\n",
    "    state_dim: int\n",
    "    activation: callable\n",
    "\n",
    "    def __init__(self, key, state_dim, width, depth, activation):\n",
    "        self.state_dim = state_dim\n",
    "        num_unique_elements = state_dim * (state_dim - 1) // 2\n",
    "        keys = jax.random.split(key, depth + 1)\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(state_dim, width, key=keys[0]),\n",
    "            *[eqx.nn.Linear(width, width, key=key) for key in keys[1:-1]],\n",
    "            eqx.nn.Linear(width, num_unique_elements, key=keys[-1])\n",
    "        ]\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, s):\n",
    "        for layer in self.layers[:-1]:\n",
    "            s = self.activation(layer(s))\n",
    "        upper_triangle_elements = self.layers[-1](s)\n",
    "\n",
    "        # Build the matrix from its upper triangular elements\n",
    "        J = jnp.zeros((self.state_dim, self.state_dim))\n",
    "        triu_indices = jnp.triu_indices(self.state_dim, k=1)\n",
    "        J = J.at[triu_indices].set(upper_triangle_elements)\n",
    "\n",
    "        # Enforce skew-symmetry: J = J - J.T\n",
    "        return J - J.T\n",
    "\n",
    "\n",
    "# --- The Combined Model ---\n",
    "\n",
    "class Combined_sPHNN_PINN(eqx.Module):\n",
    "    \"\"\"Main model combining a unified state predictor and sPHNN structure.\"\"\"\n",
    "    state_net: StateNN\n",
    "    hamiltonian_net: HamiltonianNN\n",
    "    dissipation_net: DissipationNN\n",
    "    j_net: DynamicJ_NN\n",
    "\n",
    "    def __init__(self, key, config):\n",
    "        state_key, h_key, d_key, j_key = jax.random.split(key, 4)\n",
    "        state_dim = config['state_dim']\n",
    "        # The equilibrium point for the normalized error system is the origin.\n",
    "        x0_norm = jnp.zeros(state_dim)\n",
    "\n",
    "        self.state_net = StateNN(key=state_key)\n",
    "        self.hamiltonian_net = HamiltonianNN(\n",
    "            h_key, in_size=state_dim, width=config['h_width'], depth=config['h_depth'],\n",
    "            x0=x0_norm, epsilon=config['h_epsilon']\n",
    "        )\n",
    "        self.dissipation_net = DissipationNN(\n",
    "            d_key, state_dim=state_dim, width=config['d_width'],\n",
    "            depth=config['d_depth'], activation=config['activation']\n",
    "        )\n",
    "        self.j_net = DynamicJ_NN(\n",
    "            j_key, state_dim=state_dim, width=config['j_width'],\n",
    "            depth=config['j_depth'], activation=config['activation']\n",
    "        )\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATA HANDLING\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# def generate_data(file_path=\"data_for_PIN.pkl\"):\n",
    "#     \"\"\"Loads and prepares training data from a pre-generated pickle file.\"\"\"\n",
    "#     print(f\"Loading simulation data from {file_path}...\")\n",
    "#     try:\n",
    "#         with open(file_path, 'rb') as f:\n",
    "#             results = pickle.load(f)\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Error: Data file not found at {file_path}\")\n",
    "#         print(\"Please ensure 'data_for_PIN.pkl' is in the same directory.\")\n",
    "#         return None, None, None, None, None\n",
    "#\n",
    "#     t = jnp.asarray(results['t'])\n",
    "#     s = jnp.vstack([\n",
    "#         results['e_x'], results['e_y'], results['e_z'],\n",
    "#         results['e_u'], results['e_phi']\n",
    "#     ]).T\n",
    "#     q = jnp.vstack([\n",
    "#         results['x1'], results['y1'], results['z1'], results['u1'], results['phi1'],\n",
    "#         results['x2'], results['y2'], results['z2'], results['u2'], results['phi2']\n",
    "#     ]).T\n",
    "#     s_dot_true = jnp.vstack([\n",
    "#         results['d_e_x'], results['d_e_y'], results['d_e_z'],\n",
    "#         results['d_e_u'], results['d_e_phi']\n",
    "#     ]).T\n",
    "#     # Load the analytical Hamiltonian\n",
    "#     H_analytical = jnp.asarray(results['Hamiltonian'])\n",
    "#\n",
    "#     print(\"Data loading complete.\")\n",
    "#     return t, s, q, s_dot_true, H_analytical\n",
    "\n",
    "#\n",
    "# def generate_data(file_path=\"error_system_data.pkl\"):\n",
    "#     \"\"\"\n",
    "#     Loads and prepares training data from a pre-generated pickle file containing\n",
    "#     multiple simulation runs.\n",
    "#     \"\"\"\n",
    "#     print(f\"Loading simulation data from {file_path}...\")\n",
    "#     try:\n",
    "#         with open(file_path, 'rb') as f:\n",
    "#             # The file contains a list of result dictionaries\n",
    "#             all_runs_results = pickle.load(f)\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Error: Data file not found at {file_path}\")\n",
    "#         print(\"Please run 'generate_data_for_PINN.py' to create the data file.\")\n",
    "#         return None, None, None, None, None\n",
    "#\n",
    "#     # Initialize lists to hold data from all runs\n",
    "#     all_t, all_s, all_q, all_s_dot, all_H = [], [], [], [], []\n",
    "#\n",
    "#     # Process each simulation run\n",
    "#     for i, results in enumerate(all_runs_results):\n",
    "#         print(f\"  ... processing run {i + 1}/{len(all_runs_results)}\")\n",
    "#\n",
    "#         # Extract data for the current run\n",
    "#         t = jnp.asarray(results['t'])\n",
    "#         s = jnp.vstack([\n",
    "#             results['e_x'], results['e_y'], results['e_z'],\n",
    "#             results['e_u'], results['e_phi']\n",
    "#         ]).T\n",
    "#         q = jnp.vstack([\n",
    "#             results['x1'], results['y1'], results['z1'], results['u1'], results['phi1'],\n",
    "#             results['x2'], results['y2'], results['z2'], results['u2'], results['phi2']\n",
    "#         ]).T\n",
    "#         s_dot_true = jnp.vstack([\n",
    "#             results['d_e_x'], results['d_e_y'], results['d_e_z'],\n",
    "#             results['d_e_u'], results['d_e_phi']\n",
    "#         ]).T\n",
    "#         H_analytical = jnp.asarray(results['Hamiltonian'])\n",
    "#\n",
    "#         # Append to the main lists\n",
    "#         all_t.append(t)\n",
    "#         all_s.append(s)\n",
    "#         all_q.append(q)\n",
    "#         all_s_dot.append(s_dot_true)\n",
    "#         all_H.append(H_analytical)\n",
    "#\n",
    "#     # Concatenate all runs into single arrays\n",
    "#     final_t = jnp.concatenate(all_t)\n",
    "#     final_s = jnp.concatenate(all_s)\n",
    "#     final_q = jnp.concatenate(all_q)\n",
    "#     final_s_dot = jnp.concatenate(all_s_dot)\n",
    "#     final_H = jnp.concatenate(all_H)\n",
    "#\n",
    "#     print(\"Data loading and aggregation complete.\")\n",
    "#     return final_t, final_s, final_q, final_s_dot, final_H\n",
    "\n",
    "def generate_data(file_path=\"error_system_data.pkl\"):\n",
    "    \"\"\"\n",
    "    Loads and prepares training data from a pre-generated pickle file containing\n",
    "    multiple simulation runs.\n",
    "    \"\"\"\n",
    "    print(f\"Loading simulation data from {file_path}...\")\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            all_runs_results = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        print(\"Please run 'generate_data_for_PINN.py' to create the data file.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    all_t, all_s, all_q, all_s_dot, all_H = [], [], [], [], []\n",
    "    time_offset = 0.0  # Initialize the time offset\n",
    "\n",
    "    for i, results in enumerate(all_runs_results):\n",
    "        print(f\"  ... processing run {i + 1}/{len(all_runs_results)}\")\n",
    "\n",
    "        t = jnp.asarray(results['t'])\n",
    "        s = jnp.vstack([\n",
    "            results['e_x'], results['e_y'], results['e_z'],\n",
    "            results['e_u'], results['e_phi']\n",
    "        ]).T\n",
    "        q = jnp.vstack([\n",
    "            results['x1'], results['y1'], results['z1'], results['u1'], results['phi1'],\n",
    "            results['x2'], results['y2'], results['z2'], results['u2'], results['phi2']\n",
    "        ]).T\n",
    "        s_dot_true = jnp.vstack([\n",
    "            results['d_e_x'], results['d_e_y'], results['d_e_z'],\n",
    "            results['d_e_u'], results['d_e_phi']\n",
    "        ]).T\n",
    "        H_analytical = jnp.asarray(results['Hamiltonian'])\n",
    "\n",
    "        # Append data, adding the current offset to the time vector\n",
    "        all_t.append(t + time_offset)\n",
    "        all_s.append(s)\n",
    "        all_q.append(q)\n",
    "        all_s_dot.append(s_dot_true)\n",
    "        all_H.append(H_analytical)\n",
    "\n",
    "        # Update the offset for the next run to ensure continuity\n",
    "        if t.size > 0:\n",
    "            # Add the duration of the current run to the offset\n",
    "            time_offset += (t[-1] - t[0])\n",
    "\n",
    "    # Concatenate all runs into single arrays\n",
    "    final_t = jnp.concatenate(all_t)\n",
    "    final_s = jnp.concatenate(all_s)\n",
    "    final_q = jnp.concatenate(all_q)\n",
    "    final_s_dot = jnp.concatenate(all_s_dot)\n",
    "    final_H = jnp.concatenate(all_H)\n",
    "\n",
    "    print(\"Data loading and aggregation complete.\")\n",
    "    return final_t, final_s, final_q, final_s_dot, final_H\n",
    "\n",
    "\n",
    "def normalize(data, mean, std):\n",
    "    \"\"\"Normalizes data using pre-computed statistics.\"\"\"\n",
    "    return (data - mean) / (std + 1e-8)\n",
    "\n",
    "\n",
    "def denormalize(data, mean, std):\n",
    "    \"\"\"Denormalizes data using pre-computed statistics.\"\"\"\n",
    "    return data * std + mean\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. TRAINING LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Helper functions for the new physics-based loss terms ---\n",
    "\n",
    "def _alpha(u1, u2, m):\n",
    "    \"\"\"Helper function for the dissipative field f_d.\"\"\"\n",
    "    conds = [\n",
    "        jnp.logical_and(u1 >= 1, jnp.logical_and(u2 > -1, u2 < 1)),\n",
    "        jnp.logical_and(u1 >= 1, u2 <= -1),\n",
    "        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 >= 1),\n",
    "        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), jnp.logical_and(u2 > -1, u2 < 1)),\n",
    "        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 <= -1),\n",
    "        jnp.logical_and(u1 <= -1, u2 >= 1),\n",
    "        jnp.logical_and(u1 <= -1, jnp.logical_and(u2 > -1, u2 < 1)),\n",
    "    ]\n",
    "    choices = [2 * m - 1., -1., -1., 2 * m - 1., -1., -1., 2 * m - 1.]\n",
    "    return jnp.select(conds, choices, default=-1.)\n",
    "\n",
    "\n",
    "def _beta(u1, u2, m):\n",
    "    \"\"\"Helper function for the dissipative field f_d.\"\"\"\n",
    "    conds = [\n",
    "        jnp.logical_and(u1 >= 1, jnp.logical_and(u2 > -1, u2 < 1)),\n",
    "        jnp.logical_and(u1 >= 1, u2 <= -1),\n",
    "        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 >= 1),\n",
    "        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), jnp.logical_and(u2 > -1, u2 < 1)),\n",
    "        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 <= -1),\n",
    "        jnp.logical_and(u1 <= -1, u2 >= 1),\n",
    "        jnp.logical_and(u1 <= -1, jnp.logical_and(u2 > -1, u2 < 1)),\n",
    "    ]\n",
    "    choices = [\n",
    "        2 * m * (u1 - 1), -4 * m, -2 * m * (u1 - 1), 0.,\n",
    "        -2 * m * (u1 + 1), 4 * m, 2 * m * (u1 + 1),\n",
    "    ]\n",
    "    return jnp.select(conds, choices, default=0.)\n",
    "\n",
    "\n",
    "def f_c_fn(e, q, hr_params):\n",
    "    \"\"\"Calculates the conservative vector field f_c(e).\"\"\"\n",
    "    e_x, e_y, e_u, e_phi = e[0], e[1], e[3], e[4]\n",
    "    x1, u1 = q[0], q[3]\n",
    "\n",
    "    k, f, rho, d, r, s = \\\n",
    "        hr_params['k'], hr_params['f'], hr_params['rho'], hr_params['d'], hr_params['r'], hr_params['s']\n",
    "\n",
    "    return jnp.array([\n",
    "        e_y + 2 * k * f * u1 * x1 * e_u + rho * x1 * e_phi,\n",
    "        -2 * d * x1 * e_x,\n",
    "        r * s * e_x,\n",
    "        e_x,\n",
    "        e_x\n",
    "    ])\n",
    "\n",
    "\n",
    "def f_d_fn(e, q, hr_params):\n",
    "    \"\"\"Calculates the dissipative vector field f_d(e).\"\"\"\n",
    "    e_x, e_y, e_z, e_u, e_phi = e[0], e[1], e[2], e[3], e[4]\n",
    "    x1, u1, phi1, u2 = q[0], q[3], q[4], q[8]\n",
    "\n",
    "    a, b, k, h, f, rho, g_e, r, q_param, m = \\\n",
    "        hr_params['a'], hr_params['b'], hr_params['k'], hr_params['h'], \\\n",
    "            hr_params['f'], hr_params['rho'], hr_params['ge'], hr_params['r'], \\\n",
    "            hr_params['q'], hr_params['m']\n",
    "\n",
    "    N_val = -3 * a * x1 ** 2 + 2 * b * x1 + k * h + k * f * u1 ** 2 + rho * phi1 - 2 * g_e\n",
    "    alpha_val = _alpha(u1, u2, m)\n",
    "    beta_val = _beta(u1, u2, m)\n",
    "\n",
    "    return jnp.array([\n",
    "        N_val * e_x,\n",
    "        -e_y,\n",
    "        -r * e_z,\n",
    "        alpha_val * e_u + beta_val,\n",
    "        -q_param * e_phi\n",
    "    ])\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss_fn(model: Combined_sPHNN_PINN, t_batch_norm, s_true_batch_norm, q_true_batch_norm, s_dot_true_batch_norm,\n",
    "            H_true_batch_norm,\n",
    "            lambda_conservative: float, lambda_dissipative: float, lambda_physics: float, hr_params: dict,\n",
    "            t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std):\n",
    "    \"\"\"Calculates the composite data and new physics-based losses.\"\"\"\n",
    "\n",
    "    # --- Part 1: State Prediction and Unified Data Fidelity Loss ---\n",
    "    all_states_pred_norm = jax.vmap(model.state_net)(t_batch_norm)\n",
    "    all_states_true_norm = jnp.concatenate([q_true_batch_norm, s_true_batch_norm], axis=1)\n",
    "    data_loss = jnp.mean((all_states_pred_norm - all_states_true_norm) ** 2)\n",
    "\n",
    "    q_pred_batch_norm = all_states_pred_norm[:, :10]\n",
    "    s_pred_batch_norm = all_states_pred_norm[:, 10:]\n",
    "\n",
    "    s_pred = denormalize(s_pred_batch_norm, s_mean, s_std)\n",
    "    q_pred = denormalize(q_pred_batch_norm, q_mean, q_std)\n",
    "\n",
    "    # --- Part 2: Physics Calculations ---\n",
    "    grad_H_norm_fn = jax.vmap(jax.grad(model.hamiltonian_net))\n",
    "    grad_H_norm = grad_H_norm_fn(s_pred_batch_norm)\n",
    "    grad_H = grad_H_norm / (s_std + 1e-8)\n",
    "\n",
    "    f_c_batch = jax.vmap(f_c_fn, in_axes=(0, 0, None))(s_pred, q_pred, hr_params)\n",
    "    f_d_batch = jax.vmap(f_d_fn, in_axes=(0, 0, None))(s_pred, q_pred, hr_params)\n",
    "\n",
    "    # Calculate s_dot from autodiff\n",
    "    get_autodiff_grad_s_slice = lambda net, t: jax.jvp(lambda t_scalar: net(t_scalar)[10:], (t,), (jnp.ones_like(t),))[\n",
    "        1]\n",
    "    s_dot_autodiff_norm = jax.vmap(get_autodiff_grad_s_slice, in_axes=(None, 0))(model.state_net, t_batch_norm)\n",
    "    s_dot_autodiff = s_dot_autodiff_norm * (s_std / (t_std + 1e-8))\n",
    "\n",
    "    # --- Part 3: Loss Components ---\n",
    "    # Physics Structure Loss (now named 'phys')\n",
    "    J_norm = jax.vmap(model.j_net)(s_pred_batch_norm)\n",
    "    R_norm = jax.vmap(model.dissipation_net)(s_pred_batch_norm)\n",
    "    s_dot_from_structure_norm = jax.vmap(lambda j, r, g: (j - r) @ g)(J_norm, R_norm, grad_H_norm)\n",
    "    s_dot_from_structure = s_dot_from_structure_norm * s_std\n",
    "    s_dot_diss_cons = f_c_batch + f_d_batch\n",
    "    s_dot_true_batch = denormalize(s_dot_true_batch_norm, s_dot_mean, s_dot_std)\n",
    "\n",
    "    # Conservative Loss\n",
    "    lie_derivative = jax.vmap(jnp.dot)(f_c_batch, grad_H)\n",
    "    loss_conservative = jnp.mean(lie_derivative ** 2)\n",
    "\n",
    "    # Dissipative Loss\n",
    "    dHdt_from_autodiff = jax.vmap(jnp.dot)(grad_H, s_dot_autodiff)\n",
    "    dHdt_from_equations = jax.vmap(jnp.dot)(grad_H, f_d_batch)\n",
    "    loss_dissipative = jnp.mean((dHdt_from_autodiff - dHdt_from_equations) ** 2)\n",
    "\n",
    "    # Hamiltonian Loss (for monitoring only)\n",
    "    H_pred_norm = jax.vmap(model.hamiltonian_net)(s_pred_batch_norm)\n",
    "    H_pred = denormalize(H_pred_norm, H_mean, H_std)\n",
    "    H_true = denormalize(H_true_batch_norm, H_mean, H_std)\n",
    "\n",
    "    correlation = jnp.corrcoef(H_true.flatten(), H_pred.flatten())[0, 1]\n",
    "    sign = jnp.sign(correlation)\n",
    "    H_pred_aligned = sign * H_pred - jnp.mean(sign * H_pred) + jnp.mean(H_true)\n",
    "    loss_hamiltonian = jnp.mean((H_pred_aligned - H_true) ** 2)\n",
    "\n",
    "    loss_phys = jnp.mean((s_dot_true_batch - s_dot_from_structure) ** 2)\n",
    "\n",
    "    # --- Part 4: Total Loss ---\n",
    "    state_loss = data_loss + jnp.mean((s_dot_true_batch - s_dot_diss_cons) ** 2)\n",
    "    total_loss = (state_loss\n",
    "                  + (lambda_conservative * loss_conservative)\n",
    "                  + (lambda_dissipative * loss_dissipative)\n",
    "                  + (lambda_physics * loss_phys))\n",
    "\n",
    "    loss_components = {\n",
    "        \"total\": total_loss,\n",
    "        \"data_unified\": data_loss,\n",
    "        \"phys\": loss_phys,\n",
    "        \"conservative\": loss_conservative,\n",
    "        \"dissipative\": loss_dissipative,\n",
    "        \"hamiltonian\": loss_hamiltonian,\n",
    "    }\n",
    "    return total_loss, loss_components\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def train_step(model, opt_state, optimizer, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,\n",
    "               lambda_conservative, lambda_dissipative, lambda_physics, hr_params,\n",
    "               t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std):\n",
    "    \"\"\"Performs a single training step.\"\"\"\n",
    "    (loss_val, loss_components), grads = eqx.filter_value_and_grad(loss_fn, has_aux=True)(\n",
    "        model, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,\n",
    "        lambda_conservative, lambda_dissipative, lambda_physics, hr_params,\n",
    "        t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss_val, loss_components\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def evaluate_model(model, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,\n",
    "                   lambda_conservative, lambda_dissipative, lambda_physics, hr_params,\n",
    "                   t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std):\n",
    "    \"\"\"Calculates the loss for the validation set.\"\"\"\n",
    "    loss_val, _ = loss_fn(\n",
    "        model, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,\n",
    "        lambda_conservative, lambda_dissipative, lambda_physics, hr_params,\n",
    "        t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std\n",
    "    )\n",
    "    return loss_val\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MAIN EXECUTION LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Main function to run the training and evaluation.\"\"\"\n",
    "# --- Setup and Hyperparameters ---\n",
    "key = jax.random.PRNGKey(42)\n",
    "model_key, data_key = jax.random.split(key)\n",
    "\n",
    "# Training hyperparameters\n",
    "# batch_size = 1000\n",
    "# validation_split = 0.2\n",
    "# initial_learning_rate = 1e-3\n",
    "# end_learning_rate = 5e-5\n",
    "# decay_steps = 3000\n",
    "# epochs = 1000\n",
    "\n",
    "batch_size = 20000\n",
    "validation_split = 0.2\n",
    "initial_learning_rate = 1e-3\n",
    "end_learning_rate = 5e-5\n",
    "decay_steps = 3000\n",
    "epochs = 1000\n",
    "\n",
    "# Physics loss hyperparameters with warmup\n",
    "lambda_conservative_max = 1\n",
    "lambda_dissipative_max = 5\n",
    "lambda_physics_max = 15\n",
    "lambda_warmup_epochs = 2000\n",
    "\n",
    "# System parameters\n",
    "hr_params = DEFAULT_PARAMS.copy()\n",
    "\n",
    "# --- Generate and Prepare Data ---\n",
    "import os\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'PINN Data/',\n",
    "                    'error_system_data.pkl')\n",
    "t, s, q, s_dot_true, H_analytical = generate_data(path)\n",
    "if t is None:\n",
    "    sys.exit(\"Exiting: Data loading failed.\")\n",
    "\n",
    "num_samples = s.shape[0]\n",
    "perm = jax.random.permutation(data_key, num_samples)\n",
    "t_shuffled, s_shuffled, q_shuffled, s_dot_shuffled, H_shuffled = t[perm], s[perm], q[perm], s_dot_true[perm], \\\n",
    "H_analytical[perm]\n",
    "t_shuffled = t_shuffled.reshape(-1, 1)\n",
    "\n",
    "split_idx = int(num_samples * (1 - validation_split))\n",
    "t_train, t_val = jnp.split(t_shuffled, [split_idx])\n",
    "s_train, s_val = jnp.split(s_shuffled, [split_idx])\n",
    "q_train, q_val = jnp.split(q_shuffled, [split_idx])\n",
    "s_dot_train, s_dot_val = jnp.split(s_dot_shuffled, [split_idx])\n",
    "H_train, H_val = jnp.split(H_shuffled, [split_idx])\n",
    "\n",
    "# --- Normalize Data (using ONLY training set statistics) ---\n",
    "t_mean, t_std = jnp.mean(t_train), jnp.std(t_train)\n",
    "s_mean, s_std = jnp.mean(s_train, axis=0), jnp.std(s_train, axis=0)\n",
    "q_mean, q_std = jnp.mean(q_train, axis=0), jnp.std(q_train, axis=0)\n",
    "s_dot_mean, s_dot_std = jnp.mean(s_dot_train, axis=0), jnp.std(s_dot_train, axis=0)\n",
    "H_mean, H_std = jnp.mean(H_train), jnp.std(H_train)\n",
    "\n",
    "t_train_norm = normalize(t_train, t_mean, t_std)\n",
    "s_train_norm = normalize(s_train, s_mean, s_std)\n",
    "q_train_norm = normalize(q_train, q_mean, q_std)\n",
    "s_dot_train_norm = normalize(s_dot_train, s_dot_mean, s_dot_std)\n",
    "H_train_norm = normalize(H_train, H_mean, H_std)\n",
    "\n",
    "t_val_norm = normalize(t_val, t_mean, t_std)\n",
    "s_val_norm = normalize(s_val, s_mean, s_std)\n",
    "q_val_norm = normalize(q_val, q_mean, q_std)\n",
    "s_dot_val_norm = normalize(s_dot_val, s_dot_mean, s_dot_std)\n",
    "H_val_norm = normalize(H_val, H_mean, H_std)\n",
    "\n",
    "# --- Centralized Neural Network Configuration ---\n",
    "s_dim = s_train.shape[1]\n",
    "q_dim = q_train.shape[1]\n",
    "nn_config = {\n",
    "    \"state_dim\": s_dim,\n",
    "    \"h_width\": 128, \"h_depth\": 3, \"h_epsilon\": 0.525,\n",
    "    \"d_width\": 2, \"d_depth\": 3,\n",
    "    \"j_width\": 2, \"j_depth\": 3,\n",
    "    \"activation\": jax.nn.softplus,\n",
    "}\n",
    "\n",
    "# Initialize the combined model\n",
    "model = Combined_sPHNN_PINN(key=model_key, config=nn_config)\n",
    "\n",
    "# --- Training Loop ---\n",
    "lr_schedule = optax.linear_schedule(\n",
    "    init_value=initial_learning_rate,\n",
    "    end_value=end_learning_rate,\n",
    "    transition_steps=decay_steps\n",
    ")\n",
    "optimizer = optax.adamw(learning_rate=lr_schedule)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "phys_losses, conservative_losses, dissipative_losses, hamiltonian_losses = [], [], [], []\n",
    "best_model, best_val_loss = model, jnp.inf\n",
    "\n",
    "num_batches = t_train_norm.shape[0] // batch_size\n",
    "if num_batches == 0 and t_train_norm.shape[0] > 0:\n",
    "    print(f\"Warning: batch_size ({batch_size}) > num samples. Setting num_batches to 1.\")\n",
    "    num_batches = 1\n",
    "\n",
    "print(f\"Starting training for {epochs} epochs...\")\n",
    "for epoch in range(epochs):\n",
    "    # Loss weight warmup schedule\n",
    "    warmup_factor = jnp.minimum(1.0, (epoch + 1) / lambda_warmup_epochs)\n",
    "    current_lambda_conservative = lambda_conservative_max * warmup_factor\n",
    "    current_lambda_dissipative = lambda_dissipative_max * warmup_factor\n",
    "    current_lambda_physics = lambda_physics_max * warmup_factor\n",
    "\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perm = jax.random.permutation(shuffle_key, t_train_norm.shape[0])\n",
    "    t_shuffled = t_train_norm[perm]\n",
    "    s_shuffled = s_train_norm[perm]\n",
    "    q_shuffled = q_train_norm[perm]\n",
    "    s_dot_shuffled = s_dot_train_norm[perm]\n",
    "    H_shuffled = H_train_norm[perm]\n",
    "\n",
    "    # Initialize epoch loss accumulators\n",
    "    epoch_losses = {k: 0.0 for k in [\"total\", \"data_unified\", \"phys\", \"conservative\", \"dissipative\", \"hamiltonian\"]}\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start, end = i * batch_size, (i + 1) * batch_size\n",
    "        t_b, s_b, q_b, s_dot_b, H_b = t_shuffled[start:end], s_shuffled[start:end], q_shuffled[\n",
    "                                                                                    start:end], s_dot_shuffled[\n",
    "                                                                                                start:end], H_shuffled[\n",
    "                                                                                                            start:end]\n",
    "\n",
    "        model, opt_state, train_loss_val, loss_comps = train_step(\n",
    "            model, opt_state, optimizer, t_b, s_b, q_b, s_dot_b, H_b,\n",
    "            current_lambda_conservative, current_lambda_dissipative, current_lambda_physics, hr_params,\n",
    "            t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std\n",
    "        )\n",
    "        for k in epoch_losses:\n",
    "            if k in loss_comps:\n",
    "                epoch_losses[k] += loss_comps[k]\n",
    "\n",
    "    # Calculate average losses for the epoch\n",
    "    avg_losses = {k: v / num_batches for k, v in epoch_losses.items()}\n",
    "\n",
    "    val_loss = evaluate_model(\n",
    "        model, t_val_norm, s_val_norm, q_val_norm, s_dot_val_norm, H_val_norm,\n",
    "        current_lambda_conservative, current_lambda_dissipative, current_lambda_physics, hr_params,\n",
    "        t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std\n",
    "    )\n",
    "\n",
    "    train_losses.append(avg_losses[\"total\"])\n",
    "    val_losses.append(val_loss)\n",
    "    phys_losses.append(avg_losses[\"phys\"])\n",
    "    conservative_losses.append(avg_losses[\"conservative\"])\n",
    "    dissipative_losses.append(avg_losses[\"dissipative\"])\n",
    "    hamiltonian_losses.append(avg_losses[\"hamiltonian\"])\n",
    "\n",
    "    '''if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model'''\n",
    "    best_model = model\n",
    "\n",
    "    if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "        log_str = (\n",
    "            f\"Epoch {epoch + 1}/{epochs} | Train Loss: {avg_losses['total']:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Data: {avg_losses['data_unified']:.4f} | \"\n",
    "            f\"Phys: {avg_losses['phys']:.4f} | \"\n",
    "            f\"Cons: {avg_losses['conservative']:.4f} | Diss: {avg_losses['dissipative']:.4f} | \"\n",
    "            f\"H_Loss: {avg_losses['hamiltonian']:.4f}\"\n",
    "        )\n",
    "        print(log_str)\n",
    "\n",
    "print(\"Training finished.\")\n",
    "print(f\"Best validation loss achieved: {best_val_loss:.6f}\")\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# 5. VISUALIZATION AND ANALYSIS\n",
    "# ==============================================================================\n",
    "\n",
    "output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'temp/')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "run_to_visualize_idx = 0  # <-- CHOOSE WHICH SIMULATION RUN TO PLOT\n",
    "\n",
    "print(f\"\\nGenerating visualization plots for simulation run #{run_to_visualize_idx + 1}...\")\n",
    "\n",
    "# Load the data again to isolate a single run for clean plotting\n",
    "with open(path, 'rb') as f:\n",
    "    all_runs = pickle.load(f)\n",
    "\n",
    "# Ensure the chosen index is valid\n",
    "if run_to_visualize_idx >= len(all_runs):\n",
    "    print(\n",
    "        f\"Error: 'run_to_visualize_idx' ({run_to_visualize_idx}) is out of bounds. Max is {len(all_runs) - 1}. Setting to 0.\")\n",
    "    run_to_visualize_idx = 0\n",
    "\n",
    "vis_results = all_runs[run_to_visualize_idx]\n",
    "\n",
    "# Use the selected run's data for all subsequent plotting\n",
    "t_test = jnp.asarray(vis_results['t']).reshape(-1, 1)\n",
    "s_test = jnp.vstack([\n",
    "    vis_results['e_x'], vis_results['e_y'], vis_results['e_z'],\n",
    "    vis_results['e_u'], vis_results['e_phi']\n",
    "]).T\n",
    "q_test = jnp.vstack([\n",
    "    vis_results['x1'], vis_results['y1'], vis_results['z1'], vis_results['u1'], vis_results['phi1'],\n",
    "    vis_results['x2'], vis_results['y2'], vis_results['z2'], vis_results['u2'], vis_results['phi2']\n",
    "]).T\n",
    "s_dot_test = jnp.vstack([\n",
    "    vis_results['d_e_x'], vis_results['d_e_y'], vis_results['d_e_z'],\n",
    "    vis_results['d_e_u'], vis_results['d_e_phi']\n",
    "]).T\n",
    "H_analytical_vis = jnp.asarray(vis_results['Hamiltonian'])\n",
    "\n",
    "# Normalize the visualization data using the previously computed training statistics\n",
    "t_test_norm = normalize(t_test, t_mean, t_std)\n",
    "\n",
    "# --- Get all model predictions for the full dataset ---\n",
    "all_states_pred_norm = jax.vmap(best_model.state_net)(t_test_norm)\n",
    "q_pred_norm = all_states_pred_norm[:, :10]\n",
    "s_pred_norm = all_states_pred_norm[:, 10:]\n",
    "\n",
    "s_pred = denormalize(s_pred_norm, s_mean, s_std)\n",
    "q_pred = denormalize(q_pred_norm, q_mean, q_std)\n",
    "\n",
    "# --- Calculate all derivatives for comparison ---\n",
    "grad_H_norm = jax.vmap(jax.grad(best_model.hamiltonian_net))(s_pred_norm)\n",
    "J_norm = jax.vmap(best_model.j_net)(s_pred_norm)\n",
    "R_norm = jax.vmap(best_model.dissipation_net)(s_pred_norm)\n",
    "s_dot_from_structure_norm = jax.vmap(lambda j, r, g: (j - r) @ g)(J_norm, R_norm, grad_H_norm)\n",
    "s_dot_from_structure = s_dot_from_structure_norm * s_std\n",
    "\n",
    "f_c_batch_vis = jax.vmap(f_c_fn, in_axes=(0, 0, None))(s_pred, q_pred, hr_params)\n",
    "f_d_batch_vis = jax.vmap(f_d_fn, in_axes=(0, 0, None))(s_pred, q_pred, hr_params)\n",
    "s_dot_from_equations = f_c_batch_vis + f_d_batch_vis\n",
    "\n",
    "# Need to compute autodiff for the s-slice of the StateNN's output\n",
    "get_s_slice_autodiff_grad = lambda net, t: jax.jvp(lambda t_scalar: net(t_scalar)[10:], (t,), (jnp.ones_like(t),))[1]\n",
    "s_dot_autodiff_norm = jax.vmap(get_s_slice_autodiff_grad, in_axes=(None, 0))(best_model.state_net, t_test_norm)\n",
    "s_dot_autodiff = s_dot_autodiff_norm * (s_std / (t_std + 1e-8))\n",
    "\n",
    "# --- Plot 1: Learned vs Analytical Hamiltonian ---\n",
    "print(\"Comparing learned Hamiltonian with analytical solution...\")\n",
    "H_learned_norm = jax.vmap(best_model.hamiltonian_net)(s_pred_norm)\n",
    "H_learned_flipped = (-1) * H_learned_norm\n",
    "H_learned_aligned = H_learned_flipped - jnp.mean(H_learned_flipped) + jnp.mean(H_analytical_vis)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(t_test[:1500], H_analytical_vis[:1500], label='Analytical Hamiltonian', color='blue')\n",
    "plt.plot(t_test[:1500], H_learned_aligned[:1500], label='Learned Hamiltonian (Aligned)', color='red')\n",
    "plt.title(\"Time Evolution of Hamiltonians\", fontsize=16)\n",
    "plt.xlabel(\"Time\", fontsize=14)\n",
    "plt.ylabel(\"Hamiltonian Value\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, 'hamiltonian_comparison.png'), dpi=300)\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- Plot 2: Training, Validation, and Physics Losses ---\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(train_losses, label='Total Training Loss')\n",
    "plt.plot(val_losses, label='Total Validation Loss')\n",
    "plt.plot(hamiltonian_losses, label='Hamiltonian Loss', color='red')\n",
    "plt.plot(phys_losses, label='Physics Loss', color='purple')\n",
    "plt.plot(conservative_losses, label='Conservative Loss', alpha=0.7)\n",
    "plt.plot(dissipative_losses, label='Dissipative Loss', alpha=0.7)\n",
    "plt.yscale('log')\n",
    "plt.title('Training, Validation, and Physics Losses Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (Log Scale)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.savefig(os.path.join(output_dir, 'training_losses.png'), dpi=300)\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- Plot 3: Derivative Comparison (Physics Fidelity) ---\n",
    "fig, axes = plt.subplots(s_test.shape[1], 1, figsize=(12, 12), sharex=True)\n",
    "state_labels_s_dot = [r'$\\dot{e}_x$', r'$\\dot{e}_y$', r'$\\dot{e}_z$', r'$\\dot{e}_u$', r'$\\dot{e}_\\phi$']\n",
    "fig.suptitle(\"Derivative Fidelity Comparison\", fontsize=18, y=0.99)\n",
    "\n",
    "for i in range(s_test.shape[1]):\n",
    "    axes[i].plot(t_test[:1500], s_dot_test[:1500, i], label='True Derivative', color='green', linewidth=3, alpha=0.8)\n",
    "    axes[i].plot(t_test[:1500], s_dot_from_structure[:1500, i], label='sPHNN Structure', color='red')\n",
    "    axes[i].plot(t_test[:1500], s_dot_from_equations[:1500, i], label='Analytical Eq. (f_c+f_d)', color='purple')\n",
    "    axes[i].plot(t_test[:1500], s_dot_autodiff[:1500, i], label='Autodiff', color='orange')\n",
    "\n",
    "    axes[i].set_ylabel(state_labels_s_dot[i], fontsize=14)\n",
    "    axes[i].grid(True)\n",
    "    axes[i].legend(loc='upper right')\n",
    "\n",
    "axes[-1].set_xlabel(\"Time\", fontsize=14)\n",
    "fig.savefig(os.path.join(output_dir, 'derivative_fidelity.png'), dpi=300)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "# --- Plot 4: Error System State Trajectories (s) ---\n",
    "fig, axes = plt.subplots(s_test.shape[1], 1, figsize=(12, 10), sharex=True)\n",
    "state_labels_error = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\\phi$']\n",
    "fig.suptitle(\"Error System State 's' Prediction: True vs. Predicted\", fontsize=18, y=0.99)\n",
    "for i in range(s_test.shape[1]):\n",
    "    axes[i].plot(t_test[:1500], s_test[:1500, i], 'b', label='True State', alpha=0.9)\n",
    "    axes[i].plot(t_test[:1500], s_pred[:1500, i], 'r', label='Predicted State')\n",
    "    axes[i].set_ylabel(state_labels_error[i], fontsize=14)\n",
    "    axes[i].grid(True)\n",
    "    axes[i].legend(loc='upper right')\n",
    "axes[-1].set_xlabel(\"Time\", fontsize=14)\n",
    "fig.savefig(os.path.join(output_dir, 'error_state_s_prediction.png'), dpi=300)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "# --- Plot 5: HR System State Trajectories (q) ---\n",
    "fig, axes = plt.subplots(q_test.shape[1], 1, figsize=(12, 18), sharex=True)\n",
    "state_labels_q = [\n",
    "    r'$x_1$', r'$y_1$', r'$z_1$', r'$u_1$', r'$\\phi_1$',\n",
    "    r'$x_2$', r'$y_2$', r'$z_2$', r'$u_2$', r'$\\phi_2$'\n",
    "]\n",
    "fig.suptitle(\"HR System State 'q' Prediction: True vs. Predicted\", fontsize=18, y=0.99)\n",
    "for i in range(q_test.shape[1]):\n",
    "    axes[i].plot(t_test[:1500], q_test[:1500, i], 'b', label='True State', alpha=0.9)\n",
    "    axes[i].plot(t_test[:1500], q_pred[:1500, i], 'r', label='Predicted State')\n",
    "    axes[i].set_ylabel(state_labels_q[i], fontsize=14)\n",
    "    axes[i].grid(True)\n",
    "    axes[i].legend(loc='upper right')\n",
    "axes[-1].set_xlabel(\"Time\", fontsize=14)\n",
    "fig.savefig(os.path.join(output_dir, 'hr_state_q_prediction.png'), dpi=300)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "plt.close('all')\n",
    "print(f\"All plots saved to {output_dir}\")\n"
   ],
   "id": "38f559265dbfb71d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
