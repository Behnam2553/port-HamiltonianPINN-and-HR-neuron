# Project Directory Map: Thesis


================================================================================
## Folder: /
------------------------------------------------------------
### Other Files:
- requirements.txt


================================================================================
## Folder: laboratory
------------------------------------------------------------
### Python Source Files:

#### File: `bifurcation_analysis.py`
```python
import numpy as np
import jax
import diffrax as dfx
jax.config.update("jax_enable_x64", True)
import copy
from src.hr_model.model import HindmarshRose, DEFAULT_PARAMS, DEFAULT_STATE0
from scipy.signal import find_peaks
import multiprocessing as mp
from functools import partial


# ==============================================================================
# WORKER FUNCTION (for multiprocessing)
# ==============================================================================
def run_one_simulation(current_bif_value, model_instance, initial_state,
                       bifurcation_param_name, start_time, end_time, dt_initial,
                       n_points, max_steps, solver, stepsize_controller,
                       transient_fraction, positive_peaks_only):
    """
    Runs a single simulation for one specific bifurcation parameter value.
    This function is designed to be called by a multiprocessing worker.

    Returns:
        tuple: (parameter_value, list_of_peaks)
    """
    # --- Setup Instance ---
    # Create a deep copy to ensure each process has its own model instance
    current_instance = copy.deepcopy(model_instance)

    # --- FIX IS HERE ---
    # Modify the parameter inside the 'params' dictionary directly.
    current_instance.params[bifurcation_param_name] = current_bif_value

    current_instance.initial_state = np.array(initial_state, dtype=np.float64)

    # --- Run Simulation ---
    current_instance.solve(
        solver=solver, t0=start_time, t1=end_time, dt0=dt_initial, n_points=n_points,
        stepsize_controller=stepsize_controller, max_steps=max_steps)

    # --- Process Results ---
    if current_instance.failed:
        return (current_bif_value, [np.nan])

    results = current_instance.get_results_dict(transient_fraction)
    x_curve = results['x1']

    # Find peaks in the steady-state portion
    peak_idx, _ = find_peaks(x_curve, height=0 if positive_peaks_only else None)
    peaks = x_curve[peak_idx]

    # Mask out negative peaks if requested
    if positive_peaks_only:
        positive_mask = peaks > 0
        peaks = peaks[positive_mask]

    # If no peaks are found, return NaN to maintain array structure
    if peaks.size == 0:
        peaks = np.array([np.nan])

    return (current_bif_value, peaks.tolist())


# ==============================================================================
# MAIN EXECUTION
# ==============================================================================
def main():
    """
    Main function to set up and run the parallel bifurcation analysis.
    """
    # --- Simulation Parameters ---
    sim_params = DEFAULT_PARAMS.copy()
    sim_params['rho'] = 0.7
    sim_params['m'] = 1
    model_instance = HindmarshRose(N=1, params=sim_params, initial_state=DEFAULT_STATE0, I_ext=0.8, xi=0)
    bifurcation_param_name = 'k'
    param_range = (-1.3, 0.6, 400)  # Increased steps to show benefit of parallelization

    # --- Integration Settings ---
    start_time = 0
    end_time = 2000
    dt_initial = 0.05
    n_points = int(end_time / dt_initial)
    transient_fraction = 0.5
    max_steps = int((end_time - start_time) / dt_initial) * 20
    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-8, atol=1e-10)

    # --- Parallel Execution ---
    param_start, param_end, param_steps = param_range
    bifurcation_values = np.linspace(param_start, param_end, int(param_steps))
    MAX_WORKERS = mp.cpu_count()
    # MAX_WORKERS = 8

    print(f"Starting parallel bifurcation analysis for '{bifurcation_param_name}'...")
    print(f"Running {param_steps} simulations on {MAX_WORKERS} cores.")

    # Use functools.partial to "freeze" the arguments that are the same for all simulations
    worker_func = partial(run_one_simulation,
                          model_instance=model_instance,
                          initial_state=DEFAULT_STATE0,
                          bifurcation_param_name=bifurcation_param_name,
                          start_time=start_time,
                          end_time=end_time,
                          dt_initial=dt_initial,
                          n_points=n_points,
                          max_steps=max_steps,
                          solver=solver,
                          stepsize_controller=stepsize_controller,
                          transient_fraction=transient_fraction,
                          positive_peaks_only=True)

    # Create a pool of worker processes and map the tasks

    with mp.Pool(processes=MAX_WORKERS) as pool:
        # pool.map will distribute bifurcation_values among the workers
        # and collect the results in a list once all are complete.
        results = pool.map(worker_func, bifurcation_values)

    print("All simulations finished. Processing results...")

    # --- Aggregate Results ---
    all_param_values = []
    all_peak_values = []
    for param_val, peaks in results:
        all_param_values.extend([param_val] * len(peaks))
        all_peak_values.extend(peaks)

    # --- Plotting ---
    from visualization.plotting import plot_bifurcation_diagram

    plot_bifurcation_diagram(
        param_values=all_param_values,
        peak_values=all_peak_values,
        bifurcation_param_name=bifurcation_param_name,
        title="Bifurcation Diagram",
        xlabel=None,
        ylabel=r'$x_{max}$',
        marker='.',
        s=2,
        save_fig=1)


    # # --- Optional: Save Data ---
    # import os
    # path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'Bifurcation/')
    # os.makedirs(path, exist_ok=True)
    #
    # gen_data = np.vstack([all_param_values, all_peak_values]).T
    # np.save(
    #     path + 'Bif_'+bifurcation_param_name+'_k_'+str(sim_params['k'])+'_m_'+str(sim_params['m']),
    #     gen_data)


# This check is essential for multiprocessing to work correctly on all platforms
if __name__ == "__main__":
    # On Windows, the 'spawn' start method is the default and safest.
    # Explicitly setting it prevents issues on systems where 'fork' is the default.
    mp.set_start_method("spawn", force=True)
    main()

```

#### File: `generate_data_for_PINN.py`
```python
import jax
import jax.numpy as jnp
import diffrax as dfx
import pickle
from src.hr_model.error_system import HRNetworkErrorSystem
from src.hr_model.physics import calculate_H
from src.hr_model.physics import calculate_dHdt
from src.hr_model.physics import calculate_dVdt
from src.hr_model.model import DEFAULT_PARAMS

def generate_data(
    num_runs: int,
    dynamics: str = 'complete',
    param_dict: dict = DEFAULT_PARAMS,
    initial_state_range: tuple = (-2, 2),
    seed: int = None,
    start_time: float = 0.0,
    end_time: float = 1000.0,
    dt_initial: float = 0.01,
    n_points: int = 10000,
    transient_ratio: float = 0,
    max_steps: int = None,
    solver = dfx.Tsit5(),
    stepsize_controller = dfx.PIDController(rtol=1e-10, atol=1e-12),
    I_ext = [0.8, 0.8],
    xi = [[0, 1], [1, 0]],
    output_file: str = "error_system_data.pkl"
):
    """
    Generate data by running the Error_System with multiple random initial conditions.
    For each run, compute the Hamiltonian, its time derivative, and the Lyapunov derivative.

    Parameters:
    - num_runs: Number of different initial conditions to simulate.
    - dynamics: 'complete' or 'simplified' dynamics for the error system.
    - param_dict: Dictionary of model parameters.
    - initial_state_range: Tuple (low, high) for uniform random initial states.
    - seed: Random seed for reproducibility.
    - start_time, end_time: Simulation time span.
    - dt_initial: Initial time step for the solver.
    - n_points: Number of points to save in the time series.
    - transient_ratio: Fraction of initial time series to discard as transient.
    - max_steps: Maximum number of steps for the solver.
    - solver: Diffrax solver to use.
    - stepsize_controller: Controller for adaptive step sizing.
    - I_ext: External currents for the HR neurons.
    - xi: Coupling matrix for the HR neurons.
    - output_file: File path to save the results.

    Returns:
    - Saves a list of result dictionaries to the specified output file.
    """
    # Set random seed
    if seed is not None:
        key = jax.random.PRNGKey(seed)
    else:
        key = jax.random.PRNGKey(0)  # Default seed

    # Generate keys for each run
    keys = jax.random.split(key, num_runs)

    # Prepare list to store results
    all_results = []

    # Compute max_steps if not provided
    if max_steps is None:
        max_steps = int((end_time - start_time) / dt_initial) * 20

    # Time points for saving
    t_save = jnp.linspace(start_time, end_time, n_points)

    for i in range(num_runs):
        print(f"Running simulation {i+1}/{num_runs}...")

        # Generate random hr_initial_state
        low, high = initial_state_range
        hr_initial_state = jax.random.uniform(keys[i], shape=(10,), minval=low, maxval=high)

        # Create simulator instance
        simulator = HRNetworkErrorSystem(
            params=param_dict,
            dynamics=dynamics,
            hr_initial_state=hr_initial_state,
            I_ext=I_ext,
            hr_xi=xi
        )

        # Run simulation
        simulator.solve(
            solver=solver,
            t0=start_time,
            t1=end_time,
            dt0=dt_initial,
            n_points=dfx.SaveAt(ts=t_save, dense=True),
            stepsize_controller=stepsize_controller,
            max_steps=max_steps
        )

        if not simulator.failed:
            results = simulator.get_results_dict(transient_ratio)

            # Compute Hamiltonian, dHdt, dVdt
            H = calculate_H(results, param_dict)
            dHdt = calculate_dHdt(results, param_dict)
            dVdt = calculate_dVdt(results, param_dict)

            # Add to results
            results['Hamiltonian'] = H
            results['dHdt'] = dHdt
            results['dVdt'] = dVdt

            # Also store initial state
            results['initial_state'] = hr_initial_state

            all_results.append(results)
        else:
            print(f"Simulation {i+1} failed.")

    # Save all results to file
    with open(output_file, 'wb') as f:
        pickle.dump(all_results, f)

    print(f"Data generation complete. Results saved to {output_file}")



if __name__ == '__main__':
    from visualization.plotting import plot_pinn_data

    sim_params = DEFAULT_PARAMS.copy()
    sim_params['ge'] = 0.62

    import os
    output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'PINN Data/')
    os.makedirs(output_dir, exist_ok=True)

    output_file = os.path.join(output_dir, f'error_system_data.pkl')
    # Generate data for 5 runs
    generate_data(num_runs=1,
                  dynamics = 'complete',
                  param_dict = sim_params,
                  initial_state_range = (-1, 1),
                  end_time = 1000,
                  n_points = 10000,
                  seed=55,
                  output_file=output_file
                  )

    # Load the generated data
    with open(output_file, 'rb') as f:
        all_results = pickle.load(f)


    # Define variables to plot and their titles
    variables_to_plot = ['e_x', 'e_y', 'e_z', 'e_u', 'e_phi', 'Hamiltonian', 'dHdt', 'dVdt']
    titles = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$', r'$H$', r'$dH/dt$', r'$dV/dt$']

    # Use the imported plotting function to visualize the results
    plot_pinn_data(all_results, save_fig=True)

    print(f"Generated 5 separate plots, each with 8 subplots, saved as 'run_<number>_plots.png'")

```

#### File: `run_loop.py`
```python
import subprocess, time, sys, pathlib

CMD = [sys.executable, "v_dot_parameter_sweep_2D.py"]
# CMD = [r"C:\Virtual Environments\Neuron\.venv\Scripts\python.exe", "v_dot_parameter_sweep_2D.py"]


while True:
    print("\n=== Starting simulation at", time.ctime())
    exit_code = subprocess.call(CMD)
    if exit_code == 0:
        print("=== Simulation finished successfully!")
        break
    print(f"*** Child exited with code {exit_code}.  Restarting in 15 s …")
    time.sleep(5)

```

#### File: `v_dot_parameter_sweep_1D.py`
```python

"""
v_dot_parameter_sweep_1D.py
------------------------------------------
Sweep a chosen Hindmarsh-Rose model parameter, simulate for each value,
compute the mean post-transient dV/dt, and plot the result … in parallel.
"""

from __future__ import annotations
import os
import numpy as np
import jax, jax.numpy as jnp
import diffrax as dfx
jax.config.update("jax_enable_x64", True)
import multiprocessing as mp
# ── extra imports (2-D features) ─────────────────────────────────────────
import gc
from typing import Any
# ─────────────────────────────────────────────────────────────────────────
from src.hr_model.error_system import HRNetworkErrorSystem
from src.hr_model.physics import calculate_dHdt
from src.hr_model.physics import calculate_dVdt
from src.hr_model.model import DEFAULT_PARAMS


# ───────────────────────────────────────────────────────────────────────
# CONFIGURATION
# ───────────────────────────────────────────────────────────────────────
TARGET_PARAM = "k"  # name of the parameter to sweep
PARAM_MIN, PARAM_MAX = -2.5, 2
NUM_PARAM_POINTS = 5
PARAM_VALUES = jnp.linspace(PARAM_MIN, PARAM_MAX, NUM_PARAM_POINTS)

# ── extra config copied from 2-D script ────────────────────────────────
CORE_NUM   = 5          # number of worker processes
BATCH_SIZE = 5          # params per checkpoint-batch
# ───────────────────────────────────────────────────────────────────────

# integration settings
START_TIME = 0
END_TIME = 250
DT_INITIAL = 0.01
POINT_NUM = 250
TRANSIENT_RATIO = 0.75
N_POINTS = dfx.SaveAt(ts=jnp.linspace(START_TIME, END_TIME, POINT_NUM), dense=True)
MAX_STEPS = int((END_TIME - START_TIME) / DT_INITIAL) * 20
SOLVER = dfx.Tsit5()
STEPSIZE_CONTROLLER = dfx.PIDController(rtol=1e-10, atol=1e-12)
# stepsize_controller = dfx.ConstantStepSize()

# Initial conditions (10 HR-state variables + 5 error-state variables)
INITIAL_HR_STATE0 = [
    0.1, 0.2, 0.3, 0.4, 0.1,    # neuron1
    0.2, 0.3, 0.4, 0.5, 0.2     # neuron2
]

I_EXT = [0.8, 0.8]  # external currents
XI = [[0, 1], [1, 0]]  # electrical coupling

# where to save results (root directory)
SAVE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'V_H_DOT_1D/')
os.makedirs(SAVE_DIR, exist_ok=True)


# ───────────────────────────────────────────────────────────────────────
# WORKER FUNCTION (runs in each process)
# ───────────────────────────────────────────────────────────────────────
def run_one_param(param: float) -> float:
    """Simulate for a single parameter value and return mean post-transient dV/dt."""
    # 1– copy default parameters and set the swept one
    current_params = DEFAULT_PARAMS.copy()
    current_params[TARGET_PARAM] = param

    # 2– create simulator
    simulator = HRNetworkErrorSystem(
        params=current_params,
        dynamics='simplified',
        hr_initial_state=INITIAL_HR_STATE0,
        I_ext=I_EXT,
        hr_xi=XI)

    # 3– integrate
    simulator.solve(
        solver=SOLVER,
        t0=START_TIME,
        t1=END_TIME,
        dt0=DT_INITIAL,
        n_points=N_POINTS,
        stepsize_controller=STEPSIZE_CONTROLLER,
        max_steps=MAX_STEPS
    )

    # 4– analyse
    if not simulator.failed:
        results_dict = simulator.get_results_dict(TRANSIENT_RATIO)
        dVdt_timeseries = calculate_dVdt(results_dict, current_params)
        dHdt_timeseries = calculate_dHdt(results_dict, current_params)

        final_result = [float(jnp.nanmean(dVdt_timeseries)), float(jnp.nanmean(dHdt_timeseries))]

        # --- AGGRESSIVE CLEANUP ---
        del simulator, results_dict, dVdt_timeseries, dHdt_timeseries, current_params
        jax.clear_caches()
        gc.collect()

        return final_result
    else:
        # --- AGGRESSIVE CLEANUP (for the failure case too) ---
        del simulator, current_params
        jax.clear_caches()
        gc.collect()
        return [np.nan, np.nan]

# ───────────────────────────────────────────────────────────────────────
# MAIN DRIVER
# ───────────────────────────────────────────────────────────────────────
def main() -> None:
    print(f"Starting parallel simulation loop for {TARGET_PARAM} "
          f"from {PARAM_MIN} to {PARAM_MAX}…")

    # ── new resume / checkpoint logic ────────────────────────────────────
    outfile = os.path.join(
        SAVE_DIR, f"V_H_DOT_{TARGET_PARAM}_{PARAM_MIN}_{PARAM_MAX}.npz"
    )

    if os.path.exists(outfile):
        print(f"--- Found existing results file: {outfile}")
        with np.load(outfile) as data:
            if data["mean_dVdt"].shape == (NUM_PARAM_POINTS,):
                mean_dVdt = data["mean_dVdt"].copy()
                mean_dHdt = data["mean_dHdt"].copy()
                print("--- Resuming previous sweep")
            else:
                print("--- Shape mismatch – starting fresh run")
                mean_dVdt = np.full(NUM_PARAM_POINTS, np.nan, dtype=np.float64)
                mean_dHdt = np.full(NUM_PARAM_POINTS, np.nan, dtype=np.float64)
    else:
        print("--- No checkpoint found – starting fresh run")
        mean_dVdt = np.full(NUM_PARAM_POINTS, np.nan, dtype=np.float64)
        mean_dHdt = np.full(NUM_PARAM_POINTS, np.nan, dtype=np.float64)

    # build todo list
    tasks: list[dict[str, Any]] = []
    for idx, p in enumerate(PARAM_VALUES):
        if np.isnan(mean_dVdt[idx]) or np.isnan(mean_dHdt[idx]):
            tasks.append({"param": p, "index": idx})

    if not tasks:
        print("--- All simulations already complete! ---")
        return mean_dVdt, mean_dHdt

    print(f"Total simulations to run: {len(tasks)}")

    # ── batch execution with RAM-safe pool ───────────────────────────────
    with mp.Pool(processes=CORE_NUM, maxtasksperchild=1) as pool:
        for b0 in range(0, len(tasks), BATCH_SIZE):
            batch      = tasks[b0:b0+BATCH_SIZE]
            params     = [d["param"]  for d in batch]
            indices    = [d["index"]  for d in batch]

            print(f"\n--- Batch {b0//BATCH_SIZE + 1}/{-(-len(tasks)//BATCH_SIZE)} "
                  f"({b0+1}–{b0+len(batch)}) ---")

            results    = pool.map(run_one_param, params)

            for (dV, dH), idx in zip(results, indices):
                mean_dVdt[idx] = dV
                mean_dHdt[idx] = dH

            # save checkpoint every batch
            print("--- Saving checkpoint ---")
            np.savez_compressed(
                outfile,
                PARAM_VALUES=PARAM_VALUES,
                mean_dVdt=mean_dVdt,
                mean_dHdt=mean_dHdt,
                TARGET_PARAM=TARGET_PARAM
            )

    print("\nSimulation done")
    print("Saved on a file")

    return mean_dVdt, mean_dHdt




# ───────────────────────────────────────────────────────────────────────
# ENTRY POINT  (required for multiprocessing on Windows & macOS)
# ───────────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    # On Windows the default start-method is "spawn" already.
    # Explicitly setting it once avoids accidental "fork" on some *nix installs.
    mp.freeze_support()             # makes scripts double-clickable on Windows
    mp.set_start_method("spawn", force=True)
    mean_dVdt, mean_dHdt = main()

    # ── Plot ───────────────────────────────────────────────────────────
    from visualization.plotting import plot_v_h_dot_1d

    # Ensure the simulation ran and returned valid data before plotting
    if mean_dVdt is not None and mean_dHdt is not None:
        print("Generating plot...")

        # # Define the output path for the plot, same as the data file but with .png
        # import os
        # plot_filename = os.path.join(
        #     SAVE_DIR, f"V_H_DOT_{TARGET_PARAM}_{PARAM_MIN}_{PARAM_MAX}.png"
        # )

        plot_v_h_dot_1d(
            param_values=PARAM_VALUES,
            mean_dvdt=mean_dVdt,
            mean_dhdt=mean_dHdt,
            param_name=TARGET_PARAM,
            title=f"Mean Derivatives vs. Parameter '{TARGET_PARAM}'",
            save_fig=False
        )



```

#### File: `v_dot_parameter_sweep_2D.py`
```python

"""
v_dot_parameter_sweep_2D.py
--------------------------------------------
Sweep two Hindmarsh‑Rose model parameters, simulate for each pair,
compute the mean post‑transient dV/dt, and plot the result as a colour map.
"""

from __future__ import annotations
import os
import numpy as np
import jax, jax.numpy as jnp
import diffrax as dfx
jax.config.update("jax_enable_x64", True)
import multiprocessing as mp
import gc
from typing import Any
from src.hr_model.error_system import HRNetworkErrorSystem
from src.hr_model.physics import calculate_dHdt
from src.hr_model.physics import calculate_dVdt
from src.hr_model.model import DEFAULT_PARAMS


# ───────────────────────────────────────────────────────────────────────
# CONFIGURATION
# ───────────────────────────────────────────────────────────────────────
# ‑‑ choose the two parameters to sweep ‑‑
PARAM_X, PARAM_Y = "ge", "k"  # ← change names as needed

# ranges (inclusive)
X_MIN, X_MAX, NX = 0, 1, 3  # 11 points → 0.3 … 0.9 step 0.06
Y_MIN, Y_MAX, NY = -2.5, 2, 3  # 9  points → 0.1 … 0.5 step 0.05
x_vals = jnp.linspace(X_MIN, X_MAX, NX)
y_vals = jnp.linspace(Y_MIN, Y_MAX, NY)

# Multiprocessing params
CORE_NUM = 3
BATCH_SIZE = 9

# integration settings
START_TIME = 0
END_TIME = 250
DT_INITIAL = 0.01
POINT_NUM = 250
TRANSIENT_RATIO = 0.75
N_POINTS = dfx.SaveAt(ts=jnp.linspace(START_TIME, END_TIME, POINT_NUM), dense=True)
MAX_STEPS = int((END_TIME - START_TIME) / DT_INITIAL) * 20
SOLVER = dfx.Tsit5()
STEPSIZE_CONTROLLER = dfx.PIDController(rtol=1e-10, atol=1e-12)
# stepsize_controller = dfx.ConstantStepSize()

# Initial conditions (10 HR‑state variables + 5 error‑state variables)
INITIAL_HR_STATE0 = [
    0.1, 0.2, 0.3, 0.4, 0.1,    # neuron1
    0.2, 0.3, 0.4, 0.5, 0.2     # neuron2
]

I_EXT = [0.8, 0.8]  # external currents
XI = [[0, 1], [1, 0]]  # electrical coupling

# where to save results (root directory)
SAVE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'V_H_DOT_2D/')
os.makedirs(SAVE_DIR, exist_ok=True)

# ───────────────────────────────────────────────────────────────────────
# WORKER FUNCTION
# ───────────────────────────────────────────────────────────────────────
def run_one_pair(params: tuple[float, float]) -> float:
    """Return mean post‑transient dV/dt for (x, y) parameter pair."""
    # 1– copy default parameters and set the swept one
    x_val, y_val = params
    current_params = DEFAULT_PARAMS.copy()
    current_params[PARAM_X] = x_val
    current_params[PARAM_Y] = y_val

    # 2– create simulator
    simulator = HRNetworkErrorSystem(
        params=current_params,
        dynamics='simplified',
        hr_initial_state=INITIAL_HR_STATE0,
        I_ext=I_EXT,
        hr_xi=XI)

    # 3– integrate
    # print('param pair: ',[x_val, y_val])
    simulator.solve(
        solver=SOLVER,
        t0=START_TIME,
        t1=END_TIME,
        dt0=DT_INITIAL,
        n_points=N_POINTS,
        stepsize_controller=STEPSIZE_CONTROLLER,
        max_steps=MAX_STEPS
    )

    # 4– analyse
    if not simulator.failed:
        results_dict = simulator.get_results_dict(TRANSIENT_RATIO)
        dVdt_timeseries = calculate_dVdt(results_dict, current_params)
        dHdt_timeseries = calculate_dHdt(results_dict, current_params)

        final_result = [float(jnp.nanmean(dVdt_timeseries)), float(jnp.nanmean(dHdt_timeseries))]

        # --- AGGRESSIVE CLEANUP ---
        del simulator, results_dict, dVdt_timeseries, dHdt_timeseries, current_params
        jax.clear_caches()
        gc.collect()

        return final_result
    else:
        # --- AGGRESSIVE CLEANUP (for the failure case too) ---
        del simulator, current_params
        jax.clear_caches()
        gc.collect()
        return [np.nan, np.nan]

# ───────────────────────────────────────────────────────────────────────
# MAIN DRIVER
# ───────────────────────────────────────────────────────────────────────
def main() -> tuple[np.ndarray, np.ndarray]:
    """
    Sweep PARAM_X × PARAM_Y, resume from partial runs, and keep RAM bounded.

    Returns
    -------
    (mean_dVdt, mean_dHdt)  – both shape (NX, NY)
    """
    # ── 1. Where to store results ──────────────────────────────────────────
    output_filename = os.path.join(
        SAVE_DIR,
        f"VDOT_{PARAM_X}_{X_MIN}-{X_MAX}_{PARAM_Y}_{Y_MIN}-{Y_MAX}.npz"
    )

    # ── 2. Load / initialise result arrays ─────────────────────────────────
    if os.path.exists(output_filename):
        print(f"--- Found existing results file: {output_filename}")
        with np.load(output_filename) as data:
            if data["mean_dVdt"].shape == (NX, NY):
                # .copy() ⇒ writable even if the NPZ is mem-mapped read-only
                mean_dVdt = data["mean_dVdt"].copy()
                mean_dHdt = data["mean_dHdt"].copy()
                print("--- Resuming previous sweep")
            else:
                print("--- Shape mismatch – starting fresh run")
                mean_dVdt = np.full((NX, NY), np.nan, dtype=np.float64)
                mean_dHdt = np.full((NX, NY), np.nan, dtype=np.float64)
    else:
        print("--- No checkpoint found – starting fresh run")
        mean_dVdt = np.full((NX, NY), np.nan, dtype=np.float64)
        mean_dHdt = np.full((NX, NY), np.nan, dtype=np.float64)

    # ── 3. Build todo list (need *both* matrices filled) ───────────────────
    tasks_to_do: list[dict[str, Any]] = []
    for i in range(NX):           # rows  → PARAM_X
        for j in range(NY):       # cols  → PARAM_Y
            if np.isnan(mean_dVdt[i, j]) or np.isnan(mean_dHdt[i, j]):
                tasks_to_do.append({"params": (x_vals[i], y_vals[j]),
                                   "indices": (i, j)})

    if not tasks_to_do:
        print("--- All simulations already complete! ---")
        return mean_dVdt, mean_dHdt

    print(f"Total simulations to run: {len(tasks_to_do)}")

    # ── 4. Batch-wise execution with ONE persistent worker pool ────────────
    with mp.Pool(processes=CORE_NUM, maxtasksperchild=1) as pool:
        for batch_no, start in enumerate(range(0, len(tasks_to_do), BATCH_SIZE), 1):
            current_batch  = tasks_to_do[start:start + BATCH_SIZE]
            params_batch   = [task["params"]   for task in current_batch]
            indices_batch  = [task["indices"]  for task in current_batch]

            print(f"\n--- Batch {batch_no}/{int(np.ceil(len(tasks_to_do)/BATCH_SIZE))} "
                  f"({start+1}–{min(start+BATCH_SIZE, len(tasks_to_do))}) ---")

            results_batch = pool.map(run_one_pair, params_batch)

            # insert into the big matrices
            for (dV, dH), (row, col) in zip(results_batch, indices_batch):
                mean_dVdt[row, col] = dV
                mean_dHdt[row, col] = dH

            # ── 5. Checkpoint after every batch ────────────────────────────
            print("--- Saving checkpoint ---")
            np.savez_compressed(
                output_filename,
                x_vals=x_vals,
                y_vals=y_vals,
                mean_dVdt=mean_dVdt,
                mean_dHdt=mean_dHdt,
                PARAM_X=PARAM_X,
                PARAM_Y=PARAM_Y
            )

    print("\n--- All simulations finished successfully! ---")
    return mean_dVdt, mean_dHdt



# ───────────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    mp.freeze_support()
    mp.set_start_method("spawn", force=True)
    mean_dVdt, mean_dHdt = main()

    # ── Plot colour map ────────────────────────────────────────────────
    from visualization.plotting import plot_v_h_dot_2d


    # Ensure the simulation ran and returned valid data before plotting
    if mean_dVdt is not None and mean_dHdt is not None:
        print("Generating plots...")

        # # Define the output path for the plots, using the data filename as a base
        # import os
        # plot_path_prefix = os.path.join(
        #     SAVE_DIR,
        #     f"VDOT_{PARAM_X}_{X_MIN}-{X_MAX}_{PARAM_Y}_{Y_MIN}-{Y_MAX}"
        # )

        plot_v_h_dot_2d(
            x_vals=x_vals,
            y_vals=y_vals,
            mean_dvdt=mean_dVdt,
            mean_dhdt=mean_dHdt,
            param_x_name=PARAM_X,
            param_y_name=PARAM_Y,
            save_fig=False
        )
```

================================================================================
## Folder: utils
------------------------------------------------------------

================================================================================
## Folder: visualization
------------------------------------------------------------
### Python Source Files:

#### File: `plotting.py`
```python
import os
import numpy as np
import matplotlib.pyplot as plt
# import matplotlib
# matplotlib.use('Qt5agg')


def plot_all_time_series(results, N, title="Hindmarsh-Rose Neuron States", save_fig=False):
    """
    Plots the time series of all state variables for each neuron in a grid.

    Args:
        results (dict): The dictionary of simulation results from get_results_dict.
        N (int): The number of neurons in the simulation.
        title (str): The main title for the entire figure.
        save_fig (bool): If True, saves the figure to a file. Otherwise, shows it interactively.
    """
    states = ['x', 'y', 'z', 'u', 'phi']
    state_labels = [r'$x$', r'$y$', r'$z$', r'$u$', r'$\phi$']

    fig, axes = plt.subplots(5, N, figsize=(4 * N, 10), sharex=True, squeeze=False, dpi=300)
    fig.suptitle(title, fontsize=16)

    for i, (state_var, state_label) in enumerate(zip(states, state_labels)):
        for j in range(N):
            ax = axes[i, j]
            data_key = f'{state_var}{j + 1}'

            if data_key in results:
                ax.plot(results['t'], results[data_key])
                ax.grid(True, linestyle='--', alpha=0.6)

            if j == 0:
                ax.set_ylabel(state_label, fontsize=14)

            if i == 0:
                ax.set_title(f'Neuron {j + 1}')

    plt.xlabel('Time', fontsize=14)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        filename = title.replace(' ', '_').replace(':', '') + '.png'
        save_path = os.path.join(output_dir, filename)
        plt.savefig(save_path, dpi=300)
        plt.close(fig)
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_error_and_state_differences(results, title="Error System and State Difference Dynamics", save_fig=False):
    """
    Plots both the error system states (e.g., e_x) and the direct state
    differences (e.g., x2-x1) on the same subplots for comparison.
    Args:
        results (dict): The dictionary of simulation results.
        title (str): The main title for the figure.
        save_fig (bool): If True, saves the figure to a file. Otherwise, shows it interactively.
    """
    states = ['x', 'y', 'z', 'u', 'phi']
    error_labels = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$']
    diff_labels = [r'$x_2 - x_1$', r'$y_2 - y_1$', r'$z_2 - z_1$', r'$u_2 - u_1$', r'$\phi_2 - \phi_1$']

    fig, axes = plt.subplots(len(states), 1, figsize=(12, 15), sharex=True)
    fig.suptitle(title, fontsize=16)

    for i, state_var in enumerate(states):
        ax = axes[i]
        error_key = f'e_{state_var}'
        if error_key in results:
            ax.plot(results['t'], results[error_key], label=error_labels[i])

        key1 = f'{state_var}1'
        key2 = f'{state_var}2'
        if key1 in results and key2 in results:
            difference = results[key2] - results[key1]
            ax.plot(results['t'], difference, label=diff_labels[i], alpha=0.8)

        ax.set_ylabel(f"State '{state_var}'", fontsize=14)
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.legend(loc='upper right')

    plt.xlabel('Time', fontsize=14)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        filename = title.replace(' ', '_').replace(':', '') + '.png'
        save_path = os.path.join(output_dir, filename)
        plt.savefig(save_path, dpi=300)
        plt.close(fig)
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_bifurcation_diagram(param_values, peak_values, bifurcation_param_name, title="Bifurcation Diagram",
                             xlabel=None, ylabel=r'$x_{max}$', marker='.', s=1, save_fig=False):
    """Plots a bifurcation diagram."""

    if xlabel is None:
        greek_letters = {'rho': r'$\rho$', 'alpha': r'$\alpha$', 'beta': r'$\beta$', 'gamma': r'$\gamma$'}
        xlabel = greek_letters.get(bifurcation_param_name.lower(), bifurcation_param_name)

    plt.figure(layout='tight')
    plt.scatter(param_values, peak_values, marker=marker, s=s, alpha=0.8)

    plt.xlabel(xlabel, fontsize=14)
    plt.ylabel(ylabel, fontsize=14)
    plt.title(f"{title} ({bifurcation_param_name})", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        filename = f"bifurcation_{bifurcation_param_name}.png"
        save_path = os.path.join(output_dir, filename)
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_hamiltonian(t, H, save_fig=False):
    """Plots the Hamiltonian (H)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, H, label='Hamiltonian (H)')
    plt.xlabel('Time')
    plt.ylabel('H')
    plt.title('Hamiltonian vs. Time')
    plt.grid(True)
    plt.legend()

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, 'hamiltonian_vs_time.png')
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_hamiltonian_derivative(t, dHdt, save_fig=False):
    """Plots the Hamiltonian derivative (dH/dt)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, dHdt, label='dH/dt', color='orange')
    plt.xlabel('Time')
    plt.ylabel('dH/dt')
    plt.title('Hamiltonian Derivative vs. Time')
    plt.grid(True)
    plt.legend()

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, 'dHdt_vs_time.png')
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_lyapunov_derivative(t, dVdt, save_fig=False):
    """Plots the Lyapunov derivative (dV/dt)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, dVdt, label='dV/dt', color='green')
    plt.xlabel('Time')
    plt.ylabel('dV/dt')
    plt.title('Lyapunov Derivative vs. Time')
    plt.grid(True)
    plt.legend()

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, 'dVdt_vs_time.png')
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_pinn_data(all_results, save_fig=False):
    """
    Plots the results from the data generation for the PINN.
    Creates a separate figure for each simulation run, with subplots for key variables.
    Args:
        all_results (list): A list of result dictionaries for each run.
        save_fig (bool): If True, saves the figures to files. Otherwise, shows them interactively.
    """
    variables_to_plot = ['e_x', 'e_y', 'e_z', 'e_u', 'e_phi', 'Hamiltonian', 'dHdt', 'dVdt']
    titles = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$', r'$H$', r'$dH/dt$', r'$dV/dt$']

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)

    for run_idx, results in enumerate(all_results):
        t = results['t']
        fig, axes = plt.subplots(4, 2, figsize=(12, 8), sharex=True)
        fig.suptitle(f'Run {run_idx + 1}', fontsize=16)

        for ax, var, title in zip(axes.flat, variables_to_plot, titles):
            ax.plot(t, results[var])
            ax.set_xlabel('t')
            ax.set_ylabel(title)
            ax.grid(True)

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])

        if save_fig:
            filename = f'pinn_data_run_{run_idx + 1}.png'
            save_path = os.path.join(output_dir, filename)
            plt.savefig(save_path, dpi=300)
            plt.close(fig)
        else:
            plt.show(block=True)


    if save_fig:
        print(f"All PINN data plots saved to {output_dir}")


def plot_v_h_dot_1d(param_values, mean_dvdt, mean_dhdt, param_name, title, save_fig=False):
    """
    Plots the mean dV/dt and dH/dt against a single parameter.
    Args:
        param_values (np.ndarray): The values of the swept parameter.
        mean_dvdt (np.ndarray): The corresponding mean dV/dt values.
        mean_dhdt (np.ndarray): The corresponding mean dH/dt values.
        param_name (str): The name of the parameter (e.g., 'k', 'rho').
        title (str): The title for the plot.
        save_fig (bool): If True, saves the figure to a file. Otherwise, shows it interactively.
    """
    fig, ax = plt.subplots(constrained_layout=True)

    ax.plot(param_values, mean_dvdt, color="blue", label=r'$\langle dV/dt \rangle$', marker='.', linestyle='-')
    ax.plot(param_values, mean_dhdt, color="red", label=r'$\langle dH/dt \rangle$', marker='.', linestyle='-')

    greek_letters = {'rho': r'$\rho$', 'alpha': r'$\alpha$', 'k': r'$k$'}
    xlabel = greek_letters.get(param_name.lower(), param_name)

    ax.set_xlabel(xlabel, fontsize=18)
    ax.set_ylabel(r'Mean Value', fontsize=18)
    ax.set_title(title, fontsize=20)
    ax.grid(True)
    ax.legend(loc="best", fontsize=14)
    ax.set_box_aspect(1.0)
    ax.margins(x=0)

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        filename = f'v_h_dot_vs_{param_name}.png'
        save_path = os.path.join(output_dir, filename)
        fig.savefig(save_path, dpi=300)
        plt.close(fig)
        print(f"Plot saved to {save_path}")
    else:
        plt.show(block=True)


def plot_v_h_dot_2d(x_vals, y_vals, mean_dvdt, mean_dhdt, param_x_name, param_y_name, save_fig=False):
    """
    Plots 2D heatmaps for mean dV/dt and dH/dt from a parameter sweep.
    Generates and saves two separate figures.

    Args:
        x_vals (np.ndarray): Array of values for the parameter on the y-axis.
        y_vals (np.ndarray): Array of values for the parameter on the x-axis.
        mean_dvdt (np.ndarray): 2D array of mean dV/dt values.
        mean_dhdt (np.ndarray): 2D array of mean dH/dt values.
        param_x_name (str): Name of the parameter for the y-axis.
        param_y_name (str): Name of the parameter for the x-axis.
        save_fig (bool): If True, saves the plots to files. Otherwise, shows them interactively.
    """
    Xg, Yg = np.meshgrid(y_vals, x_vals)
    greek_map = {'ge': r'$g_e$', 'rho': r'$\rho$', 'k': r'$k$', 'm': r'$m$'}
    xlabel = greek_map.get(param_y_name.lower(), param_y_name)
    ylabel = greek_map.get(param_x_name.lower(), param_x_name)

    def five_ticks(grid):
        mn, mx = np.nanmin(grid), np.nanmax(grid)
        return np.round(np.linspace(mn, mx, 5), 2)

    # --- Plot 1: dV/dt ---
    fig_dv, ax_dv = plt.subplots(constrained_layout=True)
    pcm1 = ax_dv.pcolormesh(Xg, Yg, mean_dvdt, shading="auto", cmap='viridis')
    ax_dv.set_xlabel(xlabel, fontsize=14)
    ax_dv.set_ylabel(ylabel, fontsize=14)
    ax_dv.set_title(r"Mean $\langle dV/dt \rangle$ (post-transient)", fontsize=16)
    ax_dv.set_xticks(five_ticks(y_vals))
    ax_dv.set_yticks(five_ticks(x_vals))
    cbar1 = fig_dv.colorbar(pcm1, ax=ax_dv)
    cbar1.set_label(r'$\langle dV/dt \rangle$', fontsize=14)

    # --- Plot 2: dH/dt ---
    fig_dh, ax_dh = plt.subplots(constrained_layout=True)
    pcm2 = ax_dh.pcolormesh(Xg, Yg, mean_dhdt, shading="auto", cmap='inferno')
    ax_dh.set_xlabel(xlabel, fontsize=14)
    ax_dh.set_ylabel(ylabel, fontsize=14)
    ax_dh.set_title(r"Mean $\langle dH/dt \rangle$ (post-transient)", fontsize=16)
    ax_dh.set_xticks(five_ticks(y_vals))
    ax_dh.set_yticks(five_ticks(x_vals))
    cbar2 = fig_dh.colorbar(pcm2, ax=ax_dh)
    cbar2.set_label(r'$\langle dH/dt \rangle$', fontsize=14)

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)

        dv_filename = f"v_dot_heatmap_{param_x_name}_vs_{param_y_name}.png"
        dh_filename = f"h_dot_heatmap_{param_x_name}_vs_{param_y_name}.png"

        dv_save_path = os.path.join(output_dir, dv_filename)
        dh_save_path = os.path.join(output_dir, dh_filename)

        fig_dv.savefig(dv_save_path, dpi=300)
        fig_dh.savefig(dh_save_path, dpi=300)
        print(f"Plots saved to {dv_save_path} and {dh_save_path}")
        plt.close(fig_dv)
        plt.close(fig_dh)
    else:
        plt.show(block=True)
```

================================================================================
## Folder: src
------------------------------------------------------------

================================================================================
## Folder: visualization/.ipynb_checkpoints
------------------------------------------------------------
### Python Source Files:

#### File: `plotting-checkpoint.py`
```python
# Visualization.py
# Defines simulation parameters, runs the solver, and plots the results.

import numpy as np
# import matplotlib
# matplotlib.use('TkAgg')
import matplotlib.pyplot as plt



def plot_all_time_series(results, N, title="Hindmarsh-Rose Neuron States"):
    """
    Plots the time series of all state variables for each neuron in a grid.

    Args:
        results (dict): The dictionary of simulation results from get_results_dict.
        N (int): The number of neurons in the simulation.
        title (str): The main title for the entire figure.
    """
    states = ['x', 'y', 'z', 'u', 'phi']
    state_labels = [r'$x$', r'$y$', r'$z$', r'$u$', r'$\phi$']

    # Create a figure with 5 rows (for states) and N columns (for neurons)
    # The squeeze=False argument ensures that 'axes' is always a 2D array,
    # even if N=1, which simplifies indexing.
    fig, axes = plt.subplots(5, N, figsize=(4 * N, 10), sharex=True, squeeze=False, dpi=300)
    fig.suptitle(title, fontsize=16)

    for i, (state_var, state_label) in enumerate(zip(states, state_labels)):  # Loop over states (rows)
        for j in range(N):  # Loop over neurons (columns)
            ax = axes[i, j]
            data_key = f'{state_var}{j + 1}'

            if data_key in results:
                ax.plot(results['t'], results[data_key])
                ax.grid(True, linestyle='--', alpha=0.6)

            # Set y-label for the first column
            if j == 0:
                ax.set_ylabel(state_label, fontsize=14)

            # Set title for the first row
            if i == 0:
                ax.set_title(f'Neuron {j + 1}')

    # Set common x-label
    plt.xlabel('Time', fontsize=14)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for suptitle
    plt.show(block=True)


def plot_error_and_state_differences(results, title="Error System and State Difference Dynamics"):
    """
    Plots both the error system states (e.g., e_x) and the direct state
    differences (e.g., x2-x1) on the same subplots for comparison.

    Args:
        results (dict): The dictionary of simulation results.
        title (str): The main title for the figure.
    """
    states = ['x', 'y', 'z', 'u', 'phi']
    error_labels = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$']
    diff_labels = [r'$x_2 - x_1$', r'$y_2 - y_1$', r'$z_2 - z_1$', r'$u_2 - u_1$', r'$\phi_2 - \phi_1$']

    fig, axes = plt.subplots(len(states), 1, figsize=(12, 15), sharex=True)
    fig.suptitle(title, fontsize=16)

    for i, state_var in enumerate(states):
        ax = axes[i]

        # Plot the error system state (e.g., e_x)
        error_key = f'e_{state_var}'
        if error_key in results:
            ax.plot(results['t'], results[error_key], label=error_labels[i])

        # Plot the direct state difference (e.g., x2 - x1)
        key1 = f'{state_var}1'
        key2 = f'{state_var}2'
        if key1 in results and key2 in results:
            difference = results[key2] - results[key1]
            ax.plot(results['t'], difference, label=diff_labels[i], alpha=0.8)

        # Set labels and legend for each subplot
        ax.set_ylabel(f"State '{state_var}'", fontsize=14)
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.legend(loc='upper right')

    plt.xlabel('Time', fontsize=14)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show(block=True)



def plot_bifurcation_diagram(
    param_values,
    peak_values,
    bifurcation_param_name,
    title="Bifurcation Diagram",
    xlabel=None,
    ylabel=r'$x_{max}$', # Default ylabel for x peaks
    marker='.',
    s=1, # Marker size
    save_path=None):

    """Plots a bifurcation diagram."""

    if xlabel is None:
        # Use LaTeX representation if common greek letter, otherwise just the name
        greek_letters = {'rho': r'$\rho$', 'alpha': r'$\alpha$', 'beta': r'$\beta$', 'gamma': r'$\gamma$'}
        xlabel = greek_letters.get(bifurcation_param_name.lower(), bifurcation_param_name)


    plt.figure(layout='tight') # Use tight layout

    # Filter out NaN values for plotting if necessary, or let scatter handle them
    # Scatter usually ignores NaN, which is often desired.
    plt.scatter(param_values, peak_values, marker=marker, s=s, alpha=0.8)

    plt.xlabel(xlabel, fontsize=14)
    plt.ylabel(ylabel, fontsize=14)
    plt.title(f"{title} ({bifurcation_param_name})", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)

    if save_path:
        try:
            plt.savefig(save_path, dpi=300)
            print(f"Figure saved to {save_path}")
        except Exception as e:
            print(f"Error saving figure: {e}")
    else:
        plt.show(block=True)


def plot_hamiltonian(t, H):
    """Plots the Hamiltonian (H)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, H, label='Hamiltonian (H)')
    plt.xlabel('Time')
    plt.ylabel('H')
    plt.title('Hamiltonian vs. Time')
    plt.grid(True)
    plt.legend()
    plt.show(block=True)

def plot_hamiltonian_derivative(t, dHdt):
    """Plots the Hamiltonian derivative (dH/dt)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, dHdt, label='dH/dt', color='orange')
    plt.xlabel('Time')
    plt.ylabel('dH/dt')
    plt.title('Hamiltonian Derivative vs. Time')
    plt.grid(True)
    plt.legend()
    plt.show(block=True)

def plot_lyapunov_derivative(t, dVdt):
    """Plots the Lyapunov derivative (dV/dt)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, dVdt, label='dV/dt', color='green')
    plt.xlabel('Time')
    plt.ylabel('dV/dt')
    plt.title('Lyapunov Derivative vs. Time')
    plt.grid(True)
    plt.legend()
    plt.show(block=True)

def plot_pinn_data(all_results):
    """
    Plots the results from the data generation for the PINN.
    Creates a separate figure for each simulation run, with subplots for key variables.

    Args:
        all_results (list): A list of result dictionaries, where each dictionary
                            corresponds to a single simulation run.
    """
    # Define variables to plot and their corresponding LaTeX titles
    variables_to_plot = ['e_x', 'e_y', 'e_z', 'e_u', 'e_phi', 'Hamiltonian', 'dHdt', 'dVdt']
    titles = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$', r'$H$', r'$dH/dt$', r'$dV/dt$']

    # Create a separate plot for each run in the results
    for run_idx, results in enumerate(all_results):
        t = results['t']
        fig, axes = plt.subplots(4, 2, sharex=True)
        fig.suptitle(f'Run {run_idx + 1}', fontsize=16)

        # Plot each time series in its designated subplot
        for ax, var, title in zip(axes.flat, variables_to_plot, titles):
            ax.plot(t, results[var])
            ax.set_xlabel('t')
            ax.set_ylabel(var)
            ax.grid(True)

        plt.tight_layout()
        plt.show(block=True)


def plot_v_h_dot_1d(param_values, mean_dvdt, mean_dhdt, param_name, title, save_path=None):
    """
    Plots the mean dV/dt and dH/dt against a single parameter in a combined figure.

    Args:
        param_values (np.ndarray): The values of the swept parameter.
        mean_dvdt (np.ndarray): The corresponding mean dV/dt values.
        mean_dhdt (np.ndarray): The corresponding mean dH/dt values.
        param_name (str): The name of the parameter (e.g., 'k', 'rho').
        title (str): The title for the plot.
        save_path (str, optional): Path to save the figure. If None, shows the plot.
    """
    fig, ax = plt.subplots(
        constrained_layout=True
    )

    # Plot the two curves
    ax.plot(param_values, mean_dvdt, color="blue", label=r'$\langle dV/dt \rangle$', marker='.', linestyle='-')
    ax.plot(param_values, mean_dhdt, color="red", label=r'$\langle dH/dt \rangle$', marker='.', linestyle='-')

    # Use LaTeX representation for common greek letters
    greek_letters = {'rho': r'$\rho$', 'alpha': r'$\alpha$', 'k': r'$k$'}
    xlabel = greek_letters.get(param_name.lower(), param_name)

    # Set labels, title, grid, and legend
    ax.set_xlabel(xlabel, fontsize=18)
    ax.set_ylabel(r'Mean Value', fontsize=18)
    ax.set_title(title, fontsize=20)
    ax.grid(True)
    ax.legend(loc="best", fontsize=14)

    # Enforce a square aspect ratio and remove horizontal margins
    ax.set_box_aspect(1.0)
    ax.margins(x=0)

    # Save or show the plot
    if save_path:
        fig.savefig(save_path, dpi=300)
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show(block=True)


def plot_v_h_dot_2d(x_vals, y_vals, mean_dvdt, mean_dhdt, param_x_name, param_y_name, save_path_prefix=None):
    """
    Plots 2D heatmaps for mean dV/dt and dH/dt from a parameter sweep.
    Generates and saves two separate figures.

    Args:
        x_vals (np.ndarray): Array of values for the parameter on the y-axis.
        y_vals (np.ndarray): Array of values for the parameter on the x-axis.
        mean_dvdt (np.ndarray): 2D array (shape Nx, Ny) of mean dV/dt values.
        mean_dhdt (np.ndarray): 2D array (shape Nx, Ny) of mean dH/dt values.
        param_x_name (str): Name of the parameter for the y-axis.
        param_y_name (str): Name of the parameter for the x-axis.
        save_path_prefix (str, optional): Base path for saving the plots.
                                          If None, plots are shown interactively.
    """
    # --- Common Setup ---
    # Create meshgrid. Note: The parameter for the plot's x-axis (PARAM_Y) comes first.
    Xg, Yg = np.meshgrid(y_vals, x_vals)

    # Define LaTeX labels for better formatting
    greek_map = {'ge': r'$g_e$', 'rho': r'$\rho$', 'k': r'$k$', 'm': r'$m$'}
    xlabel = greek_map.get(param_y_name.lower(), param_y_name)
    ylabel = greek_map.get(param_x_name.lower(), param_x_name)

    # Helper function to generate clean tick marks
    def five_ticks(grid):
        mn, mx = np.nanmin(grid), np.nanmax(grid)
        return np.round(np.linspace(mn, mx, 5), 2)

    # --- Plot 1: dV/dt ---
    fig_dv, ax_dv = plt.subplots(constrained_layout=True)
    pcm1 = ax_dv.pcolormesh(Xg, Yg, mean_dvdt, shading="auto", cmap='viridis')
    ax_dv.set_xlabel(xlabel, fontsize=14)
    ax_dv.set_ylabel(ylabel, fontsize=14)
    ax_dv.set_title(r"Mean $\langle dV/dt \rangle$ (post-transient)", fontsize=16)

    ax_dv.set_xticks(five_ticks(y_vals))
    ax_dv.set_yticks(five_ticks(x_vals))

    cbar1 = fig_dv.colorbar(pcm1, ax=ax_dv)
    cbar1.set_label(r'$\langle dV/dt \rangle$', fontsize=14)

    # --- Plot 2: dH/dt ---
    fig_dh, ax_dh = plt.subplots(constrained_layout=True)
    pcm2 = ax_dh.pcolormesh(Xg, Yg, mean_dhdt, shading="auto", cmap='inferno')
    ax_dh.set_xlabel(xlabel, fontsize=14)
    ax_dh.set_ylabel(ylabel, fontsize=14)
    ax_dh.set_title(r"Mean $\langle dH/dt \rangle$ (post-transient)", fontsize=16)

    ax_dh.set_xticks(five_ticks(y_vals))
    ax_dh.set_yticks(five_ticks(x_vals))

    cbar2 = fig_dh.colorbar(pcm2, ax=ax_dh)
    cbar2.set_label(r'$\langle dH/dt \rangle$', fontsize=14)

    # --- Save or Show Plots ---
    if save_path_prefix:
        dv_save_path = f"{save_path_prefix}_dVdt.png"
        dh_save_path = f"{save_path_prefix}_dHdt.png"
        fig_dv.savefig(dv_save_path, dpi=300)
        fig_dh.savefig(dh_save_path, dpi=300)
        print(f"Plots saved to {dv_save_path} and {dh_save_path}")
        plt.close(fig_dv)
        plt.close(fig_dh)
    else:
        plt.show(block=True)
```

================================================================================
## Folder: visualization/literature plots
------------------------------------------------------------
### Python Source Files:

#### File: `plot_TS_PhP.py`
```python
import os
import diffrax as dfx
import matplotlib.pyplot as plt
from matplotlib.ticker import FormatStrFormatter
from src.hr_model.model import HindmarshRose, DEFAULT_PARAMS, DEFAULT_STATE0


# ==============================================================================
# 1. LOCAL PLOTTING FUNCTIONS
# ==============================================================================

def plot_time_series(t, x, title_suffix, save_path_prefix=None):
    """Plots the time series of variable x after a transient period."""

    plt.figure(figsize=(8, 8), layout='tight')
    plt.suptitle(f'{title_suffix}', fontsize=30, y=0.957)
    plt.plot(t, x, c='k')
    plt.xlabel(r'$t$', fontsize=25)
    plt.ylabel(r'$x$', fontsize=25)
    plt.tick_params(axis='both', which='major', labelsize=20)

    if len(t) > 0:
        t_min, t_max = t.min(), t.max()
        plt.xlim([t_min, t_max + (t_max - t_min) * 0.01])

    ax = plt.gca()
    ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))

    if save_path_prefix:
        plt.savefig(save_path_prefix + '.png', format='png', dpi=300)
        plt.savefig(save_path_prefix + '.eps', format='eps')
        plt.close()
    else:
        plt.show(block=True)


def plot_phase_portrait(x, y, title_suffix, save_path_prefix=None):
    """Plots the phase portrait (x vs y) after a transient period."""

    plt.figure(figsize=(8, 8), layout='tight')
    plt.suptitle(f'{title_suffix}', fontsize=30, y=0.957)
    plt.plot(x, y, c='k')
    plt.xlabel(r'$x$', fontsize=25)
    plt.ylabel(r'$y$', fontsize=25)
    plt.tick_params(axis='both', which='major', labelsize=20)
    ax = plt.gca()
    ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))
    ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))

    if save_path_prefix:
        plt.savefig(save_path_prefix + '.png', format='png', dpi=300)
        plt.savefig(save_path_prefix + '.eps', format='eps')
        plt.close()
    else:
        plt.show(block=True)


# ==============================================================================
# 2. MAIN SCRIPT LOGIC
# ==============================================================================

# --- Define the two specific simulation setups ---
setups = [
    {'name': 'rho_fixed', 'rho': 0.7, 'k': 0.36, 'm': 0.25},
    {'name': 'rho_fixed', 'rho': 0.7, 'k': 0.36, 'm': 0.5},
    {'name': 'rho_fixed', 'rho': 0.7, 'k': 0.36, 'm': 1},
    {'name': 'k_fixed', 'rho': 0.7, 'k': 0.87, 'm': 0.25},
    {'name': 'k_fixed', 'rho': 0.7, 'k': 0.87, 'm': 0.5},
    {'name': 'k_fixed', 'rho': 0.7, 'k': 0.87, 'm': 1}
]

# --- Define Output Directory ---
output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Phase Portrait')
os.makedirs(output_dir, exist_ok=True)
print(f"Plots will be saved to: {output_dir}")

# --- Loop Through Setups and Run Simulations ---
for setup_params in setups:
    m_val = setup_params['m']
    setup_name = setup_params['name']
    print(f"\n--- Running simulation for setup: '{setup_name}' (m = {m_val}) ---")

    # --- Use simulation example from model.py ---
    sim_params = DEFAULT_PARAMS.copy()
    sim_params.update(setup_params)  # Update params for the current setup

    # Create the model instance
    hr_model = HindmarshRose(N=1, params=sim_params, initial_state=DEFAULT_STATE0, I_ext=0.8, xi=0)

    # Integration settings
    start_time = 0
    end_time = 2000
    dt_initial = 0.05
    n_points = 20000
    max_steps = int((end_time - start_time) / dt_initial) * 20
    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-8, atol=1e-10)

    # Run the solver
    hr_model.solve(
        solver=solver, t0=start_time, t1=end_time, dt0=dt_initial, n_points=n_points,
        stepsize_controller=stepsize_controller, max_steps=max_steps)

    # Get results, discarding the first 75% as transient
    transient_discard_ratio = 0.75
    results = hr_model.get_results_dict(transient_ratio=transient_discard_ratio)
    t_sol, x_sol, y_sol = results['t'], results['x1'], results['y1']

    # --- Use exact plotting logic from the original script ---
    if m_val == 1:
        xt_title_suffix, xy_title_suffix = '(a1)', '(a2)'
    elif m_val == 0.5:
        xt_title_suffix, xy_title_suffix = '(b1)', '(b2)'
    elif m_val == 0.25:
        xt_title_suffix, xy_title_suffix = '(c1)', '(c2)'
    else:
        xt_title_suffix, xy_title_suffix = '(x1)', '(x2)'

    # --- Create filenames and save plots ---
    base_filename_ts = f"timeseries_{setup_name}_{xt_title_suffix.strip('()')}"
    save_path_prefix_ts = os.path.join(output_dir, base_filename_ts)

    base_filename_pp = f"phase_portrait_{setup_name}_{xy_title_suffix.strip('()')}"
    save_path_prefix_pp = os.path.join(output_dir, base_filename_pp)

    # Plot and save the time series using the local function
    plot_time_series(t_sol, x_sol, xt_title_suffix, save_path_prefix=save_path_prefix_ts)

    # Plot and save the phase portrait using the local function
    plot_phase_portrait(x_sol, y_sol, xy_title_suffix, save_path_prefix=save_path_prefix_pp)

print("\nAll simulations and plots saved successfully.")
```

#### File: `plot_bif_lya.py`
```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from matplotlib.ticker import FuncFormatter
import os # Add this import



# --- Define paths to the data directories relative to the data ---
path_bif = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Bifurcation/')
os.makedirs(path_bif, exist_ok=True)

path_lya = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Lyapanov 1D/')
os.makedirs(path_lya, exist_ok=True)

# --- Define output directory ---
output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Bif_Lya_plots')
os.makedirs(output_dir, exist_ok=True)



bif_names = ['Bif_k_rho_0.7_m_1.npy', 'Bif_k_rho_0.7_m_0.5.npy', 'Bif_k_rho_0.7_m_0.25.npy',
             'Bif_rho_k_0.87_m_1.npy', 'Bif_rho_k_0.87_m_0.5.npy', 'Bif_rho_k_0.87_m_0.25.npy']
lya_names = ['Lya_k_rho_0.7_m_1.txt', 'Lya_k_rho_0.7_m_0.5.txt', 'Lya_k_rho_0.7_m_0.25.txt',
             'Lya_rho_k_0.87_m_1.txt', 'Lya_rho_k_0.87_m_0.5.txt', 'Lya_rho_k_0.87_m_0.25.txt']
save_names = ['k_m_1', 'k_m_0.5', 'k_m_0.25',
              'rho_m_1', 'rho_m_0.5', 'rho_m_0.25']
x_names = [r'$k$', r'$k$', r'$k$', r'$\rho$', r'$\rho$', r'$\rho$']
titles = ['(a)', '(b)', '(c)', '(a)', '(b)', '(c)']

leg_poss = [(0, 0), (0, 0), (0, 0), (0, 0), (0.75, 0), (0, 0)]

for bif_name, lya_name, save_name, title, leg_pos, x_name in zip(
        bif_names, lya_names, save_names, titles, leg_poss, x_names):

    # Load data
    data_lya = np.array(pd.read_csv(path_lya+lya_name, delim_whitespace=True, header=None))
    data_bif = np.load(path_bif+bif_name)


    # Function to format ticks to 2 decimal places
    def format_func(value, tick_number):
        return f'{value:.2f}'


    # Define the size of each subplot (square dimensions)
    subplot_size = 6  # in inches

    # Calculate the total figure size
    total_width = subplot_size
    total_height = 1.25 * subplot_size

    # Create the figure and subplots with adjusted size and spacing
    fig, (ax1, ax2) = plt.subplots(
        2, 1,
        figsize=(total_width, total_height),
        gridspec_kw={'height_ratios': [1, 1]}
    )
    fig.suptitle(title, fontsize=30, y=0.95)

    ### Subplot 1: Scatter Plot
    # Plot the data
    ax1.scatter(data_bif[:, 0], data_bif[:, 1], s=0.05, c='k')

    # Set labels and format
    ax1.set_ylabel(r'$x_{\text{max}}$', fontsize=25)
    ax1.set_xticks([])
    ax1.tick_params(axis='both', which='major', labelsize=16)
    ax1.set_xlim([data_bif[:, 0].min(), data_bif[:, 0].max()])
    ax1.xaxis.set_major_formatter(FuncFormatter(format_func))
    ax1.yaxis.set_major_formatter(FuncFormatter(format_func))

    # Set y-limits to data range
    y_min_ax1 = data_bif[:, 1].min()
    y_max_ax1 = data_bif[:, 1].max()
    ax1.set_ylim([y_min_ax1, y_max_ax1])

    # Set y-ticks
    ax1.set_yticks(np.linspace(y_min_ax1, y_max_ax1, 5))

    # Adjust aspect ratio
    ax1.set_aspect('auto')

    ### Subplot 2: Line Plot
    # Plot the data
    ax2.plot(data_lya[:, 0], data_lya[:, 1], label="LE 1", linewidth=1.5, c='b')
    ax2.plot(data_lya[:, 0], data_lya[:, 3], label="LE 2", linewidth=1.5, c='r')

    # Set labels and format

    ax2.set_xlabel(x_name, fontsize=25)
    ax2.set_ylabel(r'$LEs$', fontsize=25)
    ax2.tick_params(axis='both', which='major', labelsize=16)
    ax2.set_xlim([data_lya[:, 0].min(), data_lya[:, 0].max()])
    ax2.set_xticks(np.linspace(data_lya[:, 0].min(), data_lya[:, 0].max(), 5))
    ax2.xaxis.set_major_formatter(FuncFormatter(format_func))
    ax2.yaxis.set_major_formatter(FuncFormatter(format_func))
    ax2.legend(fontsize=12, loc='lower left', bbox_to_anchor=leg_pos)
    ax2.grid(True)

    # Set y-limits to data range

    y_min_ax2 = min(data_lya[:, 1][~np.isnan(data_lya[:, 1])].min(),
                    data_lya[:, 3][~np.isnan(data_lya[:, 3])].min())
    y_max_ax2 = max(data_lya[:, 1][~np.isnan(data_lya[:, 1])].max(),
                    data_lya[:, 3][~np.isnan(data_lya[:, 3])].max())
    ax2.set_ylim([y_min_ax2, y_max_ax2])

    # Set y-ticks
    ax2.set_yticks(np.linspace(y_min_ax2, y_max_ax2, 5))

    # Adjust aspect ratio
    ax2.set_aspect('auto')

    # **Adjust layout to prevent labels from being cut off**
    plt.tight_layout()

    # If labels are still cut off, adjust the margins manually
    # fig.subplots_adjust(left=0.15, right=0.95, top=0.95, bottom=0.1, hspace=0.3)

    # --- Save the figure in both PNG and EPS formats ---
    base_output_path = os.path.join(output_dir, save_name)
    plt.savefig(base_output_path + '.png', format='png', dpi=100)
    plt.savefig(base_output_path + '.eps', format='eps')
    plt.close()
```

#### File: `plot_error_system.py`
```python
import os
import matplotlib
matplotlib.use('Qt5Agg')
import matplotlib.pyplot as plt
import diffrax as dfx
import jax.numpy as jnp
from src.hr_model.error_system import HRNetworkErrorSystem
from src.hr_model.model import DEFAULT_PARAMS

# ======================================================================
# 1. SIMULATION SETUP & EXECUTION
# ======================================================================

# --- Initial Conditions & Parameters ---
# Initial state (x, y, z, u, φ for each neuron)
INITIAL_HR_STATE0 = [
    0.1, 0.2, 0.3, 0.4, 0.1,   # neuron 1
    0.2, 0.3, 0.4, 0.5, 0.2    # neuron 2
]

# [cite_start]External currents and coupling matrix [cite: 151]
I_ext = [0.8, 0.8]
xi = [[0, 1], [1, 0]]
sim_params = DEFAULT_PARAMS.copy()

# --- Create Simulator Instance ---
# Using 'complete' dynamics as in the example
simulator = HRNetworkErrorSystem(params=sim_params, dynamics='complete',
                                 hr_initial_state=INITIAL_HR_STATE0, I_ext=I_ext, hr_xi=xi)

# --- Integration Settings ---
start_time = 0
end_time = 1000
dt_initial = 0.01
point_num = 10000
transient_ratio = 0
n_points = dfx.SaveAt(ts=jnp.linspace(start_time, end_time, point_num), dense=True)
max_steps = int((end_time - start_time) / dt_initial) * 20
solver = dfx.Tsit5()
stepsize_controller = dfx.PIDController(rtol=1e-10, atol=1e-12)

# --- Run Simulation ---
print("Running simulation...")
simulator.solve(
    solver=solver,
    t0=start_time,
    t1=end_time,
    dt0=dt_initial,
    n_points=n_points,
    stepsize_controller=stepsize_controller,
    max_steps=max_steps
)
print("Simulation finished.")

# --- Get Results ---
results = simulator.get_results_dict(transient_ratio)


# ======================================================================
# 2. PLOTTING
# This section is preserved from the original plot_error_system.py
# ======================================================================

# --- Define the Directory ---
output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Error System')
os.makedirs(output_dir, exist_ok=True)
print(f"Saving figures to '{output_dir}/'")

# --- Define Plotting Series ---
series = [
    ('e_x', r'$e_x$', 'darkblue', '(a)'),
    ('e_y', r'$e_y$', 'C2', '(b)'),
    ('e_z', r'$e_z$', 'C1', '(c)'),
    ('e_u', r'$e_u$', 'tomato', '(d)'),
    ('e_phi', r'$e_\phi$', 'purple', '(e)'),
]

# --- Generate and Save Plots ---
for key, label, color, title in series:
    # Create a new square figure with high resolution and tight layout
    fig, ax = plt.subplots(
        figsize=(4, 4),
        dpi=200,
        constrained_layout=True)

    # Plot the data
    ax.plot(results['t'], results[key], c=color, linewidth=1.5)

    # Set labels, title, grid, aspect ratio, and margins
    ax.set_xlabel(r'$t$',   fontsize=18)
    ax.set_ylabel(label,    fontsize=18)
    ax.set_title(f'{title.strip("$")}', fontsize=20)
    ax.grid(True)
    ax.set_box_aspect(1.0)
    ax.margins(x=0)

    # Define save paths and save the figure in both PNG and EPS formats
    png_path = os.path.join(output_dir, f'{key}.png')
    eps_path = os.path.join(output_dir, f'{key}.eps')
    fig.savefig(png_path, format='png', dpi=200)
    fig.savefig(eps_path, format='eps')

    plt.close(fig)

print("All plots have been generated and saved successfully.")
```

#### File: `plot_lya2D_heatmap.py`
```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.colors as mcolors
import os





# --- Define the Directory ---
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Lyapanov 2D/')
os.makedirs(path, exist_ok=True)

file_names = ['Max_Lya_k_m.txt', 'Max_Lya_rho_m.txt', 'Max_Lya_rho_k_m_1.txt',
              'Max_Lya_rho_k_m_0.5.txt', 'Max_Lya_rho_k_m_0.25.txt']

save_names = ['K_M', 'Rho_M', 'Rho_K0_M_1',
              'Rho_K0_M_0.5', 'Rho_K0_M_0.25']

x_names = [r'$k$', r'$\rho$', r'$\rho$', r'$\rho$', r'$\rho$']
y_names = [r'$m$', r'$m$', r'$k$', r'$k$', r'$k$']
titles = ['(a)', '(b)', '(c1)', '(c2)', '(c3)']

global_min = float('inf')
global_max = float('-inf')

# First pass: Determine global min and max across all files
for file_name in file_names:
    df = pd.read_csv(path + file_name, delim_whitespace=True, header=None)
    pivot_table = df.pivot(index=0, columns=2, values=1).fillna(0)
    array_2d = np.rot90(pivot_table.to_numpy(), k=1)
    global_min = min(global_min, np.nanmin(array_2d))
    global_max = max(global_max, np.nanmax(array_2d))

# Iterate through files again to create plots with consistent colorbar
for file_name, save_name, x_name, y_name, title in zip(
        file_names, save_names, x_names, y_names, titles):

    # Read the file into a DataFrame
    df = pd.read_csv(path + file_name, delim_whitespace=True, header=None)

    # Pivot the DataFrame to reshape it into a 2D array
    pivot_table = df.pivot(index=0, columns=2, values=1)

    # Fill NaNs with a value if necessary, h.g., 0
    pivot_table = pivot_table.fillna(0)

    # Convert to a 2D numpy array
    array_2d = np.rot90(pivot_table.to_numpy(), k=1)

    # Define custom colormap: black to yellow to red
    colors = ['black', 'yellow', 'red']
    n_bins = int((global_max * 100 / (
                global_max - global_min)) * 128) if global_min != global_max else 256  # Handle case where min == max
    cmap_custom = mcolors.LinearSegmentedColormap.from_list("custom", colors, N=n_bins)

    # Combine white for negative values with the custom colormap
    if global_min < 0:
        cmap_pos = cmap_custom(np.linspace(0, 1, n_bins))
        cmap_min_len = int(
            (abs(global_min) * 100 / (global_max - global_min)) * 128) if global_min != global_max else 128
        cmap_min = np.array([[1, 1, 1, 1]] * cmap_min_len)
        cmap_combined = np.vstack((cmap_min, cmap_custom(np.linspace(0, 1, n_bins))))
        cmap = mcolors.ListedColormap(cmap_combined)
        vmin_plot = global_min
        vmax_plot = global_max
    else:
        cmap = cmap_custom
        vmin_plot = global_min
        vmax_plot = global_max

    # Plot the heatmap
    fig, ax = plt.subplots(figsize=(8, 8), dpi=1000)
    plt.suptitle(title, fontsize=30, y=0.90)
    img = ax.imshow(array_2d, aspect='auto', cmap=cmap, origin='lower',
                    extent=[pivot_table.index.min(), pivot_table.index.max(),
                            pivot_table.columns.min(), pivot_table.columns.max()],
                    vmin=vmin_plot, vmax=vmax_plot)

    # Add colorbar with reduced distance from plot
    cbar = fig.colorbar(img, ax=ax, pad=0.02)

    # Set colorbar ticks
    colorbar_ticks = []
    if global_min < 0:
        colorbar_ticks.append(round(global_min, 2))
    colorbar_ticks.append(0.00)
    if global_max > 0:
        num_intervals = 3
        positive_ticks = np.linspace(0, global_max, num_intervals + 1)[1:]  # Exclude 0
        colorbar_ticks.extend([round(tick, 2) for tick in positive_ticks])
        colorbar_ticks.append(round(global_max, 2))
    elif global_max == 0 and global_min < 0:
        colorbar_ticks.append(round(global_max, 2))

    colorbar_ticks = sorted(list(set(colorbar_ticks)))  # Remove duplicates and sort
    cbar.set_ticks(colorbar_ticks)
    cbar.set_ticklabels([f'{tick:.2f}' for tick in colorbar_ticks])

    # Set colorbar label at the top
    cbar.ax.set_title(r'$\lambda_{\mathrm{max}}$', pad=20, fontsize=25)

    # Set axis labels
    ax.set_xlabel(x_name, fontsize=25)
    ax.set_ylabel(y_name, fontsize=25)

    # Set font size for axis ticks
    ax.tick_params(axis='both', which='major', labelsize=20)

    # Set font size for colorbar ticks
    cbar.ax.tick_params(labelsize=20)

    # --- Set x-axis ticks ---
    x_min, x_max = pivot_table.index.min(), pivot_table.index.max()
    x_ticks = np.linspace(x_min, x_max, 5)
    ax.set_xticks(x_ticks)
    ax.set_xticklabels([f'{tick:.2f}' for tick in x_ticks])

    # --- Set y-axis ticks ---
    y_min, y_max = pivot_table.columns.min(), pivot_table.columns.max()
    y_ticks = np.linspace(y_min, y_max, 5)
    ax.set_yticks(y_ticks)
    ax.set_yticklabels([f'{tick:.2f}' for tick in y_ticks])

    # Adjust layout to prevent cropping
    plt.tight_layout()

    # --- Save the figure in both PNG and EPS formats ---
    base_output_path = os.path.join(path, save_name)
    plt.savefig(base_output_path + '.png', format='png', dpi=300)
    plt.savefig(base_output_path + '.eps', format='eps')
    plt.close()
```

#### File: `plot_v_h_dot_1D.py`
```python
import matplotlib
matplotlib.use("Qt5Agg")
import matplotlib.pyplot as plt
import numpy as np
import os

# --- Define Directory ---
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'V_H_DOT_1D/')
os.makedirs(path, exist_ok=True)

file_name = 'V_H_DOT_k_-2.5_2.npz'

data = np.load(path + file_name)
PARAM_VALUES = data['PARAM_VALUES']
mean_dVdt = data['mean_dVdt']
mean_dHdt = data['mean_dHdt']
TARGET_PARAM = data['TARGET_PARAM']

# ── Combined Plot ───────────────────────────────────────────────────────────

import matplotlib
matplotlib.use('Qt5Agg')
import matplotlib.pyplot as plt

# mask out any non-finite values
x_vals = np.asarray(PARAM_VALUES, dtype=float)

# compute x-limits (assuming PARAM_VALUES has no NaN/Inf)
x_min, x_max = np.nanmin(x_vals), np.nanmax(x_vals)

# create the figure and axes
fig, ax = plt.subplots(
    figsize=(4, 4),
    dpi=200,
    constrained_layout=True
)

# plot your two curves
ax.plot(PARAM_VALUES, mean_dVdt, color="blue", label=r'$dV/dt$')
ax.plot(PARAM_VALUES, mean_dHdt, color="red", label=r'$dH/dt$')

# optional: ensure ticks include endpoints
ax.set_xticks([x_min] + list(ax.get_xticks()) + [x_max])

# labels, title, grid, legend
ax.set_xlabel(r'$k$', fontsize=18)
ax.set_ylabel(r'$dV/dt, dH/dt$', fontsize=18)
ax.set_title("(b)", fontsize=20)
ax.grid(True)
ax.legend(loc="best", fontsize=14)

# enforce square aspect and remove x-margins
ax.set_box_aspect(1.0)
ax.set_xlim(x_min, x_max)
ax.margins(x=0)

png_path = os.path.join(path, f'k.png')
eps_path = os.path.join(path, f'k.eps')
fig.savefig(png_path, format='png', dpi=200)
fig.savefig(eps_path, format='eps')



```

#### File: `plot_v_h_dot_2D.py`
```python
# plot_vdot_2d_swapped.py  – PARAM_X on x-axis, PARAM_Y on y-axis
# --------------------------------------------------------------

import matplotlib
matplotlib.use("Qt5Agg")
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import os

# ── Edit these two lines to your file location ─────────────────────────
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'V_H_DOT_2D/')
os.makedirs(path, exist_ok=True)

file_name = 'VDOT_ge_0-1_m_0-1.npz'

# ── Load data ──────────────────────────────────────────────────────────
data = np.load(path / file_name)
x_vals     = data['x_vals']                 # PARAM_X values
y_vals     = data['y_vals']                 # PARAM_Y values
mean_dVdt  = data['mean_dVdt']              # shape (NX, NY)
mean_dHdt  = data['mean_dHdt']

# maskout some values
# mean_dHdt[np.where(mean_dHdt < -1)] = np.nan
# mean_dHdt[np.where(mean_dHdt > 450)] = np.nan

PARAM_X = data['PARAM_X'].item()
PARAM_Y = data['PARAM_Y'].item()

# ── Build grid with x on rows, y on columns (needs transpose) ──────────
Xg, Yg = np.meshgrid(x_vals, y_vals)        # shapes (NY, NX)


import matplotlib
matplotlib.use('Qt5Agg')
import matplotlib.pyplot as plt
import numpy as np

# Helper to compute 5 ticks from data grid
def five_ticks(grid):
    mn, mx = np.min(grid), np.max(grid)
    ticks = np.linspace(mn, mx, 5)
    ticks = np.round(ticks, 1)
    return ticks

# ── Plot 1 : 〈dV/dt〉 ───────────────────────────────────────────────────
fig_dV, ax_dV = plt.subplots(figsize=(4, 4), dpi=200, constrained_layout=True)

ax_dV.set_box_aspect(1.0)
pcm1 = ax_dV.pcolormesh(Xg, Yg, mean_dVdt.T, shading='auto', cmap='seismic')
ax_dV.set_xlabel(r'$ge$', fontsize=18)
ax_dV.set_ylabel(r'$m$', fontsize=18)
ax_dV.set_title('(b1)', fontsize=20)

# set 5 ticks on x and y
xticks = five_ticks(Xg)
yticks = five_ticks(Yg)
ax_dV.set_xticks(xticks)
ax_dV.set_yticks(yticks)
ax_dV.set_xticklabels([f"{t:.1f}" for t in xticks])
ax_dV.set_yticklabels([f"{t:.1f}" for t in yticks])

# colorbar with title‐label on top
cbar1 = fig_dV.colorbar(pcm1, ax=ax_dV, pad=0.0001, fraction=0.05)
cbar1.ax.set_title(r'$dV/dt$', pad=7, fontdict={'fontsize':15})

# ── Plot 2 : 〈dH/dt〉 ───────────────────────────────────────────────────
fig_dH, ax_dH = plt.subplots(figsize=(4, 4), dpi=200, constrained_layout=True)

ax_dH.set_box_aspect(1.0)
pcm2 = ax_dH.pcolormesh(Xg, Yg, mean_dHdt.T, shading='auto', cmap='seismic')
ax_dH.set_xlabel(r'$ge$', fontsize=18)
ax_dH.set_ylabel(r'$m$', fontsize=18)
ax_dH.set_title('(b2)', fontsize=20)

# set 5 ticks on x and y
ax_dH.set_xticks(xticks)
ax_dH.set_yticks(yticks)
ax_dH.set_xticklabels([f"{t:.1f}" for t in xticks])
ax_dH.set_yticklabels([f"{t:.1f}" for t in yticks])

cbar2 = fig_dH.colorbar(pcm2, ax=ax_dH, pad=0.0001, fraction=0.05)
cbar2.ax.set_title(r'$dH/dt$', pad=7, fontdict={'fontsize':15})

png_dV_path = os.path.join(path, f'k.png')
eps_dV_path = os.path.join(path, f'k.eps')
fig_dV.savefig(png_dV_path, format='png', dpi=200)
fig_dV.savefig(eps_dV_path, format='eps')

png_dH_path = os.path.join(path, f'k.png')
eps_dH_path = os.path.join(path, f'k.eps')
fig_dH.savefig(png_dH_path, format='png', dpi=200)
fig_dH.savefig(eps_dH_path, format='eps')



```

================================================================================
## Folder: src/hr_model
------------------------------------------------------------
### Python Source Files:

#### File: `error_system.py`
```python
import numpy as np
import jax, jax.numpy as jnp
import diffrax as dfx
jax.config.update("jax_enable_x64", True)
from src.hr_model.model import HindmarshRose as HR
from src.hr_model.model import DEFAULT_PARAMS

class HRNetworkErrorSystem:
    """
    Simulates a system combining a 2‑neuron Hindmarsh‑Rose network
    (using the HindmarshRose class) and an associated error system.
    """
    NUM_ERROR_VARS = 5

    def __init__(self, params=None, dynamics=None, hr_initial_state=None, I_ext=None, hr_xi=None):
        self.dynamics = dynamics
        self.params = params.copy()
        self.hr_initial_state = jnp.array(hr_initial_state, dtype=jnp.float64)
        self.error_initial_state = (self.hr_initial_state[len(self.hr_initial_state) // 2:]
                                    - self.hr_initial_state[:len(self.hr_initial_state) // 2])
        self.xi = jnp.array(hr_xi, dtype=jnp.float64)
        self.I_ext = jnp.array(I_ext, dtype=jnp.float64)

        # --- Create the HindmarshRose instance ---
        self.hr_network = HR(
            N=2,
            params=self.params,
            initial_state=self.hr_initial_state,
            I_ext=self.I_ext,
            xi=self.xi
        )

        # --- Combine initial states for the overall system ---
        self.combined_state0 = jnp.concatenate([self.hr_initial_state, self.error_initial_state], dtype=jnp.float64)

        # Attributes to store results
        self.t = None
        self.solution = None
        self.derivative = None
        self.failed = None

    @staticmethod
    def complete_vector_field(current_error_state, current_hr_state, params):
        """
        Calculates derivatives for the 'complete' error system dynamics.
        This static method can be called from anywhere without needing an instance.
        """
        # Unpack HR state variables needed by the error system
        x1, _, _, u1, phi1 = current_hr_state[0:5]
        _, _, _, u2, _ = current_hr_state[5:10]

        # Unpack current error state
        e_x, e_y, e_z, e_u, e_phi = current_error_state

        # --- Complete Error System Derivatives ---
        de_xdt = ((((e_y - (params['a'] * ((e_x ** 3) + (3 * (e_x ** 2) * x1) + (3 * e_x * (x1 ** 2))))
                     + (params['b'] * ((e_x ** 2) + (2 * e_x * x1))) + (params['k'] * params['h'] * e_x))
                    + (params['k'] * params['f'] * ((x1 * (e_u ** 2)) + (2 * u1 * x1 * e_u) + (e_x * (e_u ** 2)) +
                                                    (2 * u1 * e_x * e_u) + ((u1 ** 2) * e_x))))
                   + (params['rho'] * ((phi1 * e_x) + (e_x * e_phi) + (x1 * e_phi))) - (2 * params['ge'] * e_x))
        )
        de_ydt = -params['d'] * ((e_x ** 2) + (2 * e_x * x1)) - e_y
        de_zdt = params['r'] * ((params['s'] * e_x) - e_z)

        # --- piece-wise de_u/dt ---
        condlist = [
            (u1 >= 1) & (-1 < u2) & (u2 < 1), (u1 >= 1) & (u2 <= -1),
            (-1 < u1) & (u1 < 1) & (u2 >= 1), (-1 < u1) & (u1 < 1) & (-1 < u2) & (u2 < 1),
            (-1 < u1) & (u1 < 1) & (u2 <= -1), (u1 <= -1) & (u2 >= 1),
            (u1 <= -1) & (-1 < u2) & (u2 < 1),
        ]
        choicelist = [
            e_x + (2 * params['m'] - 1) * e_u + 2 * params['m'] * (u1 - 1), e_x - e_u - 4 * params['m'],
            e_x - e_u - 2 * params['m'] * (u1 - 1), e_x + (2 * params['m'] - 1) * e_u,
            e_x - e_u - 2 * params['m'] * (u1 + 1), e_x - e_u + 4 * params['m'],
            e_x + (2 * params['m'] - 1) * e_u + 2 * params['m'] * (u1 + 1),
        ]
        de_udt = jnp.select(condlist, choicelist, default=e_x - e_u)
        de_phidt = e_x - params['q'] * e_phi

        return jnp.array([de_xdt, de_ydt, de_zdt, de_udt, de_phidt], dtype=jnp.float64)

    @staticmethod
    def simplified_vector_field(current_error_state, current_hr_state, params):
        """
        Calculates derivatives for the 'simplified' error system dynamics.
        This static method can be called from anywhere without needing an instance.
        """
        # Unpack HR state variables needed by the error system
        x1, _, _, u1, phi1 = current_hr_state[0:5]
        _, _, _, u2, _ = current_hr_state[5:10]

        # Unpack current error state
        e_x, e_y, e_z, e_u, e_phi = current_error_state

        # --- Simplified Error System Derivatives ---
        de_xdt = (
                (((-3 * params['a'] * (x1 ** 2)) + (2 * params['b'] * x1) + (params['k'] * params['h'])
                  + (params['k'] * params['f'] * (u1 ** 2)) + (params['rho'] * phi1) - (2 * params['ge'])) * e_x)
                + e_y + (2 * params['k'] * params['f'] * u1 * x1 * e_u) + (params['rho'] * x1 * e_phi)
        )
        de_ydt = (-2 * params['d'] * x1 * e_x) - e_y
        de_zdt = params['r'] * ((params['s'] * e_x) - e_z)

        # --- piece-wise de_u/dt ---
        condlist = [
            (u1 >= 1) & (-1 < u2) & (u2 < 1), (u1 >= 1) & (u2 <= -1),
            (-1 < u1) & (u1 < 1) & (u2 >= 1), (-1 < u1) & (u1 < 1) & (-1 < u2) & (u2 < 1),
            (-1 < u1) & (u1 < 1) & (u2 <= -1), (u1 <= -1) & (u2 >= 1),
            (u1 <= -1) & (-1 < u2) & (u2 < 1),
        ]
        choicelist = [
            e_x + (2 * params['m'] - 1) * e_u + 2 * params['m'] * (u1 - 1), e_x - e_u - 4 * params['m'],
            e_x - e_u - 2 * params['m'] * (u1 - 1), e_x + (2 * params['m'] - 1) * e_u,
            e_x - e_u - 2 * params['m'] * (u1 + 1), e_x - e_u + 4 * params['m'],
            e_x + (2 * params['m'] - 1) * e_u + 2 * params['m'] * (u1 + 1),
        ]
        de_udt = jnp.select(condlist, choicelist, default=e_x - e_u)
        de_phidt = e_x - (params['q'] * e_phi)

        return jnp.array([de_xdt, de_ydt, de_zdt, de_udt, de_phidt], dtype=jnp.float64)

    def _error_system_ode(self, t, current_error_state, current_hr_state):
        """Calculates derivatives ONLY for the error system variables by calling the appropriate static method."""
        if self.dynamics == "simplified":
            return HRNetworkErrorSystem.simplified_vector_field(current_error_state, current_hr_state, self.params)
        elif self.dynamics == "complete":
            return HRNetworkErrorSystem.complete_vector_field(current_error_state, current_hr_state, self.params)
        else:
            raise ValueError(f"Unknown dynamics mode: {self.dynamics}")

        # In your HRNetworkErrorSystem class...

    def _clipped_ode_func(self, t, combined_state, args):
        """
        Combined HR-network + error-system ODE for SIMPLIFIED dynamics.
        The ERROR variables are box-constrained to self.error_clip_bound.
        """
        # Add near the top of your class for clarity
        ERROR_SLICE = slice(10, 15)  # indices of e_x … e_phi
        LOW, HIGH = [-5, 5] # box limits

        # ── 1. Unpack raw (unclipped) state ─────────────────────────────────
        hr_state = combined_state[:10]  # x1…phi2
        err_state = combined_state[ERROR_SLICE]  # e_x…e_phi

        # ── 2. Compute raw derivatives ─────────────────────────────────────
        hr_d = self.hr_network._ode_func_internal(t, hr_state)
        err_d = self._error_system_ode(t, err_state, hr_state)
        combined_derivatives = jnp.concatenate([hr_d, err_d])

        # ── 3. Velocity-clamp ONLY the error-state derivatives ─────────────
        err_d_clamped = jnp.where(
            ((err_state >= HIGH) & (err_d > 0.0)) |  # trying to exit above +5
            ((err_state <= LOW) & (err_d < 0.0)),  # trying to exit below −5
            0.0,  # stop outward motion
            err_d  # keep everything else
        )

        final_derivatives = combined_derivatives.at[ERROR_SLICE].set(err_d_clamped)
        return final_derivatives

    def _unclipped_ode_func(self, t, combined_state, args):
        """
        Combined HR-network + error-system ODE for COMPLETE dynamics (no clipping).
        """
        # --- Unpack the combined state vector ---
        current_hr_state = combined_state[0:10]
        current_error_state = combined_state[10:15]

        # --- Calculate HR Derivatives using the HindmarshRose instance ---
        hr_derivatives_flat = self.hr_network._ode_func_internal(t, current_hr_state)

        # --- Calculate Error System Derivatives ---
        error_derivatives = self._error_system_ode(t, current_error_state, current_hr_state)

        # --- Combine derivatives into a single vector ---
        combined_derivatives = jnp.concatenate([hr_derivatives_flat, error_derivatives])
        return combined_derivatives

    # ------------------------------------------------------------------
    # Diffrax‑based solver
    # ------------------------------------------------------------------
    def solve(self, solver=None, t0=None, t1=None, dt0=None, n_points=None,
              stepsize_controller=None, max_steps=None):
        """
        Integrate the combined HR + error system with Diffrax.
        """
        # --- Choose the correct vector field function BEFORE calling the solver ---
        if self.dynamics == "simplified":
            vector_field = self._clipped_ode_func
        elif self.dynamics == "complete":
            vector_field = self._unclipped_ode_func
        else:
            # This case should not be reached with the current code
            raise ValueError(f"Unknown dynamics mode: {self.dynamics}")

        try:
            sol = dfx.diffeqsolve(
                terms=dfx.ODETerm(vector_field),  # Use the selected function
                solver=solver,
                t0=t0,
                t1=t1,
                dt0=dt0,
                y0=self.combined_state0,
                saveat=n_points,
                stepsize_controller=stepsize_controller,
                max_steps=max_steps,
                args=None
            )
        except Exception as exc:
            print(f"Solver failed with exception: {exc}")
            self.failed = True
            self.t = self.solution = None
            return np.nan, np.nan

        # Normal exit ------------------------------------------------------
        self.failed = False
        self.t = jnp.asarray(sol.ts)
        self.solution = jnp.asarray(sol.ys)
        # Recompute derivative using the selected vector field
        self.derivative = jnp.asarray(jax.vmap(vector_field, in_axes=(0, 0, None))(sol.ts, sol.ys, None))

    # ------------------------------------------------------------------
    # Extract results
    # ------------------------------------------------------------------
    def get_results_dict(self, transient_ratio: float = 0.0):
        """
        Return a dict containing the post-transient time vector ('t') and all
        state variables of the combined HR + error system.
        """
        # ── locate cut-off index using the actual (non-uniform) time stamps ─
        cutoff_time = self.t[0] + transient_ratio * (self.t[-1] - self.t[0])
        start_idx = jnp.searchsorted(self.t, cutoff_time, side="left")

        t_post   = self.t[start_idx:]
        sol_post = self.solution[start_idx:]
        deriv_post = self.derivative[start_idx:]

        var_names = [
            'x1', 'y1', 'z1', 'u1', 'phi1',
            'x2', 'y2', 'z2', 'u2', 'phi2',
            'e_x', 'e_y', 'e_z', 'e_u', 'e_phi'
        ]

        deriv_names = [f'd_{name}' for name in var_names]

        result = {'t': t_post}
        result.update({name: sol_post[:, i] for i, name in enumerate(var_names)})
        result.update({name: deriv_post[:, i] for i, name in enumerate(deriv_names)}) # Use pre-saved derivatives
        return result


# --- Example Usage ---
if __name__ == '__main__':
    from visualization.plotting import plot_error_and_state_differences

    # initial state (x, y, z, u, φ for each neuron)
    INITIAL_HR_STATE0 = [
        0.1, 0.2, 0.3, 0.4, 0.1,   # neuron 1
        0.2, 0.3, 0.4, 0.5, 0.2    # neuron 2
    ]

    # external currents and coupling matrix
    I_ext = [0.8, 0.8]
    xi = [[0, 1], [1, 0]]

    # Example modification of parameters
    sim_params = DEFAULT_PARAMS.copy()
    # sim_params['ge'] = 0.65

    # Create simulator instance
    simulator = HRNetworkErrorSystem(params=sim_params, dynamics='complete',
                                     hr_initial_state=INITIAL_HR_STATE0, I_ext=I_ext, hr_xi=xi)

    # integration settings
    start_time = 0
    end_time = 1000
    dt_initial = 0.01
    point_num = 10000
    transient_ratio = 0
    n_points = dfx.SaveAt(ts=jnp.linspace(start_time, end_time, point_num), dense=True)
    max_steps = int((end_time - start_time) / dt_initial) * 20

    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-10, atol=1e-12)
    # stepsize_controller = dfx.ConstantStepSize()

    # run simulation ----------------------------------------------------
    print("Running simulation...")
    import time
    tic = time.perf_counter()

    simulator.solve(
        solver=solver,
        t0=start_time,
        t1=end_time,
        dt0=dt_initial,
        n_points=n_points,
        stepsize_controller=stepsize_controller,
        max_steps=max_steps
    )

    toc = time.perf_counter()
    print(f"Finished in {(toc - tic):.2f} s")

    # Get results dictionary
    results = simulator.get_results_dict(transient_ratio)

    # Plotting
    plot_error_and_state_differences(results)

    # # # Write to file
    # # import pickle
    # # with open('error_system.pkl', 'wb') as f:
    # #     pickle.dump(results, f)
```

#### File: `model.py`
```python
import numpy as np
import jax, jax.numpy as jnp
import diffrax as dfx
jax.config.update("jax_enable_x64", True)

# ──────────────────────────────────────────────────────────────────────────────
# Default parameters dictionary
DEFAULT_PARAMS = {
    # --- Neuron Params ---
    'a': 1.0,
    'b': 3.0,
    'c': 1.0,
    'd': 5.0,
    'f': 0.2,
    'h': 0.3,
    'k': 0.87,
    'm': 0.5,
    'q': 0.005,
    'r': 0.006,
    's': 5.2,
    'x0': -1.56,
    'rho': 0.7,
    'ge': 0.7,
}

# Default initial state for a single neuron
DEFAULT_STATE0 = [0.0, 0.0, 0.0, 1.0, 0.0]

class HindmarshRose:
    """Simulates a network of N coupled Hindmarsh-Rose neurons."""

    def __init__(self, N, params=None, initial_state=None, I_ext=None, xi=None):
        self.N = N
        self.params = params.copy()
        self.initial_state = jnp.array(initial_state, dtype=jnp.float64).flatten()

        # External current
        if isinstance(I_ext, (int, float)):
            self.I_ext = jnp.full(self.N, float(I_ext), dtype=jnp.float64)
        else:
            self.I_ext = jnp.array(I_ext, dtype=jnp.float64)

        # Electrical coupling matrix
        if isinstance(xi, (int, float)):
            self.xi = jnp.full((self.N, self.N), float(xi), dtype=jnp.float64)
        else:
            self.xi = jnp.array(xi, dtype=jnp.float64)
        if self.N > 0:
            self.xi = jnp.fill_diagonal(self.xi, 0, inplace=False)

        # Attributes to store results later
        self.t = None
        self.solution = None
        self.failed = None

    @staticmethod
    def vector_field(t, state, N, params, I_ext, xi):
        """
        Calculates the time derivatives for a network of N coupled Hindmarsh-Rose neurons.
        This static method contains the core physics and can be called from anywhere
        without needing an instance of the class, e.g., HindmarshRose.vector_field(...)
        """
        # Reshape the flat state vector into a 2D array (N neurons x 5 variables)
        state_matrix = state.reshape((N, 5))
        x, y, z, u, phi = state_matrix.T

        # Electrical Coupling
        x_diff = x[jnp.newaxis, :] - x[:, jnp.newaxis]
        electrical_coupling = params['ge'] * jnp.sum(jnp.asarray(xi) * x_diff, axis=1)

        # --- Calculate derivatives ---
        dxdt = (y - (params['a'] * x ** 3) + (params['b'] * x ** 2)
                + (params['k'] * (params['h'] + (params['f'] * (u ** 2))) * x)
                + (params['rho'] * phi * x) + I_ext
                + electrical_coupling
                )
        dydt = params['c'] - (params['d'] * x ** 2) - y
        dzdt = params['r'] * (params['s'] * (x + params['x0']) - z)
        dudt = -u + (params['m'] * (jnp.abs(u + 1.0) - jnp.abs(u - 1.0))) + x
        dphidt = x - (params['q'] * phi)

        # Assign calculated derivative vectors to the output matrix
        d_state_dt_matrix = jnp.zeros_like(state_matrix).at[:, 0].set(dxdt).at[:, 1].set(dydt) \
                                .at[:, 2].set(dzdt).at[:, 3].set(dudt) \
                                .at[:, 4].set(dphidt)

        return d_state_dt_matrix.flatten()

    def _ode_func_internal(self, t, state):
        # This instance method now calls the static method, passing its instance attributes.
        return HindmarshRose.vector_field(t, state, self.N, self.params, self.I_ext, self.xi)

    # ────────────────────────────────────────────────────────────────────────
    def solve(self, solver=None, t0=None, t1=None, dt0=None, n_points=None,
              stepsize_controller=None, max_steps=None):
        """Integrate the model with Diffrax."""

        try:
            sol = dfx.diffeqsolve(
                terms=dfx.ODETerm(lambda t, y, args: self._ode_func_internal(t, y)),
                solver=solver,
                t0=t0,
                t1=t1,
                dt0=dt0,
                y0=self.initial_state,
                saveat=dfx.SaveAt(ts=jnp.linspace(t0, t1, n_points)),
                stepsize_controller=stepsize_controller,
                max_steps=max_steps
            )
        except Exception as exc:
            print(f"Solver failed with exception: {exc}")
            self.failed = True
            self.t = self.solution = None
            return np.nan, np.nan

        self.t = jnp.asarray(sol.ts)
        self.solution = jnp.asarray(sol.ys)
        self.failed = False
        self.derivative = jnp.asarray(jax.vmap(self._ode_func_internal)(sol.ts, sol.ys))

    # ────────────────────────────────────────────────────────────────────────
    def get_results_dict(self, transient_ratio: float = 0.0):
        """
        Return the simulated time series as a dict *after* removing the initial
        transient period.
        """
        # <<< 4. MODIFIED: Entire method updated to handle derivatives
        if self.t is None:
            print("Simulation has not been run or has failed. Returning empty dictionary.")
            return {}

        # ── locate cut-off index using the actual time stamps ─
        cutoff_time = self.t[0] + transient_ratio * (self.t[-1] - self.t[0])
        start_idx = jnp.searchsorted(self.t, cutoff_time, side="left")

        # --- Slice all result arrays to remove transient ---
        t_post = self.t[start_idx:]
        sol_post = self.solution[start_idx:]
        deriv_post = self.derivative[start_idx:]

        # --- Create names for state variables and their derivatives ---
        var_names = [f"{v}{i+1}" for i in range(self.N) for v in ("x", "y", "z", "u", "phi")]
        deriv_names = [f"d_{name}" for name in var_names]

        # ── assemble and return dict ────────────────────────────────────
        result = {'t': t_post}
        result.update({name: sol_post[:, i] for i, name in enumerate(var_names)})
        result.update({name: deriv_post[:, i] for i, name in enumerate(deriv_names)})

        return result









# --- Example Usage ---
if __name__ == '__main__':
    from visualization.plotting import plot_all_time_series
    import os

    #=======================================================================================
    # --- Example 1: Single Neuron ---
    #=======================================================================================

    print("Simulating Single Neuron...")
    # Initialize using defaults where possible
    sim_params = DEFAULT_PARAMS.copy()
    sim_params['ge'] = 0.2

    # create the model
    hr_single = HindmarshRose(N=1, params=sim_params, initial_state=DEFAULT_STATE0, I_ext=0.8, xi=0)

    # integration settings
    start_time = 0
    end_time = 1000
    dt_initial = 0.05
    n_points = 10000
    transient_ratio = 0.5
    max_steps  = int((end_time - start_time) / dt_initial) * 20

    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-8, atol=1e-10)
    # stepsize_controller = dfx.ConstantStepSize()

    # Run the solver
    import time
    start = time.perf_counter()

    hr_single.solve(
        solver=solver, t0=start_time, t1=end_time, dt0=dt_initial, n_points=n_points,
        stepsize_controller=stepsize_controller, max_steps=max_steps)

    finish = time.perf_counter()
    time = finish - start
    print(time)

    results_single = hr_single.get_results_dict(transient_ratio)

    plot_all_time_series(results_single, N=1, title="One Neuron Simulation", save_fig=0)

    #=======================================================================================
    # --- Example 2: Two Coupled Neurons ---
    #=======================================================================================

    print("\nSimulating Two Coupled Neurons...")

    # Initialize using defaults where possible
    sim_params = DEFAULT_PARAMS.copy()
    sim_params['ge'] = 0.2

    # initial state (x, y, z, u, φ for each neuron)
    state0_coupled = [
        0.1, 0.2, 0.3, 0.4, 0.1,   # neuron 1
        0.2, 0.3, 0.4, 0.5, 0.2    # neuron 2
    ]

    # external currents and coupling matrix
    I_ext_coupled = [0.8, 0.8]
    xi_coupled = [[0, 1], [1, 0]]

    # create the model
    hr_coupled = HindmarshRose(
        N=2,
        params=sim_params,
        initial_state=state0_coupled,
        I_ext=I_ext_coupled,
        xi=xi_coupled
    )

    # integration settings
    start_time = 0
    end_time   = 1000
    dt_initial = 0.05
    n_points   = 10000
    transient_ratio = 0.5
    max_steps  = int((end_time - start_time) / dt_initial) * 20

    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-10, atol=1e-12)
    # stepsize_controller = dfx.ConstantStepSize()

    # run the solver
    import time
    start = time.perf_counter()
    hr_coupled.solve(
        solver=solver,
        t0=start_time,
        t1=end_time,
        dt0=dt_initial,
        n_points=n_points,
        stepsize_controller=stepsize_controller,
        max_steps=max_steps)
    finish = time.perf_counter()
    time = finish - start
    print(time)

    results_coupled = hr_coupled.get_results_dict(transient_ratio)

    plot_all_time_series(results_coupled, N=2, title="Two Coupled Neurons Simulation", save_fig=False)
```

#### File: `physics.py`
```python
import numpy as np
import jax, jax.numpy as jnp
import diffrax as dfx
jax.config.update("jax_enable_x64", True)


# ---------------------------------------------------------------------------
# Hamiltonian --------------------------------------------------
# ---------------------------------------------------------------------------
def calculate_H(
    results: dict[str, np.ndarray],
    params:  dict[str, float],
    C: float = 0.0,           # integration constant (defaults to 0)
) -> np.ndarray:
    """
    Return the time series of the Hamiltonian **H** evaluated along a trajectory.

    Parameters
    ----------
    results
        Output of ``Error_System.HRNetworkErrorSystem.get_results_dict``.
        Must contain the keys
        ``'x1', 'u1', 'e_x', 'e_y', 'e_z', 'e_u', 'e_phi'``.
    params
        Parameter dictionary (e.g. ``NeuralModel.DEFAULT_PARAMS``) with at least
        ``a, b, d, f, h, k, m, q, r, rho, s, ge``.
        Only ``d, k, f, r, rho, s`` are used here.
    C
        Arbitrary additive constant.  Leave at 0 unless you need a specific
        reference level.

    Returns
    -------
    numpy.ndarray
        1-D array of length ``len(results['x1'])`` – the Hamiltonian value at
        each recorded time step.
    """
    # ------------------------------------------------------------------
    # 1. Unpack parameters we actually need
    # ------------------------------------------------------------------
    d   = params["d"]
    k   = params["k"]
    f   = params["f"]
    r   = params["r"]
    rho = params["rho"]
    s   = params["s"]

    # ------------------------------------------------------------------
    # 2. Extract trajectory arrays
    # ------------------------------------------------------------------
    required = ("x1", "u1", "e_x", "e_y", "e_z", "e_u", "e_phi")
    try:
        x1, u1, e_x, e_y, e_z, e_u, e_phi = (
            jnp.asarray(results[k]) for k in required
        )
    except KeyError as missing:
        raise KeyError(f"results dict is missing key: {missing}") from None

    # Shape validation (same pattern as in calculate_dHdt)
    ref_shape = x1.shape
    for key, arr in zip(required, (x1, u1, e_x, e_y, e_z, e_u, e_phi)):
        if arr.shape != ref_shape:
            raise ValueError(
                f"Shape mismatch for results['{key}'] – expected {ref_shape}, got {arr.shape}"
            )

    # ------------------------------------------------------------------
    # 3. Hamiltonian formula (vectorised)
    # ------------------------------------------------------------------
    H = (
        2 * d * x1 * e_x**2
        + e_y**2
        - 4 * d * k * f * u1 * x1**2 * e_u**2
        - 2 * d * rho * x1**2 * e_phi**2
        + e_z
        - r * s * e_phi
        + C
    )

    # Guard against numerical overflow/underflow
    H = jnp.where(jnp.isfinite(H), H, jnp.nan)
    return H

# ---------------------------------------------------------------------------
# Hamiltonian Rate of Change---------------------------------------
# ---------------------------------------------------------------------------
def calculate_dHdt(results: dict[str, np.ndarray], params: dict[str, float]) -> np.ndarray:  # noqa: N802 – keep MATLAB-style name
    """Return the time series of **dH/dt** evaluated along a trajectory.

    Parameters
    ----------
    results
        Dictionary produced by :pymeth:`Error_System.HRNetworkErrorSystem.get_results_dict`.
        Must contain the keys ``'x1', 'u1', 'phi1', 'u2', 'e_x', 'e_y', 'e_z', 'e_u', 'e_phi'``.
    params
        Parameter dictionary (e.g. :pydata:`NeuralModel.DEFAULT_PARAMS`).
        Must include at least the entries ::

            a, b, d, f, h, k, m, q, r, rho, s, ge

    Returns
    -------
    numpy.ndarray
        1-D array with ``len(results['x1'])`` elements – the value of dH/dt at
        each recorded time step.
    """
    # ----------------------------------------------------------------------
    # 1. Unpack parameters
    # ----------------------------------------------------------------------
    a   = params["a"]
    b   = params["b"]
    d   = params["d"]
    f   = params["f"]
    h   = params["h"]
    k   = params["k"]
    m   = params["m"]
    q   = params["q"]
    r   = params["r"]
    rho = params["rho"]
    s   = params["s"]
    ge  = params["ge"]

    # ----------------------------------------------------------------------
    # 2. Extract trajectory arrays
    # ----------------------------------------------------------------------
    required = ("x1", "u1", "phi1", "u2", "e_x", "e_y", "e_z", "e_u", "e_phi")
    try:
        x1, u1, phi1, u2, e_x, e_y, e_z, e_u, e_phi = (
            jnp.asarray(results[k]) for k in required
        )
    except KeyError as missing:
        raise KeyError(f"results dict is missing key: {missing}") from None

    # Shape validation ------------------------------------------------------
    ref_shape = x1.shape
    for key, arr in zip(required, (x1, u1, phi1, u2, e_x, e_y, e_z, e_u, e_phi)):
        if arr.shape != ref_shape:
            raise ValueError(f"Shape mismatch for results['{key}'] – expected {ref_shape}, got {arr.shape}")

    # ----------------------------------------------------------------------
    # 3. Helper functions N, alpha, beta (vectorised)
    # ----------------------------------------------------------------------
    N = (
        -3 * a * x1**2 + 2 * b * x1 + k * h + k * f * u1**2 + rho * phi1 - 2 * ge
    )

    conds = [
        (u1 >= 1) & (u2 > -1) & (u2 < 1),
        (u1 >= 1) & (u2 <= -1),
        (u1 > -1) & (u1 < 1) & (u2 >= 1),
        (u1 > -1) & (u1 < 1) & (u2 > -1) & (u2 < 1),
        (u1 > -1) & (u1 < 1) & (u2 <= -1),
        (u1 <= -1) & (u2 >= 1),
        (u1 <= -1) & (u2 > -1) & (u2 < 1),
    ]

    alpha_choices = [2 * m - 1, -1, -1, 2 * m - 1, -1, -1, 2 * m - 1]
    beta_choices  = [
        2 * m * (u1 - 1),
        -4 * m,
        -2 * m * (u1 - 1),
        0,
        -2 * m * (u1 + 1),
        4 * m,
        2 * m * (u1 + 1),
    ]

    alpha = jnp.select(conds, alpha_choices, default=-1)
    beta  = jnp.select(conds, beta_choices,  default=0)

    # ----------------------------------------------------------------------
    # 4. Assemble dH/dt (Eq. Hdot)
    # ----------------------------------------------------------------------
    dHdt = (
        4 * d * x1 * N * e_x**2
        - 2 * e_y**2
        - r * e_z
        - 8 * d * k * f * u1 * x1**2 * alpha * e_u**2
        - 8 * d * k * f * u1 * x1**2 * beta * e_u
        + 4 * d * rho * q * x1**2 * e_phi**2
        + q * r * s * e_phi
    )

    # Guard against numerical overflow/underflow
    dHdt = jnp.where(jnp.isfinite(dHdt), dHdt, jnp.nan)
    return dHdt


# ---------------------------------------------------------------------------
# Lyapunov Rate of change -----------------------------------------
# ---------------------------------------------------------------------------

def calculate_dVdt(results, params):
    """
    Calculates the time series of dV/dt based on simulation results.

    Args:
        results (dict): Dictionary containing the time series output from
                        HRNetworkErrorSystem.get_results_dict(). Must contain keys
                        'x1', 'u1', 'phi1', 'u2', 'e_x', 'e_y', 'e_z', 'e_u', 'e_phi'.
        params (dict): Dictionary containing the model parameters. Must contain
                       keys 'a', 'b', 'k', 'h', 'f', 'rho', 'ge', 'gc', 'lam',
                       'v_syn', 'theta', 'd', 'r', 's', 'q', 'm'.

    Returns:
        np.ndarray: Time series array of dV/dt values. Returns None if
                    required keys are missing.
    """

    # --- Extract Parameters ---
    a = params['a']
    b = params['b']
    k = params['k']
    h = params['h']
    f = params['f']
    rho = params['rho']
    ge = params['ge']
    d = params['d']
    r = params['r']
    s = params['s']
    q = params['q']
    m = params['m']

    # --- Extract Time Series Variables ---
    x1 = results['x1']
    u1 = results['u1']
    phi1 = results['phi1']
    u2 = results['u2']
    e_x = results['e_x']
    e_y = results['e_y']
    e_z = results['e_z']
    e_u = results['e_u']
    e_phi = results['e_phi']

    # Ensure inputs are numpy arrays
    x1, u1, phi1, u2 = jnp.asarray(x1), jnp.asarray(u1), jnp.asarray(phi1), jnp.asarray(u2)
    e_x, e_y, e_z, e_u, e_phi = jnp.asarray(e_x), jnp.asarray(e_y), jnp.asarray(e_z), jnp.asarray(e_u), jnp.asarray(e_phi)

    # --- Calculate dV/dt Components ---
    dVdt_term1 = (((-3 * a * (x1 ** 2)) + (2 * b * x1) + (k * h) +
                  (k * f * (u1 ** 2)) + (rho * phi1) - (2 * ge)) * (e_x ** 2))
    dVdt_term2 = -(e_y ** 2)
    dVdt_term3 = -(r * (e_z ** 2))
    dVdt_term4 = -(q * (e_phi ** 2))
    dVdt_term5 = 2 * k * f * u1 * x1 * e_x * e_u
    dVdt_term6 = (1 - (2 * d * x1)) * e_x * e_y
    dVdt_term7 = r * s * e_x * e_z
    dVdt_term8 = ((rho * x1) + 1) * e_x * e_phi

    # --- Calculate Piecewise Term ---

    conditions = [
        (u1 >= 1) & (-1 < u2) & (u2 < 1),
        (u1 >= 1) & (u2 <= -1),
        (-1 < u1) & (u1 < 1) & (u2 >= 1),
        (-1 < u1) & (u1 < 1) & (-1 < u2) & (u2 < 1),
        (-1 < u1) & (u1 < 1) & (u2 <= -1),
        (u1 <= -1) & (u2 >= 1),
        (u1 <= -1) & (-1 < u2) & (u2 < 1)
    ]
    choices = [
        (e_x * e_u) + ((2 * m - 1) * (e_u ** 2)) + (2 * m * (u1 - 1) * e_u),
        (e_x * e_u) - (e_u ** 2) - (4 * m * e_u),
        (e_x * e_u) - (e_u ** 2) - (2 * m * (u1 - 1) * e_u),
        (e_x * e_u) + ((2 * m - 1) * (e_u ** 2)),
        (e_x * e_u) - (e_u ** 2) - (2 * m * (u1 + 1) * e_u),
        (e_x * e_u) - (e_u ** 2) + (4 * m * e_u),
        (e_x * e_u) + ((2 * m - 1) * (e_u ** 2)) + (2 * m * (u1 + 1) * e_u)
    ]
    default_choice = (e_x * e_u) - (e_u ** 2)
    piecewise_term = jnp.select(conditions, choices, default=default_choice)


    # --- Combine all terms ---
    dVdt = (dVdt_term1 + dVdt_term2 + dVdt_term3 + dVdt_term4 +
            dVdt_term5 + dVdt_term6 + dVdt_term7 + dVdt_term8 +
            piecewise_term)

    dVdt = jnp.where(jnp.isfinite(dVdt), dVdt, jnp.nan)     # replace inf / nan by nan

    return dVdt


# --- Example Usage ---
if __name__ == '__main__':
    from src.hr_model.error_system import HRNetworkErrorSystem
    from src.hr_model.model import DEFAULT_PARAMS
    from visualization.plotting import (
        plot_hamiltonian,
        plot_hamiltonian_derivative,
        plot_lyapunov_derivative
    )

    # initial state (x, y, z, u, φ for each neuron)
    INITIAL_HR_STATE0 = [
        0.1, 0.2, 0.3, 0.4, 0.1,   # neuron 1
        0.2, 0.3, 0.4, 0.5, 0.2    # neuron 2
    ]

    # external currents and coupling matrix
    I_ext = [0.8, 0.8]
    xi = [[0, 1], [1, 0]]

    # Example modification of parameters
    sim_params = DEFAULT_PARAMS.copy()
    # sim_params['ge'] = 0.65

    # Create simulator instance
    simulator = HRNetworkErrorSystem(params=sim_params, dynamics='complete',
                                     hr_initial_state=INITIAL_HR_STATE0, I_ext=I_ext, hr_xi=xi)

    # integration settings
    start_time = 0
    end_time = 1000
    dt_initial = 0.01
    point_num = 10000
    transient_ratio = 0
    n_points = dfx.SaveAt(ts=jnp.linspace(start_time, end_time, point_num), dense=True)
    max_steps = int((end_time - start_time) / dt_initial) * 20

    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-10, atol=1e-12)
    # stepsize_controller = dfx.ConstantStepSize()

    # run simulation ----------------------------------------------------
    print("Running simulation...")
    import time
    tic = time.perf_counter()

    simulator.solve(
        solver=solver,
        t0=start_time,
        t1=end_time,
        dt0=dt_initial,
        n_points=n_points,
        stepsize_controller=stepsize_controller,
        max_steps=max_steps
    )

    toc = time.perf_counter()
    print(f"Finished in {(toc - tic):.2f} s")

    # Get results dictionary
    results = simulator.get_results_dict(transient_ratio)

    # --- Calculate H, dH/dt, and dV/dt ---
    print("Calculating physical quantities...")
    H = calculate_H(results, sim_params)
    dHdt = calculate_dHdt(results, sim_params)
    dVdt = calculate_dVdt(results, sim_params)
    print("Calculations complete.")

    # --- Plotting ---
    print("Generating plots...")

    # --- Plotting ---
    print("Generating plots...")

    plot_hamiltonian(results['t'], H, save_fig=1)
    plot_hamiltonian_derivative(results['t'], dHdt, save_fig=1)
    plot_lyapunov_derivative(results['t'], dVdt,  save_fig=1)
```

================================================================================
## Folder: src/.ipynb_checkpoints
------------------------------------------------------------

================================================================================
## Folder: src/sph_pinn
------------------------------------------------------------
### Other Files:
- 3Stage_phpinn.ipynb
- phpinn.ipynb
- pinn.ipynb

### Python Source Files:

#### File: `optimize_hyperparams.py`
```python
"""
optimize_hyperparams.py
-----------------------
This script uses the Optuna framework to perform an automated hyperparameter
search for the Combined_sPHNN_PINN model defined in pH_PINN.py.
It defines an "objective" function that Optuna repeatedly calls with different
hyperparameter combinations. Each call (a "trial") trains the model for a
fixed number of epochs and returns the best training Hamiltonian loss.
Optuna uses these results to intelligently search for the optimal set of hyperparameters.
Results of the study are saved to a SQLite database file (optimize_hyperparams.db)
in the 'results/PINN Data/' directory, allowing the optimization to be paused
and resumed.
To run this script:
1. Make sure you have Optuna and its storage dependencies installed:
   pip install optuna
   pip install "optuna[storages]"
2. Place this file in the `src/sph_pinn/` directory.
3. Run from the root directory of your project: python -m src.sph_pinn.optimize_hyperparams

To view the results dashboard after running (run from project root):
optuna-dashboard "sqlite:///results/PINN Data/optimize_hyperparams.db"
"""
import os
import sys
import jax
import jax.numpy as jnp
import equinox as eqx
import optax
import optuna

# Ensure the script can find other modules in the project
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

# --- Import necessary components from your existing files ---
from src.hr_model.model import DEFAULT_PARAMS
from src.sph_pinn.pH_PINN import (
    StateNN, HamiltonianNN, DissipationNN, DynamicJ_NN, # Import component networks
    generate_data,
    normalize,
    create_windows, # Import windowing function
    train_step,
    evaluate_model,
)

# JAX configuration
jax.config.update("jax_enable_x64", True)

# ==============================================================================
# 1. LOCAL MODEL DEFINITION FOR HYPERPARAMETER FLEXIBILITY
# ==============================================================================

# We redefine the combined model here to match the new architecture in pH_PINN.py
# and to easily pass hyperparameters from the Optuna trial.
class Combined_sPHNN_PINN(eqx.Module):
    """Main model combining a unified state predictor and sPHNN structure."""
    state_net: StateNN
    hamiltonian_net: HamiltonianNN
    dissipation_net: DissipationNN
    j_net: DynamicJ_NN

    def __init__(self, key, config):
        state_key, h_key, d_key, j_key = jax.random.split(key, 4)

        state_dim = config['state_dim']
        q_dim = config['q_dim']

        # Unpack configs for clarity
        cfg_state = config['state_nn']
        cfg_h = config['hamiltonian_nn']
        cfg_d = config['dissipation_nn']
        cfg_j = config['j_net']

        self.state_net = StateNN(
            key=state_key,
            out_size=state_dim + q_dim,
            width=cfg_state['width'],
            depth=cfg_state['depth'],
            activation=cfg_state['activation'],
            mapping_size=cfg_state['fourier_features']['mapping_size'],
            scale=cfg_state['fourier_features']['scale']
        )
        self.hamiltonian_net = HamiltonianNN(
            h_key, state_dim=state_dim,
            hidden_size=cfg_h['hidden_size'],
            ficnn_width=cfg_h['ficnn']['width'],
            ficnn_depth=cfg_h['ficnn']['depth'],
            ficnn_activation=cfg_h['ficnn']['activation']
        )
        self.dissipation_net = DissipationNN(
            d_key, state_dim=state_dim,
            hidden_size=cfg_d['hidden_size'],
            width=cfg_d['width'],
            depth=cfg_d['depth'],
            activation=cfg_d['activation']
        )
        self.j_net = DynamicJ_NN(
            j_key, state_dim=state_dim,
            hidden_size=cfg_j['hidden_size'],
            width=cfg_j['width'],
            depth=cfg_j['depth'],
            activation=cfg_j['activation']
        )

# ==============================================================================
# 2. OBJECTIVE FUNCTION FOR OPTUNA
# ==============================================================================

def objective(trial, epochs_per_trial, static_data):
    """
    The main objective function that Optuna will minimize.
    Args:
        trial (optuna.Trial): An Optuna trial object used to suggest hyperparameters.
        epochs_per_trial (int): The number of epochs to train for during each trial.
        static_data (dict): A dictionary containing all pre-processed UN-WINDOWED data and stats.
    Returns:
        float: The best training Hamiltonian loss achieved during the trial.
    """
    # --- 1. Suggest Hyperparameters from the Search Space ---
    key = jax.random.PRNGKey(42)  # Use a fixed key for reproducibility across trials
    model_key, _ = jax.random.split(key)

    # Data pre-processing
    window_size = trial.suggest_int("window_size", 32, 128)

    # StateNN Fourier Features & Architecture
    mapping_size = trial.suggest_int("mapping_size", 32, 128)
    scale = trial.suggest_float("scale", 10, 500, log=True)
    state_width = trial.suggest_int("state_width", 8, 512)
    state_depth = trial.suggest_int("state_depth", 2, 6)
    state_activation_name = trial.suggest_categorical("state_activation", ["tanh", "softplus"])
    state_activation = getattr(jax.nn, state_activation_name)

    # HamiltonianNN (LSTM + FICNN)
    h_hidden_size = trial.suggest_int("h_hidden_size", 8, 512)
    h_ficnn_width = trial.suggest_int("h_ficnn_width", 8, 512)
    h_ficnn_depth = trial.suggest_int("h_ficnn_depth", 2, 6)

    # DissipationNN and J_NN (LSTMs + MLPs)
    jr_hidden_size = trial.suggest_int("jr_hidden_size", 16, 64)
    d_width = trial.suggest_int("d_width", 4, 64)
    d_depth = trial.suggest_int("d_depth", 2, 6)
    j_width = trial.suggest_int("j_width", 4, 64)
    j_depth = trial.suggest_int("j_depth", 2, 6)

    # Optimizer
    lr_initial = trial.suggest_float("lr_initial", 1e-4, 1e-2, log=True)
    decay_steps = trial.suggest_int("decay_steps", 500, 4000)

    # Training and Loss
    batch_size = trial.suggest_categorical("batch_size", [1024, 2048])
    lambda_conservative_max = trial.suggest_float("lambda_conservative_max", 0.1, 20, log=True)
    lambda_dissipative_max = trial.suggest_float("lambda_dissipative_max", 0.1, 20, log=True)
    lambda_physics_max = trial.suggest_float("lambda_physics_max", 0.1, 20, log=True)
    lambda_warmup_epochs = trial.suggest_int("lambda_warmup_epochs", 500, 3000)

    # --- 2. Create Windowed Data for this Trial ---
    (t_train_w, s_train_w, q_train_w, s_dot_train_w, H_train_w) = create_windows(
        window_size, static_data['t_train_norm'], static_data['s_train_norm'], static_data['q_train_norm'],
        static_data['s_dot_train_norm'], static_data['H_train_norm']
    )
    (t_val_w, s_val_w, q_val_w, s_dot_val_w, H_val_w) = create_windows(
        window_size, static_data['t_val_norm'], static_data['s_val_norm'], static_data['q_val_norm'],
        static_data['s_dot_val_norm'], static_data['H_val_norm']
    )

    # --- 3. Build Model and Optimizer with Suggested Values ---
    nn_config = {
        "state_dim": static_data['s_dim'],
        "q_dim": static_data['q_dim'],
        "state_nn": {
            "width": state_width, "depth": state_depth, "activation": state_activation,
            "fourier_features": {"mapping_size": mapping_size, "scale": scale}
        },
        "hamiltonian_nn": {
            "hidden_size": h_hidden_size,
            "ficnn": {"width": h_ficnn_width, "depth": h_ficnn_depth, "activation": jax.nn.softplus}
        },
        "dissipation_nn": {
            "hidden_size": jr_hidden_size, "width": d_width, "depth": d_depth, "activation": jax.nn.softplus
        },
        "j_net": {
            "hidden_size": jr_hidden_size, "width": j_width, "depth": j_depth, "activation": jax.nn.softplus
        }
    }
    model = Combined_sPHNN_PINN(key=model_key, config=nn_config)

    lr_schedule = optax.linear_schedule(
        init_value=lr_initial, end_value=1e-5, transition_steps=decay_steps
    )
    optimizer = optax.adamw(learning_rate=lr_schedule)
    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))

    # --- 4. Run the Training Loop ---
    best_training_hamiltonian_loss = jnp.inf
    num_batches = t_train_w.shape[0] // batch_size
    if num_batches == 0:
        num_batches = 1

    for epoch in range(epochs_per_trial):
        warmup_factor = jnp.minimum(1.0, (epoch + 1) / lambda_warmup_epochs)
        current_lambda_conservative = lambda_conservative_max * warmup_factor
        current_lambda_dissipative = lambda_dissipative_max * warmup_factor
        current_lambda_physics = lambda_physics_max * warmup_factor

        key, shuffle_key = jax.random.split(key)
        perm = jax.random.permutation(shuffle_key, t_train_w.shape[0])
        t_shuffled, s_shuffled, q_shuffled, s_dot_shuffled, H_shuffled = (
            t_train_w[perm], s_train_w[perm], q_train_w[perm], s_dot_train_w[perm], H_train_w[perm]
        )

        epoch_hamiltonian_loss = 0.0
        for i in range(num_batches):
            start, end = i * batch_size, (i + 1) * batch_size
            t_b, s_b, q_b, s_dot_b, H_b = (
                t_shuffled[start:end], s_shuffled[start:end], q_shuffled[start:end],
                s_dot_shuffled[start:end], H_shuffled[start:end]
            )
            model, opt_state, _, loss_components = train_step(
                model, opt_state, optimizer, t_b, s_b, q_b, s_dot_b, H_b,
                current_lambda_conservative, current_lambda_dissipative, current_lambda_physics,
                static_data['hr_params'], static_data['t_mean'], static_data['t_std'],
                static_data['s_mean'], static_data['s_std'], static_data['q_mean'],
                static_data['q_std'], static_data['s_dot_mean'], static_data['s_dot_std'],
                static_data['H_mean'], static_data['H_std']
            )
            epoch_hamiltonian_loss += loss_components['hamiltonian']

        avg_epoch_hamiltonian_loss = epoch_hamiltonian_loss / num_batches

        if avg_epoch_hamiltonian_loss < best_training_hamiltonian_loss:
            best_training_hamiltonian_loss = avg_epoch_hamiltonian_loss

    # --- 5. Return the Final Metric to Optuna ---
    return best_training_hamiltonian_loss


# ==============================================================================
# 3. MAIN EXECUTION BLOCK
# ==============================================================================

if __name__ == "__main__":
    # --- 1. Load and Prepare Data (Done Once) ---
    print("Loading and preparing data for optimization...")
    data_path = os.path.join(os.path.dirname(__file__), '..', '..', 'results', 'PINN Data', 'error_system_data.pkl')
    t, s, q, s_dot_true, H_analytical = generate_data(data_path)
    if t is None:
        sys.exit("Exiting: Data loading failed. Make sure 'error_system_data.pkl' exists.")

    validation_split = 0.2
    num_samples = s.shape[0]
    key = jax.random.PRNGKey(123)
    perm = jax.random.permutation(key, num_samples)
    t_shuffled, s_shuffled, q_shuffled, s_dot_shuffled, H_shuffled = \
        t[perm], s[perm], q[perm], s_dot_true[perm], H_analytical[perm]
    t_shuffled = t_shuffled.reshape(-1, 1)

    split_idx = int(num_samples * (1 - validation_split))
    t_train, t_val = jnp.split(t_shuffled, [split_idx])
    s_train, s_val = jnp.split(s_shuffled, [split_idx])
    q_train, q_val = jnp.split(q_shuffled, [split_idx])
    s_dot_train, s_dot_val = jnp.split(s_dot_shuffled, [split_idx])
    H_train, H_val = jnp.split(H_shuffled, [split_idx])

    # --- Normalize Data ---
    t_mean, t_std = jnp.mean(t_train), jnp.std(t_train)
    s_mean, s_std = jnp.mean(s_train, axis=0), jnp.std(s_train, axis=0)
    q_mean, q_std = jnp.mean(q_train, axis=0), jnp.std(q_train, axis=0)
    s_dot_mean, s_dot_std = jnp.mean(s_dot_train, axis=0), jnp.std(s_dot_train, axis=0)
    H_mean, H_std = jnp.mean(H_train), jnp.std(H_train)

    static_data = {
        's_dim': s_train.shape[1], 'q_dim': q_train.shape[1],
        'hr_params': DEFAULT_PARAMS.copy(),
        't_train_norm': normalize(t_train, t_mean, t_std),
        's_train_norm': normalize(s_train, s_mean, s_std),
        'q_train_norm': normalize(q_train, q_mean, q_std),
        's_dot_train_norm': normalize(s_dot_train, s_dot_mean, s_dot_std),
        'H_train_norm': normalize(H_train, H_mean, H_std),
        't_val_norm': normalize(t_val, t_mean, t_std),
        's_val_norm': normalize(s_val, s_mean, s_std),
        'q_val_norm': normalize(q_val, q_mean, q_std),
        's_dot_val_norm': normalize(s_dot_val, s_dot_mean, s_dot_std),
        'H_val_norm': normalize(H_val, H_mean, H_std),
        't_mean': t_mean, 't_std': t_std, 's_mean': s_mean, 's_std': s_std,
        'q_mean': q_mean, 'q_std': q_std, 's_dot_mean': s_dot_mean, 's_dot_std': s_dot_std,
        'H_mean': H_mean, 'H_std': H_std,
    }

    # --- 2. Create and Run the Optuna Study ---
    print("\nStarting Optuna hyperparameter search...")

    results_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'PINN Data')
    os.makedirs(results_dir, exist_ok=True)

    db_name = os.path.basename(__file__).replace('.py', '.db')
    db_path = os.path.join(results_dir, db_name)

    storage_name = f"sqlite:///{db_path}"
    study_name = "sphnn_pinn_optimization_study_v2" # New study name for new architecture

    # Use a lambda to pass static arguments to the objective function
    objective_with_args = lambda trial: objective(trial, epochs_per_trial=500, static_data=static_data)

    study = optuna.create_study(
        study_name=study_name,
        storage=storage_name,
        direction="minimize",
        load_if_exists=True
    )

    study.optimize(objective_with_args, n_trials=50) # Increased trials for larger search space

    # --- 3. Print and Save the Results ---
    print("\nOptimization finished.")
    print(f"Study results are saved in: {storage_name}")
    print("Number of finished trials: ", len(study.trials))

    best_trial = study.best_trial

    print("Best trial:")
    print(f"  Value (Best Training Hamiltonian Loss): {best_trial.value:.6f}")
    print("  Best Hyperparameters: ")
    for key, value in best_trial.params.items():
        print(f"    {key}: {value}")

    output_txt_path = os.path.join(results_dir, "best_hyperparams_v2.txt")
    with open(output_txt_path, 'w') as f:
        f.write("Best Hyperparameter Optimization Results (v2 Architecture)\n")
        f.write("========================================================\n\n")
        f.write(f"Best Value (Training Hamiltonian Loss): {best_trial.value}\n\n")
        f.write("Best Hyperparameters:\n")
        for key, value in best_trial.params.items():
            f.write(f"    {key}: {value}\n")

    print(f"\n✅ Best hyperparameters saved to: {output_txt_path}")
```

#### File: `pH_PINN.py`
```python
import jax, jax.numpy as jnp
import equinox as eqx
import optax
import matplotlib.pyplot as plt
import pickle
import sys
from src.hr_model.model import DEFAULT_PARAMS
import os

# JAX configuration to use 64-bit precision.
jax.config.update("jax_enable_x64", True)


# ==============================================================================
# 1. NEURAL NETWORK DEFINITIONS
# ==============================================================================

class FourierFeatures(eqx.Module):
    """Encodes a 1D input into a higher-dimensional space using Fourier features."""
    b_matrix: jax.Array
    output_size: int = eqx.field(static=True)

    def __init__(self, key, in_size=1, mapping_size=32, scale=1):
        n_pairs = mapping_size // 2
        self.b_matrix = jax.random.normal(key, (n_pairs, in_size)) * scale
        self.output_size = n_pairs * 2

    def __call__(self, t):
        if t.ndim == 1:
            t = t[None, :]
        t_proj = t @ self.b_matrix.T
        return jnp.concatenate([jnp.sin(t_proj), jnp.cos(t_proj)], axis=-1).squeeze()


class StateNN(eqx.Module):
    """An MLP with Fourier Features to approximate the combined state [q(t), s(t)]."""
    layers: list
    activation: callable

    def __init__(self, key, out_size, width, depth, activation, mapping_size, scale):
        fourier_key, *layer_keys = jax.random.split(key, depth + 1)
        self.activation = activation

        fourier_layer = FourierFeatures(fourier_key, in_size=1, mapping_size=mapping_size, scale=scale)

        self.layers = [
            fourier_layer,
            eqx.nn.Linear(fourier_layer.output_size, width, key=layer_keys[0]),
            *[eqx.nn.Linear(width, width, key=key) for i in range(1, depth - 1)],
            eqx.nn.Linear(width, out_size, key=layer_keys[-1])
        ]

    def __call__(self, t):
        x = self.layers[0](t)
        for layer in self.layers[1:-1]:
            x = self.activation(layer(x))
        return self.layers[-1](x)


# --- sPHNN Component Networks (Adapted for Windowed Data with LSTMs) ---

class _FICNN(eqx.Module):
    """Internal helper class for a Fully Input Convex Neural Network."""
    w_layers: list
    u_layers: list
    final_layer: eqx.nn.Linear
    activation: callable = eqx.field(static=True)

    def __init__(self, key, in_size: int, out_size: int, width: int, depth: int, activation: callable):
        self.activation = activation
        keys = jax.random.split(key, depth)
        self.w_layers = [eqx.nn.Linear(in_size, width, key=keys[0])]
        self.w_layers.extend([eqx.nn.Linear(in_size, width, key=key) for key in keys[1:-1]])
        self.u_layers = [eqx.nn.Linear(width, width, use_bias=False, key=key) for key in keys[1:-1]]
        self.final_layer = eqx.nn.Linear(width, out_size, use_bias=False, key=keys[-1])

    def __call__(self, s):
        z = self.activation(self.w_layers[0](s))
        for i in range(len(self.u_layers)):
            # Enforce non-negative weights for convexity
            u_layer_non_negative = eqx.tree_at(lambda l: l.weight, self.u_layers[i], jnp.abs(self.u_layers[i].weight))
            z = self.activation(u_layer_non_negative(z) + self.w_layers[i + 1](s))
        return self.final_layer(z)[0]


class HamiltonianNN(eqx.Module):
    """
    Learns a convex Hamiltonian H(s) using an LSTM to process history and
    a FICNN to guarantee convexity.
    """
    lstm: eqx.nn.LSTMCell
    ficnn: _FICNN
    state_dim: int = eqx.field(static=True)
    hidden_size: int = eqx.field(static=True)

    def __init__(self, key, state_dim, hidden_size, ficnn_width, ficnn_depth, ficnn_activation):
        lstm_key, ficnn_key = jax.random.split(key)

        self.state_dim = state_dim
        self.hidden_size = hidden_size

        self.lstm = eqx.nn.LSTMCell(input_size=state_dim, hidden_size=hidden_size, key=lstm_key)

        ficnn_in_size = state_dim + hidden_size
        self.ficnn = _FICNN(ficnn_key, in_size=ficnn_in_size, out_size=1,
                            width=ficnn_width, depth=ficnn_depth, activation=ficnn_activation)

    def __call__(self, s_window):
        h0 = jnp.zeros((self.hidden_size,))
        c0 = jnp.zeros((self.hidden_size,))
        initial_carry = (h0, c0)

        def lstm_scan(carry, x):
            new_carry = self.lstm(x, carry)
            return new_carry, new_carry[0]

        final_carry, _ = jax.lax.scan(lstm_scan, initial_carry, s_window)
        final_hidden_state = final_carry[0]

        s_last = s_window[-1]
        augmented_input = jnp.concatenate([s_last, final_hidden_state])
        return self.ficnn(augmented_input)


class DissipationNN(eqx.Module):
    """
    Learns a positive semi-definite dissipation matrix R(s) = L(s)L(s)^T
    using an LSTM followed by an MLP.
    """
    lstm: eqx.nn.LSTMCell
    layers: list
    activation: callable
    state_dim: int = eqx.field(static=True)

    def __init__(self, key, state_dim, hidden_size, width, depth, activation):
        self.state_dim = state_dim
        self.activation = activation
        num_l_elements = state_dim * (state_dim + 1) // 2

        lstm_key, *layer_keys = jax.random.split(key, depth + 1)

        self.lstm = eqx.nn.LSTMCell(input_size=state_dim, hidden_size=hidden_size, key=lstm_key)

        # MLP layers after the LSTM
        self.layers = [
            eqx.nn.Linear(hidden_size, width, key=layer_keys[0]),
            *[eqx.nn.Linear(width, width, key=key) for key in layer_keys[1:-1]],
            eqx.nn.Linear(width, num_l_elements, key=layer_keys[-1])
        ]

    def __call__(self, s_window):
        # LSTM part
        h0 = jnp.zeros((self.lstm.hidden_size,))
        c0 = jnp.zeros((self.lstm.hidden_size,))
        initial_carry = (h0, c0)

        def lstm_scan(carry, x):
            new_carry = self.lstm(x, carry)
            return new_carry, new_carry[0]

        final_carry, _ = jax.lax.scan(lstm_scan, initial_carry, s_window)
        x = final_carry[0]  # Use final hidden state as input to MLP

        # MLP part
        for layer in self.layers[:-1]:
            x = self.activation(layer(x))
        l_elements = self.layers[-1](x)

        # Matrix construction
        L = jnp.zeros((self.state_dim, self.state_dim))
        tril_indices = jnp.tril_indices(self.state_dim)
        L = L.at[tril_indices].set(l_elements)

        positive_diag = jax.nn.softplus(jnp.diag(L))
        L = L.at[jnp.diag_indices(self.state_dim)].set(positive_diag)
        return L @ L.T


class DynamicJ_NN(eqx.Module):
    """
    Learns a skew-symmetric structure matrix J(s) using an LSTM
    followed by an MLP.
    """
    lstm: eqx.nn.LSTMCell
    layers: list
    activation: callable
    state_dim: int = eqx.field(static=True)

    def __init__(self, key, state_dim, hidden_size, width, depth, activation):
        self.state_dim = state_dim
        self.activation = activation
        num_unique_elements = state_dim * (state_dim - 1) // 2

        lstm_key, *layer_keys = jax.random.split(key, depth + 1)

        self.lstm = eqx.nn.LSTMCell(input_size=state_dim, hidden_size=hidden_size, key=lstm_key)

        # MLP layers after the LSTM
        self.layers = [
            eqx.nn.Linear(hidden_size, width, key=layer_keys[0]),
            *[eqx.nn.Linear(width, width, key=key) for key in layer_keys[1:-1]],
            eqx.nn.Linear(width, num_unique_elements, key=layer_keys[-1])
        ]

    def __call__(self, s_window):
        # LSTM part
        h0 = jnp.zeros((self.lstm.hidden_size,))
        c0 = jnp.zeros((self.lstm.hidden_size,))
        initial_carry = (h0, c0)

        def lstm_scan(carry, x):
            new_carry = self.lstm(x, carry)
            return new_carry, new_carry[0]

        final_carry, _ = jax.lax.scan(lstm_scan, initial_carry, s_window)
        x = final_carry[0]  # Use final hidden state as input to MLP

        # MLP part
        for layer in self.layers[:-1]:
            x = self.activation(layer(x))
        upper_triangle_elements = self.layers[-1](x)

        # Matrix construction
        J = jnp.zeros((self.state_dim, self.state_dim))
        triu_indices = jnp.triu_indices(self.state_dim, k=1)
        J = J.at[triu_indices].set(upper_triangle_elements)
        return J - J.T


# --- The Combined Model ---
class Combined_sPHNN_PINN(eqx.Module):
    """Main model combining a unified state predictor and sPHNN structure."""
    state_net: StateNN
    hamiltonian_net: HamiltonianNN
    dissipation_net: DissipationNN
    j_net: DynamicJ_NN

    def __init__(self, key, config):
        state_key, h_key, d_key, j_key = jax.random.split(key, 4)

        state_dim = config['state_dim']
        q_dim = config['q_dim']

        # Unpack configs for clarity
        cfg_state = config['state_nn']
        cfg_h = config['hamiltonian_nn']
        cfg_d = config['dissipation_nn']
        cfg_j = config['j_net']

        self.state_net = StateNN(
            key=state_key,
            out_size=state_dim + q_dim,
            width=cfg_state['width'],
            depth=cfg_state['depth'],
            activation=cfg_state['activation'],
            mapping_size=cfg_state['fourier_features']['mapping_size'],
            scale=cfg_state['fourier_features']['scale']
        )
        self.hamiltonian_net = HamiltonianNN(
            h_key, state_dim=state_dim,
            hidden_size=cfg_h['hidden_size'],
            ficnn_width=cfg_h['ficnn']['width'],
            ficnn_depth=cfg_h['ficnn']['depth'],
            ficnn_activation=cfg_h['ficnn']['activation']
        )
        self.dissipation_net = DissipationNN(
            d_key, state_dim=state_dim,
            hidden_size=cfg_d['hidden_size'],
            width=cfg_d['width'],
            depth=cfg_d['depth'],
            activation=cfg_d['activation']
        )
        self.j_net = DynamicJ_NN(
            j_key, state_dim=state_dim,
            hidden_size=cfg_j['hidden_size'],
            width=cfg_j['width'],
            depth=cfg_j['depth'],
            activation=cfg_j['activation']
        )


# ==============================================================================
# 2. DATA HANDLING (omitted for brevity, no changes)
# ==============================================================================
def generate_data(file_path="error_system_data.pkl"):
    """
    Loads and prepares training data from a pre-generated pickle file containing
    multiple simulation runs.
    """
    print(f"Loading simulation data from {file_path}...")
    try:
        with open(file_path, 'rb') as f:
            # The file contains a list of result dictionaries
            all_runs_results = pickle.load(f)
    except FileNotFoundError:
        print(f"Error: Data file not found at {file_path}")
        print("Please run 'generate_data_for_PINN.py' to create the data file.")
        return None, None, None, None, None

    # Initialize lists to hold data from all runs
    all_t, all_s, all_q, all_s_dot, all_H = [], [], [], [], []

    # Process each simulation run
    for i, results in enumerate(all_runs_results):
        print(f"  ... processing run {i + 1}/{len(all_runs_results)}")

        # Extract data for the current run
        t = jnp.asarray(results['t'])
        s = jnp.vstack([
            results['e_x'], results['e_y'], results['e_z'],
            results['e_u'], results['e_phi']
        ]).T
        q = jnp.vstack([
            results['x1'], results['y1'], results['z1'], results['u1'], results['phi1'],
            results['x2'], results['y2'], results['z2'], results['u2'], results['phi2']
        ]).T
        s_dot_true = jnp.vstack([
            results['d_e_x'], results['d_e_y'], results['d_e_z'],
            results['d_e_u'], results['d_e_phi']
        ]).T
        H_analytical = jnp.asarray(results['Hamiltonian'])

        # Append to the main lists
        all_t.append(t)
        all_s.append(s)
        all_q.append(q)
        all_s_dot.append(s_dot_true)
        all_H.append(H_analytical)

    # Concatenate all runs into single arrays
    final_t = jnp.concatenate(all_t)
    final_s = jnp.concatenate(all_s)
    final_q = jnp.concatenate(all_q)
    final_s_dot = jnp.concatenate(all_s_dot)
    final_H = jnp.concatenate(all_H)

    print("Data loading and aggregation complete.")
    return final_t, final_s, final_q, final_s_dot, final_H


def normalize(data, mean, std):
    """Normalizes data using pre-computed statistics."""
    return (data - mean) / (std + 1e-8)


def denormalize(data, mean, std):
    """Denormalizes data using pre-computed statistics."""
    return data * std + mean


def create_windows(window_size: int, *arrays):
    """
    Creates overlapping windows from a set of time-series arrays using
    JAX-native indexing.
    """
    num_samples = arrays[0].shape[0]
    num_windows = num_samples - window_size + 1

    if num_windows <= 0:
        # Return empty arrays with correct dimensions if the input is smaller than the window
        return tuple(jnp.empty((0, window_size) + arr.shape[1:]) for arr in arrays)

    # Create a 2D array of indices that represents all windows
    start_indices = jnp.arange(num_windows)[:, None]
    window_offsets = jnp.arange(window_size)[None, :]
    indices = start_indices + window_offsets

    # Use the indices to gather the windowed data from each array
    windowed_arrays = [arr[indices] for arr in arrays]

    return tuple(windowed_arrays)


# ==============================================================================
# 3. TRAINING LOGIC (omitted for brevity, no changes)
# ==============================================================================
# --- Helper functions for the new physics-based loss terms ---

def _alpha(u1, u2, m):
    """Helper function for the dissipative field f_d."""
    conds = [
        jnp.logical_and(u1 >= 1, jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(u1 >= 1, u2 <= -1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 >= 1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 <= -1),
        jnp.logical_and(u1 <= -1, u2 >= 1),
        jnp.logical_and(u1 <= -1, jnp.logical_and(u2 > -1, u2 < 1)),
    ]
    choices = [2 * m - 1., -1., -1., 2 * m - 1., -1., -1., 2 * m - 1.]
    return jnp.select(conds, choices, default=-1.)


def _beta(u1, u2, m):
    """Helper function for the dissipative field f_d."""
    conds = [
        jnp.logical_and(u1 >= 1, jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(u1 >= 1, u2 <= -1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 >= 1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 <= -1),
        jnp.logical_and(u1 <= -1, u2 >= 1),
        jnp.logical_and(u1 <= -1, jnp.logical_and(u2 > -1, u2 < 1)),
    ]
    choices = [
        2 * m * (u1 - 1), -4 * m, -2 * m * (u1 - 1), 0.,
        -2 * m * (u1 + 1), 4 * m, 2 * m * (u1 + 1),
    ]
    return jnp.select(conds, choices, default=0.)


def f_c_fn(e, q, hr_params):
    """Calculates the conservative vector field f_c(e)."""
    e_x, e_y, e_u, e_phi = e[0], e[1], e[3], e[4]
    x1, u1 = q[0], q[3]

    k, f, rho, d, r, s = \
        hr_params['k'], hr_params['f'], hr_params['rho'], hr_params['d'], hr_params['r'], hr_params['s']

    return jnp.array([
        e_y + 2 * k * f * u1 * x1 * e_u + rho * x1 * e_phi,
        -2 * d * x1 * e_x,
        r * s * e_x,
        e_x,
        e_x
    ])


def f_d_fn(e, q, hr_params):
    """Calculates the dissipative vector field f_d(e)."""
    e_x, e_y, e_z, e_u, e_phi = e[0], e[1], e[2], e[3], e[4]
    x1, u1, phi1, u2 = q[0], q[3], q[4], q[8]

    a, b, k, h, f, rho, g_e, r, q_param, m = \
        hr_params['a'], hr_params['b'], hr_params['k'], hr_params['h'], \
            hr_params['f'], hr_params['rho'], hr_params['ge'], hr_params['r'], \
            hr_params['q'], hr_params['m']

    N_val = -3 * a * x1 ** 2 + 2 * b * x1 + k * h + k * f * u1 ** 2 + rho * phi1 - 2 * g_e
    alpha_val = _alpha(u1, u2, m)
    beta_val = _beta(u1, u2, m)

    return jnp.array([
        N_val * e_x,
        -e_y,
        -r * e_z,
        alpha_val * e_u + beta_val,
        -q_param * e_phi
    ])


@eqx.filter_jit
def loss_fn(model: Combined_sPHNN_PINN, t_batch_norm, s_true_batch_norm, q_true_batch_norm, s_dot_true_batch_norm,
            H_true_batch_norm,
            lambda_conservative: float, lambda_dissipative: float, lambda_physics: float, hr_params: dict,
            t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std):
    """
    Calculates composite loss based on the LAST time step of each window.
    """
    # --- Part 1: State Prediction for the entire window ---
    batch_size, window_size = t_batch_norm.shape[0], t_batch_norm.shape[1]
    # Flatten time input for efficient vmap across batch and window dimensions
    t_batch_flat = t_batch_norm.reshape(-1, 1)
    all_states_pred_flat = jax.vmap(model.state_net)(t_batch_flat)
    # Reshape predictions back to window structure
    all_states_pred_windows = all_states_pred_flat.reshape(batch_size, window_size, -1)
    s_pred_norm_windows = all_states_pred_windows[:, :, 10:]

    # --- Part 2: Select the LAST time step for loss calculation ---
    all_states_pred_last_norm = all_states_pred_windows[:, -1, :]
    q_true_last_norm = q_true_batch_norm[:, -1, :]
    s_true_last_norm = s_true_batch_norm[:, -1, :]
    s_dot_true_last_norm = s_dot_true_batch_norm[:, -1, :]
    H_true_last_norm = H_true_batch_norm[:, -1]
    t_last_norm = t_batch_norm[:, -1]

    # --- Part 3: Unified Data Fidelity Loss (on last step) ---
    all_states_true_last_norm = jnp.concatenate([q_true_last_norm, s_true_last_norm], axis=1)
    data_loss = jnp.mean((all_states_pred_last_norm - all_states_true_last_norm) ** 2)

    # Get predicted q at the last time step
    q_pred_last_norm = all_states_pred_last_norm[:, :10]

    # Denormalize the values at the last time step for physics calculations
    s_pred_last = denormalize(s_true_last_norm, s_mean, s_std)  # Use true s for stability
    q_pred_last = denormalize(q_pred_last_norm, q_mean, q_std)

    # --- Part 4: Physics Calculations (using windows for LSTM models) ---
    grad_H_window_fn = jax.vmap(jax.grad(model.hamiltonian_net))
    grad_H_norm_window = grad_H_window_fn(s_pred_norm_windows)
    grad_H_norm = grad_H_norm_window[:, -1, :]  # Take gradient wrt the last state
    grad_H = grad_H_norm / (s_std + 1e-8)

    f_c_batch = jax.vmap(f_c_fn, in_axes=(0, 0, None))(s_pred_last, q_pred_last, hr_params)
    f_d_batch = jax.vmap(f_d_fn, in_axes=(0, 0, None))(s_pred_last, q_pred_last, hr_params)

    # Calculate s_dot from autodiff AT THE LAST TIME STEP
    get_autodiff_grad_s_slice = lambda net, t: jax.jvp(lambda t_scalar: net(t_scalar)[10:], (t,), (jnp.ones_like(t),))[
        1]
    s_dot_autodiff_last_norm = jax.vmap(get_autodiff_grad_s_slice, in_axes=(None, 0))(model.state_net, t_last_norm)
    s_dot_autodiff_last = s_dot_autodiff_last_norm * (s_std / (t_std + 1e-8))

    # --- Part 5: Loss Components (calculated on last step) ---
    # Physics Structure Loss
    J_norm = jax.vmap(model.j_net)(s_pred_norm_windows)
    R_norm = jax.vmap(model.dissipation_net)(s_pred_norm_windows)
    s_dot_from_structure_norm = jax.vmap(lambda j, r, g: (j - r) @ g)(J_norm, R_norm, grad_H_norm)
    s_dot_from_structure = s_dot_from_structure_norm * s_std
    s_dot_diss_cons = f_c_batch + f_d_batch
    s_dot_true_last_batch = denormalize(s_dot_true_last_norm, s_dot_mean, s_dot_std)
    loss_phys = jnp.mean((s_dot_true_last_batch - s_dot_from_structure) ** 2)

    # Conservative Loss
    lie_derivative = jax.vmap(jnp.dot)(f_c_batch, grad_H)
    loss_conservative = jnp.mean(lie_derivative ** 2)

    # Dissipative Loss
    dHdt_from_autodiff = jax.vmap(jnp.dot)(grad_H, s_dot_autodiff_last)
    dHdt_from_equations = jax.vmap(jnp.dot)(grad_H, f_d_batch)
    loss_dissipative = jnp.mean((dHdt_from_autodiff - dHdt_from_equations) ** 2)

    # Hamiltonian Loss (for monitoring only)
    H_pred_norm = jax.vmap(model.hamiltonian_net)(s_pred_norm_windows)
    H_pred = denormalize(H_pred_norm, H_mean, H_std)
    H_true_last = denormalize(H_true_last_norm, H_mean, H_std)

    correlation = jnp.corrcoef(H_true_last.flatten(), H_pred.flatten())[0, 1]
    sign = jnp.sign(correlation)
    H_pred_aligned = sign * H_pred - jnp.mean(sign * H_pred) + jnp.mean(H_true_last)
    loss_hamiltonian = jnp.mean((H_pred_aligned - H_true_last) ** 2)

    # --- Part 6: Total Loss ---
    state_loss = data_loss + jnp.mean((s_dot_true_last_batch - s_dot_diss_cons) ** 2)
    total_loss = (state_loss
                  + (lambda_conservative * loss_conservative)
                  + (lambda_dissipative * loss_dissipative)
                  + (lambda_physics * loss_phys))

    loss_components = {
        "total": total_loss,
        "data_unified": data_loss,
        "phys": loss_phys,
        "conservative": loss_conservative,
        "dissipative": loss_dissipative,
        "hamiltonian": loss_hamiltonian,
    }
    return total_loss, loss_components


@eqx.filter_jit
def train_step(model, opt_state, optimizer, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,
               lambda_conservative, lambda_dissipative, lambda_physics, hr_params,
               t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std):
    """Performs a single training step on a batch of windows."""
    (loss_val, loss_components), grads = eqx.filter_value_and_grad(loss_fn, has_aux=True)(
        model, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,
        lambda_conservative, lambda_dissipative, lambda_physics, hr_params,
        t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std
    )
    updates, opt_state = optimizer.update(grads, opt_state, model)
    model = eqx.apply_updates(model, updates)
    return model, opt_state, loss_val, loss_components


@eqx.filter_jit
def evaluate_model(model, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,
                   lambda_conservative, lambda_dissipative, lambda_physics, hr_params,
                   t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std):
    """Calculates the loss for the validation set (a batch of windows)."""
    loss_val, _ = loss_fn(
        model, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,
        lambda_conservative, lambda_dissipative, lambda_physics, hr_params,
        t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std
    )
    return loss_val


# ==============================================================================
# 4. MAIN EXECUTION LOGIC
# ==============================================================================
def main():
    """Main function to run the training and evaluation."""
    # --- Setup and Hyperparameters ---
    key = jax.random.PRNGKey(42)
    model_key, data_key = jax.random.split(key)

    # Training hyperparameters
    window_size = 64
    batch_size = 4096
    validation_split = 0.2
    initial_learning_rate = 1e-3
    end_learning_rate = 5e-5
    decay_steps = 3000
    epochs = 5000

    # Physics loss hyperparameters with warmup
    lambda_conservative_max = 1
    lambda_dissipative_max = 5
    lambda_physics_max = 15
    lambda_warmup_epochs = 2000

    # System parameters
    hr_params = DEFAULT_PARAMS.copy()

    # --- Generate and Prepare Data ---
    path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'PINN Data/',
                        'error_system_data.pkl')
    t, s, q, s_dot_true, H_analytical = generate_data(path)
    if t is None:
        sys.exit("Exiting: Data loading failed.")

    t = t.reshape(-1, 1)

    # --- 1. Create Windowed Data FIRST (from the original, ordered data) ---
    print(f"Creating data windows with size {window_size}...")
    (t_w, s_w, q_w, s_dot_w, H_w) = create_windows(
        window_size, t, s, q, s_dot_true, H_analytical
    )

    # --- 2. Shuffle the WINDOWS, not the individual points ---
    num_windows = t_w.shape[0]
    perm = jax.random.permutation(data_key, num_windows)
    t_w_shuffled, s_w_shuffled, q_w_shuffled, s_dot_w_shuffled, H_w_shuffled = \
        t_w[perm], s_w[perm], q_w[perm], s_dot_w[perm], H_w[perm]

    # --- 3. Split the shuffled windows into training and validation sets ---
    split_idx = int(num_windows * (1 - validation_split))
    t_train_w, t_val_w = jnp.split(t_w_shuffled, [split_idx])
    s_train_w, s_val_w = jnp.split(s_w_shuffled, [split_idx])
    q_train_w, q_val_w = jnp.split(q_w_shuffled, [split_idx])
    s_dot_train_w, s_dot_val_w = jnp.split(s_dot_w_shuffled, [split_idx])
    H_train_w, H_val_w = jnp.split(H_w_shuffled, [split_idx])

    # --- Compute Normalization Stats (from flat training data BEFORE windowing) ---
    num_train_samples = int(s.shape[0] * (1 - validation_split))
    t_train_flat, s_train_flat, q_train_flat, s_dot_train_flat, H_train_flat = \
        t[:num_train_samples], s[:num_train_samples], q[:num_train_samples], \
            s_dot_true[:num_train_samples], H_analytical[:num_train_samples]

    t_mean, t_std = jnp.mean(t_train_flat), jnp.std(t_train_flat)
    s_mean, s_std = jnp.mean(s_train_flat, axis=0), jnp.std(s_train_flat, axis=0)
    q_mean, q_std = jnp.mean(q_train_flat, axis=0), jnp.std(q_train_flat, axis=0)
    s_dot_mean, s_dot_std = jnp.mean(s_dot_train_flat, axis=0), jnp.std(s_dot_train_flat, axis=0)
    H_mean, H_std = jnp.mean(H_train_flat), jnp.std(H_train_flat)

    # --- 4. Normalize the Windowed Data ---
    t_train_norm = normalize(t_train_w, t_mean, t_std)
    s_train_norm = normalize(s_train_w, s_mean, s_std)
    q_train_norm = normalize(q_train_w, q_mean, q_std)
    s_dot_train_norm = normalize(s_dot_train_w, s_dot_mean, s_dot_std)
    H_train_norm = normalize(H_train_w, H_mean, H_std)

    t_val_norm = normalize(t_val_w, t_mean, t_std)
    s_val_norm = normalize(s_val_w, s_mean, s_std)
    q_val_norm = normalize(q_val_w, q_mean, q_std)
    s_dot_val_norm = normalize(s_dot_val_w, s_dot_mean, s_dot_std)
    H_val_norm = normalize(H_val_w, H_mean, H_std)

    # --- Centralized Neural Network Configuration ---
    s_dim = s_train_flat.shape[1]
    q_dim = q_train_flat.shape[1]

    nn_config = {
        "state_dim": s_dim,
        "q_dim": q_dim,
        "state_nn": {
            "width": 256,
            "depth": 3,
            "activation": jax.nn.tanh,
            "fourier_features": {
                "mapping_size": 32,
                "scale": 300,
            }
        },
        "hamiltonian_nn": {
            "hidden_size": 64,
            "ficnn": {
                "width": 128,
                "depth": 3,
                "activation": jax.nn.softplus,
            }
        },
        "dissipation_nn": {
            "hidden_size": 32,
            "width": 8,
            "depth": 3,
            "activation": jax.nn.softplus,
        },
        "j_net": {
            "hidden_size": 32,
            "width": 8,
            "depth": 3,
            "activation": jax.nn.softplus,
        }
    }

    # Initialize the combined model
    model = Combined_sPHNN_PINN(key=model_key, config=nn_config)

    # --- Training Loop (omitted for brevity, no changes) ---
    lr_schedule = optax.linear_schedule(
        init_value=initial_learning_rate,
        end_value=end_learning_rate,
        transition_steps=decay_steps
    )
    optimizer = optax.adamw(learning_rate=lr_schedule)
    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))

    train_losses, val_losses = [], []
    phys_losses, conservative_losses, dissipative_losses, hamiltonian_losses = [], [], [], []
    best_model, best_val_loss = model, jnp.inf

    num_batches = t_train_norm.shape[0] // batch_size
    if num_batches == 0 and t_train_norm.shape[0] > 0:
        print(f"Warning: batch_size ({batch_size}) > num_windows. Setting num_batches to 1.")
        num_batches = 1

    print(f"Starting training for {epochs} epochs...")
    for epoch in range(epochs):
        # Loss weight warmup schedule
        warmup_factor = jnp.minimum(1.0, (epoch + 1) / lambda_warmup_epochs)
        current_lambda_conservative = lambda_conservative_max * warmup_factor
        current_lambda_dissipative = lambda_dissipative_max * warmup_factor
        current_lambda_physics = lambda_physics_max * warmup_factor

        key, shuffle_key = jax.random.split(key)
        perm = jax.random.permutation(shuffle_key, t_train_norm.shape[0])
        t_shuffled, s_shuffled, q_shuffled, s_dot_shuffled, H_shuffled = \
            t_train_norm[perm], s_train_norm[perm], q_train_norm[perm], s_dot_train_norm[perm], H_train_norm[perm]

        epoch_losses = {k: 0.0 for k in ["total", "data_unified", "phys", "conservative", "dissipative", "hamiltonian"]}

        for i in range(num_batches):
            start, end = i * batch_size, (i + 1) * batch_size
            t_b, s_b, q_b, s_dot_b, H_b = t_shuffled[start:end], s_shuffled[start:end], q_shuffled[
                                                                                        start:end], s_dot_shuffled[
                                                                                                    start:end], H_shuffled[
                                                                                                                start:end]

            model, opt_state, train_loss_val, loss_comps = train_step(
                model, opt_state, optimizer, t_b, s_b, q_b, s_dot_b, H_b,
                current_lambda_conservative, current_lambda_dissipative, current_lambda_physics, hr_params,
                t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std
            )
            for k in epoch_losses:
                if k in loss_comps:
                    epoch_losses[k] += loss_comps[k]

        avg_losses = {k: v / num_batches for k, v in epoch_losses.items()}

        val_loss = evaluate_model(
            model, t_val_norm, s_val_norm, q_val_norm, s_dot_val_norm, H_val_norm,
            current_lambda_conservative, current_lambda_dissipative, current_lambda_physics, hr_params,
            t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std
        )

        train_losses.append(avg_losses["total"])
        val_losses.append(val_loss)
        phys_losses.append(avg_losses["phys"])
        conservative_losses.append(avg_losses["conservative"])
        dissipative_losses.append(avg_losses["dissipative"])
        hamiltonian_losses.append(avg_losses["hamiltonian"])

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model = model

        if (epoch + 1) % 100 == 0 or epoch == 0:
            log_str = (
                f"Epoch {epoch + 1}/{epochs} | Train Loss: {avg_losses['total']:.4f} | Val Loss: {val_loss:.4f} | "
                f"Data: {avg_losses['data_unified']:.4f} | "
                f"Phys: {avg_losses['phys']:.4f} | "
                f"Cons: {avg_losses['conservative']:.4f} | Diss: {avg_losses['dissipative']:.4f} | "
                f"H_Loss: {avg_losses['hamiltonian']:.4f}"
            )
            print(log_str)

    print("Training finished.")
    print(f"Best validation loss achieved: {best_val_loss:.6f}")

    # ==============================================================================
    # 5. VISUALIZATION AND ANALYSIS
    # ==============================================================================

    output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'temp/')
    os.makedirs(output_dir, exist_ok=True)

    run_to_visualize_idx = 0

    print(f"\nGenerating visualization plots for simulation run #{run_to_visualize_idx + 1}...")

    with open(path, 'rb') as f:
        all_runs = pickle.load(f)
    if run_to_visualize_idx >= len(all_runs):
        print(f"Error: 'run_to_visualize_idx' is out of bounds. Setting to 0.")
        run_to_visualize_idx = 0
    vis_results = all_runs[run_to_visualize_idx]

    t_test = jnp.asarray(vis_results['t']).reshape(-1, 1)
    s_test = jnp.vstack(
        [vis_results['e_x'], vis_results['e_y'], vis_results['e_z'], vis_results['e_u'], vis_results['e_phi']]).T
    q_test = jnp.vstack(
        [vis_results['x1'], vis_results['y1'], vis_results['z1'], vis_results['u1'], vis_results['phi1'],
         vis_results['x2'], vis_results['y2'], vis_results['z2'], vis_results['u2'], vis_results['phi2']]).T
    s_dot_test = jnp.vstack([vis_results['d_e_x'], vis_results['d_e_y'], vis_results['d_e_z'], vis_results['d_e_u'],
                             vis_results['d_e_phi']]).T
    H_analytical_vis = jnp.asarray(vis_results['Hamiltonian'])

    t_test_norm = normalize(t_test, t_mean, t_std)

    all_states_pred_norm = jax.vmap(best_model.state_net)(t_test_norm)
    q_pred_norm = all_states_pred_norm[:, :10]
    s_pred_norm = all_states_pred_norm[:, 10:]

    s_pred = denormalize(s_pred_norm, s_mean, s_std)
    q_pred = denormalize(q_pred_norm, q_mean, q_std)

    # --- Create windows for the test data to feed into LSTM models ---
    (s_pred_norm_windows,) = create_windows(window_size, s_pred_norm)

    grad_H_norm_windows = jax.vmap(jax.grad(best_model.hamiltonian_net))(s_pred_norm_windows)
    grad_H_norm = grad_H_norm_windows[:, -1, :] # Take last gradient
    J_norm = jax.vmap(best_model.j_net)(s_pred_norm_windows)
    R_norm = jax.vmap(best_model.dissipation_net)(s_pred_norm_windows)
    
    # We only have predictions for the end of each window, so we align them with the original time axis.
    s_dot_from_structure_norm = jax.vmap(lambda j, r, g: (j - r) @ g)(J_norm, R_norm, grad_H_norm)
    s_dot_from_structure = s_dot_from_structure_norm * s_std

    f_c_batch_vis = jax.vmap(f_c_fn, in_axes=(0, 0, None))(s_pred, q_pred, hr_params)
    f_d_batch_vis = jax.vmap(f_d_fn, in_axes=(0, 0, None))(s_pred, q_pred, hr_params)
    s_dot_from_equations = f_c_batch_vis + f_d_batch_vis

    get_s_slice_autodiff_grad = lambda net, t: jax.jvp(lambda t_scalar: net(t_scalar)[10:], (t,), (jnp.ones_like(t),))[
        1]
    s_dot_autodiff_norm = jax.vmap(get_s_slice_autodiff_grad, in_axes=(None, 0))(best_model.state_net, t_test_norm)
    s_dot_autodiff = s_dot_autodiff_norm * (s_std / (t_std + 1e-8))

    print("Comparing learned Hamiltonian with analytical solution...")
    H_learned_norm = jax.vmap(best_model.hamiltonian_net)(s_pred_norm_windows)
    H_learned_aligned = denormalize(H_learned_norm, H_mean, H_std)
    
    # Align the length of analytical H with the windowed predictions
    H_analytical_vis_aligned = H_analytical_vis[window_size - 1:]
    
    correlation = jnp.corrcoef(H_analytical_vis_aligned.flatten(), H_learned_aligned.flatten())[0, 1]
    sign = jnp.sign(correlation if not jnp.isnan(correlation) else 1.0)
    H_learned_aligned = sign * H_learned_aligned - jnp.mean(sign * H_learned_aligned) + jnp.mean(H_analytical_vis_aligned)
    
    # Adjust time axis for windowed predictions
    t_test_windowed = t_test[window_size-1:]

    plt.figure(figsize=(12, 7))
    plt.plot(t_test[:2000], H_analytical_vis[:2000], label='Analytical Hamiltonian', color='blue')
    plt.plot(t_test_windowed[:2000], H_learned_aligned[:2000], label='Learned Hamiltonian (Aligned)', color='red', linestyle='--')
    plt.title("Time Evolution of Hamiltonians", fontsize=16)
    plt.xlabel("Time", fontsize=14)
    plt.ylabel("Hamiltonian Value", fontsize=14)
    plt.legend(fontsize=12)
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, 'hamiltonian_comparison.png'), dpi=300)
    plt.tight_layout()

    plt.figure(figsize=(12, 7))
    plt.plot(train_losses, label='Total Training Loss')
    plt.plot(val_losses, label='Total Validation Loss')
    plt.plot(hamiltonian_losses, label='Hamiltonian Loss', color='red')
    plt.plot(phys_losses, label='Physics Loss', color='purple')
    plt.plot(conservative_losses, label='Conservative Loss', alpha=0.7)
    plt.plot(dissipative_losses, label='Dissipative Loss', alpha=0.7)
    plt.yscale('log')
    plt.title('Training, Validation, and Physics Losses Over Epochs', fontsize=16)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss (Log Scale)', fontsize=12)
    plt.legend()
    plt.grid(True, which="both", ls="--")
    plt.savefig(os.path.join(output_dir, 'training_losses.png'), dpi=300)
    plt.tight_layout()

    fig, axes = plt.subplots(s_test.shape[1], 1, figsize=(12, 12), sharex=True)
    state_labels_s_dot = [r'$\dot{e}_x$', r'$\dot{e}_y$', r'$\dot{e}_z$', r'$\dot{e}_u$', r'$\dot{e}_\phi$']
    fig.suptitle("Derivative Fidelity Comparison", fontsize=18, y=0.99)
    for i in range(s_test.shape[1]):
        axes[i].plot(t_test[:2000], s_dot_test[:2000, i], label='True Derivative', color='green', linewidth=3, alpha=0.8)
        axes[i].plot(t_test_windowed[:2000], s_dot_from_structure[:2000, i], label='sPHNN Structure', color='red', linestyle='--')
        axes[i].plot(t_test[:2000], s_dot_from_equations[:2000, i], label='Analytical Eq. (f_c+f_d)', color='purple',
                     linestyle=':')
        axes[i].plot(t_test[:2000], s_dot_autodiff[:2000, i], label='Autodiff', color='orange', linestyle='-.')
        axes[i].set_ylabel(state_labels_s_dot[i], fontsize=14)
        axes[i].grid(True)
        axes[i].legend(loc='upper right')
    axes[-1].set_xlabel("Time", fontsize=14)
    fig.savefig(os.path.join(output_dir, 'derivative_fidelity.png'), dpi=300)
    plt.tight_layout(rect=[0, 0, 1, 0.97])

    fig, axes = plt.subplots(s_test.shape[1], 1, figsize=(12, 10), sharex=True)
    state_labels_error = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$']
    fig.suptitle("Error System State 's' Prediction: True vs. Predicted", fontsize=18, y=0.99)
    for i in range(s_test.shape[1]):
        axes[i].plot(t_test[:2000], s_test[:2000, i], 'b', label='True State', alpha=0.9)
        axes[i].plot(t_test[:2000], s_pred[:2000, i], 'r--', label='Predicted State')
        axes[i].set_ylabel(state_labels_error[i], fontsize=14)
        axes[i].grid(True)
        axes[i].legend(loc='upper right')
    axes[-1].set_xlabel("Time", fontsize=14)
    fig.savefig(os.path.join(output_dir, 'error_state_s_prediction.png'), dpi=300)
    plt.tight_layout(rect=[0, 0, 1, 0.97])

    fig, axes = plt.subplots(q_test.shape[1], 1, figsize=(12, 18), sharex=True)
    state_labels_q = [r'$x_1$', r'$y_1$', r'$z_1$', r'$u_1$', r'$\phi_1$', r'$x_2$', r'$y_2$', r'$z_2$', r'$u_2$',
                      r'$\phi_2$']
    fig.suptitle("HR System State 'q' Prediction: True vs. Predicted", fontsize=18, y=0.99)
    for i in range(q_test.shape[1]):
        axes[i].plot(t_test[:2000], q_test[:2000, i], 'b', label='True State', alpha=0.9)
        axes[i].plot(t_test[:2000], q_pred[:2000, i], 'r--', label='Predicted State')
        axes[i].set_ylabel(state_labels_q[i], fontsize=14)
        axes[i].grid(True)
        axes[i].legend(loc='upper right')
    axes[-1].set_xlabel("Time", fontsize=14)
    fig.savefig(os.path.join(output_dir, 'hr_state_q_prediction.png'), dpi=300)
    plt.tight_layout(rect=[0, 0, 1, 0.97])

    plt.close('all')
    print(f"All plots saved to {output_dir}")


if __name__ == "__main__":
    main()
```

#### File: `read_best_hyperparams.py`
```python
import optuna
import os


def main():
    """
    Loads an Optuna study from a SQLite database file and prints the
    details of the best trial.
    """
    # --- Define Study Details ---
    # These must match the values used in your optimize_hyperparams.py script.

    # MODIFICATION: Correctly navigate from src/sph_pinn up to the project root ('..', '..')
    # and then down to the results directory.
    results_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'PINN Data')

    db_name = 'optimize_hyperparams.db'
    db_path = os.path.join(results_dir, db_name)

    storage_name = f"sqlite:///{db_path}"
    study_name = "sphnn_pinn_optimization_study"

    # --- Load the Study ---
    print(f"Loading study '{study_name}' from {storage_name}...")
    try:
        study = optuna.load_study(study_name=study_name, storage=storage_name)
    except Exception as e:
        print(f"\nError loading study: {e}")
        print(f"Please ensure the database file exists at: {db_path}")
        return

    # --- Get the Best Trial and Print Results ---
    best = study.best_trial

    print("\n" + "=" * 40)
    print("         Best Trial Found")
    print("=" * 40)
    print(f"  Trial Number: {best.number}")
    print(f"  Best Value (Loss): {best.value:.6f}")
    print("\n  Best Hyperparameters:")
    for key, value in best.params.items():
        print(f"    '{key}': {value},")
    print("=" * 40)


if __name__ == "__main__":
    main()
```

#### File: `test_gpu.py`
```python
import jax
import sys

print(f"JAX running on: {jax.default_backend()}")
if jax.default_backend() != 'gpu':
    print("WARNING: JAX is not using the GPU. Check your JAX installation and CUDA setup.", file=sys.stderr)
else:
    print("✅ JAX is using the GPU.")

# You can also list all available devices
print("Available devices:")
for device in jax.devices():
    print(f"- {device}")

```

================================================================================
## Folder: src/sph_pinn/.ipynb_checkpoints
------------------------------------------------------------
### Other Files:
- test-checkpoint.ipynb

### Python Source Files:

#### File: `test_gpu-checkpoint.py`
```python
# import jax
# import sys
#
# print(f"JAX running on: {jax.default_backend()}")
# if jax.default_backend() != 'gpu':
#     print("WARNING: JAX is not using the GPU. Check your JAX installation and CUDA setup.", file=sys.stderr)
# else:
#     print("✅ JAX is using the GPU.")
#
# # You can also list all available devices
# print("Available devices:")
# for device in jax.devices():
#     print(f"- {device}")

import matplotlib, os
print("Interpreter:", os.sys.executable)
print("Backend:", matplotlib.get_backend())

```